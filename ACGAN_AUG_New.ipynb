{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e3b62dae",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting package metadata (current_repodata.json): ...working... failed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "CondaHTTPError: HTTP 000 CONNECTION FAILED for url <https://repo.anaconda.com/pkgs/main/win-64/current_repodata.json>\n",
      "Elapsed: -\n",
      "\n",
      "An HTTP error occurred when trying to retrieve this URL.\n",
      "HTTP errors are often intermittent, and a simple retry will get you on your way.\n",
      "\n",
      "If your current network has https://www.anaconda.com blocked, please file\n",
      "a support request with your network engineering team.\n",
      "\n",
      "'https//repo.anaconda.com/pkgs/main/win-64'\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!conda install pandas seaborn -y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "370513dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: opencv-python in d:\\miniconda3\\envs\\torch_1-11\\lib\\site-packages (4.5.5.64)\n",
      "Requirement already satisfied: torchsummary in d:\\miniconda3\\envs\\torch_1-11\\lib\\site-packages (1.5.1)\n",
      "Requirement already satisfied: scikit-learn in d:\\miniconda3\\envs\\torch_1-11\\lib\\site-packages (1.1.0)\n",
      "Requirement already satisfied: torchviz in d:\\miniconda3\\envs\\torch_1-11\\lib\\site-packages (0.0.2)\n",
      "Requirement already satisfied: utils in d:\\miniconda3\\envs\\torch_1-11\\lib\\site-packages (1.0.1)\n",
      "Requirement already satisfied: imageio in d:\\miniconda3\\envs\\torch_1-11\\lib\\site-packages (2.19.2)\n",
      "Requirement already satisfied: numpy>=1.19.3 in d:\\miniconda3\\envs\\torch_1-11\\lib\\site-packages (from opencv-python) (1.21.5)\n",
      "Requirement already satisfied: scipy>=1.3.2 in d:\\miniconda3\\envs\\torch_1-11\\lib\\site-packages (from scikit-learn) (1.7.3)\n",
      "Requirement already satisfied: joblib>=1.0.0 in d:\\miniconda3\\envs\\torch_1-11\\lib\\site-packages (from scikit-learn) (1.1.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in d:\\miniconda3\\envs\\torch_1-11\\lib\\site-packages (from scikit-learn) (3.1.0)\n",
      "Requirement already satisfied: torch in d:\\miniconda3\\envs\\torch_1-11\\lib\\site-packages (from torchviz) (1.11.0)\n",
      "Requirement already satisfied: graphviz in d:\\miniconda3\\envs\\torch_1-11\\lib\\site-packages (from torchviz) (0.20)\n",
      "Requirement already satisfied: pillow>=8.3.2 in d:\\miniconda3\\envs\\torch_1-11\\lib\\site-packages (from imageio) (9.0.1)\n",
      "Requirement already satisfied: typing_extensions in d:\\miniconda3\\envs\\torch_1-11\\lib\\site-packages (from torch->torchviz) (4.1.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install opencv-python torchsummary scikit-learn torchviz utils imageio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6a6a2202",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.parallel\n",
    "import torch.nn.functional as F\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.optim as optim\n",
    "import torch.utils.data\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.utils as vutils\n",
    "import utils\n",
    "import time\n",
    "from torch.nn.functional import one_hot\n",
    "from torchvision.utils import save_image\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import SubsetRandomSampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6cf2ef10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['sm_37', 'sm_50', 'sm_60', 'sm_61', 'sm_70', 'sm_75', 'sm_80', 'sm_86', 'compute_37'] cuda\n"
     ]
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(torch.cuda.get_arch_list(), device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b8079c45",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import argparse\n",
    "import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import HTML\n",
    "from IPython.display import clear_output\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "from tqdm import notebook\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import gc\n",
    "import matplotlib.colors as mat_color\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import random\n",
    "import cv2\n",
    "from torchvision.datasets import ImageNet, ImageFolder\n",
    "from torch.utils.data import DataLoader\n",
    "from torchsummary import summary\n",
    "from torchviz import make_dot\n",
    "import imageio\n",
    "from torchvision import datasets, transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3eb59879",
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_weights(net):\n",
    "    for m in net.modules():\n",
    "        if isinstance(m, nn.Conv2d):\n",
    "            m.weight.data.normal_(0, 0.02)\n",
    "            m.bias.data.zero_()\n",
    "        elif isinstance(m, nn.ConvTranspose2d):\n",
    "            m.weight.data.normal_(0, 0.02)\n",
    "            m.bias.data.zero_()\n",
    "        elif isinstance(m, nn.Linear):\n",
    "            m.weight.data.normal_(0, 0.02)\n",
    "            m.bias.data.zero_()\n",
    "\n",
    "class Generator(nn.Module):\n",
    "    # Network Architecture is exactly same as in infoGAN (https://arxiv.org/abs/1606.03657)\n",
    "    # Architecture : FC1024_BR-FC7x7x128_BR-(64)4dc2s_BR-(1)4dc2s_S\n",
    "    def __init__(self, input_dim=100, output_dim=1, input_size=32, class_num=10):\n",
    "        super(Generator, self).__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.input_size = input_size\n",
    "        self.class_num = class_num\n",
    "\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(self.input_dim + self.class_num, 1024),\n",
    "            nn.BatchNorm1d(1024),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(1024, 128 * (self.input_size // 4) * (self.input_size // 4)),\n",
    "            nn.BatchNorm1d(128 * (self.input_size // 4) * (self.input_size // 4)),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        self.deconv = nn.Sequential(\n",
    "            nn.ConvTranspose2d(128, 64, 4, 2, 1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(64, self.output_dim, 4, 2, 1),\n",
    "            nn.Tanh(),\n",
    "        )\n",
    "        initialize_weights(self)\n",
    "\n",
    "    def forward(self, input, label):\n",
    "        x = torch.cat([input, label], 1)\n",
    "        x = self.fc(x)\n",
    "        x = x.view(-1, 128, (self.input_size // 4), (self.input_size // 4))\n",
    "        x = self.deconv(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "    \n",
    "class Discriminator(nn.Module):\n",
    "    # Network Architecture is exactly same as in infoGAN (https://arxiv.org/abs/1606.03657)\n",
    "    # Architecture : (64)4c2s-(128)4c2s_BL-FC1024_BL-FC1_S\n",
    "    def __init__(self, input_dim=1, output_dim=1, input_size=32, class_num=10):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.input_size = input_size\n",
    "        self.class_num = class_num\n",
    "\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(self.input_dim, 64, 4, 2, 1),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Conv2d(64, 128, 4, 2, 1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.LeakyReLU(0.2),\n",
    "        )\n",
    "        self.fc1 = nn.Sequential(\n",
    "            nn.Linear(128 * (self.input_size // 4) * (self.input_size // 4), 1024),\n",
    "            nn.BatchNorm1d(1024),\n",
    "            nn.LeakyReLU(0.2),\n",
    "        )\n",
    "        self.dc = nn.Sequential(\n",
    "            nn.Linear(1024, self.output_dim),\n",
    "            nn.Sigmoid(),\n",
    "        )\n",
    "        self.cl = nn.Sequential(\n",
    "            nn.Linear(1024, self.class_num),\n",
    "        )\n",
    "        initialize_weights(self)\n",
    "\n",
    "    def forward(self, input):\n",
    "        x = self.conv(input)\n",
    "        x = x.view(-1, 128 * (self.input_size // 4) * (self.input_size // 4))\n",
    "        x = self.fc1(x)\n",
    "        d = self.dc(x)\n",
    "        c = self.cl(x)\n",
    "\n",
    "        return d, c\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8b174419",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_animation(path, num):\n",
    "    images = []\n",
    "    for e in range(num):\n",
    "        img_name = path + '_generate_animation_epoch%03d' % (e+1) + '.png'\n",
    "        images.append(imageio.imread(img_name))\n",
    "    imageio.mimsave(path + '_generate_animation.gif', images, fps=5)\n",
    "\n",
    "def loss_plot(hist, path = 'Train_hist.png', model_name = ''):\n",
    "    x = range(len(hist['D_loss']))\n",
    "\n",
    "    y1 = hist['D_loss']\n",
    "    y2 = hist['G_loss']\n",
    "\n",
    "    plt.plot(x, y1, label='D_loss')\n",
    "    plt.plot(x, y2, label='G_loss')\n",
    "\n",
    "    plt.xlabel('Iter')\n",
    "    plt.ylabel('Loss')\n",
    "\n",
    "    plt.legend(loc=4)\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "\n",
    "    path = os.path.join(path, model_name + '_loss.png')\n",
    "    plt.savefig(path)\n",
    "    plt.close()\n",
    "\n",
    "def save_images(images, size, image_path):\n",
    "    return imsave(images, size, image_path)\n",
    "\n",
    "def imsave(images, size, path):\n",
    "    image = np.squeeze(merge(images, size))\n",
    "    print(\"image saved at ->\", path)\n",
    "    return imageio.imwrite(path, image)\n",
    "\n",
    "def merge(images, size):\n",
    "    h, w = images.shape[1], images.shape[2]\n",
    "    if (images.shape[3] in (3,4)):\n",
    "        c = images.shape[3]\n",
    "        img = np.zeros((h * size[0], w * size[1], c))\n",
    "        for idx, image in enumerate(images):\n",
    "            i = idx % size[1]\n",
    "            j = idx // size[1]\n",
    "            img[j * h:j * h + h, i * w:i * w + w, :] = image\n",
    "        return img\n",
    "    elif images.shape[3]==1:\n",
    "        img = np.zeros((h * size[0], w * size[1]))\n",
    "        for idx, image in enumerate(images):\n",
    "            i = idx % size[1]\n",
    "            j = idx // size[1]\n",
    "            img[j * h:j * h + h, i * w:i * w + w] = image[:,:,0]\n",
    "        return img\n",
    "    else:\n",
    "        raise ValueError('in merge(images,size) images parameter ''must have dimensions: HxW or HxWx3 or HxWx4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "73154b3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ACGAN(object):\n",
    "    def __init__(self):\n",
    "        self.model_name = 'ACGAN'\n",
    "        self.I_want_to_train_faster = 333\n",
    "        self.num_epoch = round(1000/self.I_want_to_train_faster)\n",
    "        self.base_path = './data'\n",
    "        self.base_folder = \"Covid-19 Image Dataset\"\n",
    "        self.classic_folder = 'Coivd-19_Classic'\n",
    "        self.synthetic_folder = 'Coivd-19_Synthetic'\n",
    "        self.data_dir = os.path.join(self.base_path, self.classic_folder)\n",
    "        self.batch_size = 128\n",
    "        self.train_path = os.path.join(self.data_dir, \"train\")\n",
    "        self.test_path = os.path.join(self.data_dir, \"test\")\n",
    "        self.labels = os.listdir(self.train_path)\n",
    "        self.no_norm = mat_color.Normalize(vmin=0, vmax=255, clip=False)\n",
    "        self.label_dict = {\n",
    "            i : self.labels[i]\n",
    "            for i in range(len(self.labels))\n",
    "        }\n",
    "        self.img_size = 128 * 2\n",
    "        self.num_of_ch = 3\n",
    "        # size of z latent vector (i.e. size of generator input)\n",
    "        self.size_of_z = self.img_size\n",
    "        self.num_of_label = len(self.labels)\n",
    "        self.sample_num = self.num_of_label ** 2\n",
    "        self.learning_rate_g = 0.00000002 * self.I_want_to_train_faster\n",
    "        self.learning_rate_d = 0.00000002 * self.I_want_to_train_faster\n",
    "        # beta1 hyperparam for adam\n",
    "        self.adam_beta_1 = 0.5\n",
    "        # beta2 hyperparam for adam\n",
    "        self.adam_beta_2 = 0.999\n",
    "        self.real_label = 1.0\n",
    "        self.fake_label = 0.0\n",
    "        self.syn_criterion = nn.BCELoss().to(device) # synthesizing\n",
    "        self.class_criterion = nn.CrossEntropyLoss().to(device) # classification\n",
    "        \n",
    "        self.train_loader, self.test_loader, self.train_data, self.test_data = self.load_dataset()\n",
    "        self.generator = Generator(input_dim=self.size_of_z, \n",
    "                                   output_dim=self.num_of_ch, \n",
    "                                   input_size=self.img_size, \n",
    "                                   class_num=self.num_of_label).to(device)\n",
    "        self.discriminator = Discriminator(input_dim=self.num_of_ch, \n",
    "                                           output_dim=1, \n",
    "                                           input_size=self.img_size, \n",
    "                                           class_num=self.num_of_label).to(device)\n",
    "\n",
    "        self.optimizer_d = optim.Adam(self.discriminator.parameters(), \n",
    "                                      lr=self.learning_rate_d, \n",
    "                                      betas=(self.adam_beta_1, self.adam_beta_2))\n",
    "        self.optimizer_g = optim.Adam(self.generator.parameters(), \n",
    "                                      lr=self.learning_rate_g, \n",
    "                                      betas=(self.adam_beta_1, self.adam_beta_2))\n",
    "\n",
    "        # fixed noise & label\n",
    "        self.sample_z_fixed = torch.zeros((self.sample_num, self.size_of_z))\n",
    "        for i in range(self.num_of_label):\n",
    "            self.sample_z_fixed[i*self.num_of_label] = torch.rand(1, self.size_of_z)\n",
    "            for j in range(1, self.num_of_label):\n",
    "                self.sample_z_fixed[i*self.num_of_label + j] = self.sample_z_fixed[i*self.num_of_label]\n",
    "\n",
    "        temp = torch.zeros((self.num_of_label, 1))\n",
    "        for i in range(self.num_of_label):\n",
    "            temp[i, 0] = i\n",
    "\n",
    "        temp_y = torch.zeros((self.sample_num, 1))\n",
    "        for i in range(self.num_of_label):\n",
    "            temp_y[i*self.num_of_label: (i+1)*self.num_of_label] = temp\n",
    "\n",
    "        self.sample_y_fixed = torch.zeros((self.sample_num, self.num_of_label)).scatter_(1, temp_y.type(torch.LongTensor), 1)\n",
    "        self.sample_z_fixed, self.sample_y_fixed = self.sample_z_fixed.to(device), self.sample_y_fixed.to(device)\n",
    "        self.save_network_image = False\n",
    "        \n",
    "    def load_dataset(self):\n",
    "        train_dir = self.train_path\n",
    "        test_dir = self.test_path\n",
    "        transform = transforms.Compose([\n",
    "            transforms.Resize((self.img_size, self.img_size)),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "            ])\n",
    "        train_data = datasets.ImageFolder(train_dir, transform=transform)\n",
    "        train_loader = DataLoader(train_data, self.batch_size, shuffle=True, num_workers=1)\n",
    "\n",
    "        test_data = datasets.ImageFolder(test_dir ,transform=transform)\n",
    "        test_loader = DataLoader(test_data, self.batch_size, shuffle=True, num_workers=1)\n",
    "\n",
    "        return train_loader, test_loader, train_data, test_data\n",
    "\n",
    "    def modeltorchviz(model, input_1, input_2):\n",
    "        if input_2 != None:\n",
    "            y = model(input_1.to(device), input_2.to(device))\n",
    "        else:\n",
    "            y = model(input_1to(device))\n",
    "        if input_2 != None:\n",
    "            MyConvNetVis = make_dot(y, params=dict(list(model.named_parameters()) + [('x', input_1)] + [('x', input_2)]))\n",
    "        else:\n",
    "            MyConvNetVis = make_dot(y, params=dict(list(model.named_parameters()) + [('x', input_1)]))\n",
    "        MyConvNetVis.format = \"png\"\n",
    "        MyConvNetVis.directory = \"images\"\n",
    "        MyConvNetVis.view() \n",
    "    \n",
    "    def visualize_results(self, epoch, fix=True):\n",
    "        self.generator.eval()\n",
    "\n",
    "        image_frame_dim = round(np.sqrt(self.sample_num))\n",
    "        if fix:\n",
    "            \"\"\" fixed noise \"\"\"\n",
    "            samples = self.generator(self.sample_z_fixed, self.sample_y_fixed)\n",
    "        else:\n",
    "            \"\"\" random noise \"\"\"\n",
    "            sample_y_ = torch.zeros(self.batch_size, self.class_num).scatter_(1, torch.randint(0, self.class_num - 1, (self.batch_size, 1)).type(torch.LongTensor), 1)\n",
    "            sample_z_ = torch.rand((self.batch_size, self.z_dim))\n",
    "            sample_z_, sample_y_ = sample_z_.cuda(), sample_y_.cuda()\n",
    "            samples = self.generator(sample_z_, sample_y_)\n",
    "\n",
    "        samples = samples.cpu().data.numpy().transpose(0, 2, 3, 1)\n",
    "        samples = (samples + 1) / 2\n",
    "        save_images(samples[:image_frame_dim * image_frame_dim, :, :, :], [image_frame_dim, image_frame_dim],\n",
    "                    os.path.join('.', 'GANAug/plots/ACGAN') + \"/\" + self.model_name + '_generate_animation_epoch%03d' % epoch + '.png')\n",
    "    \n",
    "    def train(self):\n",
    "        for func in [\n",
    "            lambda: os.mkdir(os.path.join('.', 'GANAug')),\n",
    "            lambda: os.mkdir(os.path.join('.', 'GANAug/model')),\n",
    "            lambda: os.mkdir(os.path.join('.', 'GANAug/plots')),\n",
    "            lambda: os.mkdir(os.path.join('.', 'GANAug/model/ACGAN')),\n",
    "            lambda: os.mkdir(os.path.join('.', 'GANAug/plots/ACGAN')),\n",
    "            lambda: os.mkdir(os.path.join('.', 'GANAug/output_images')),\n",
    "            lambda: os.mkdir(os.path.join('.', 'GANAug/output_images/ACGAN')),\n",
    "            lambda: os.mkdir(os.path.join('.', 'GANAug/output_images/ACGAN/' + self.label_dict[0])),\n",
    "            lambda: os.mkdir(os.path.join('.', 'GANAug/output_images/ACGAN/' + self.label_dict[1])),\n",
    "            lambda: os.mkdir(os.path.join('.', 'GANAug/output_images/ACGAN/' + self.label_dict[2]))]:\n",
    "            try:\n",
    "                func()\n",
    "            except Exception as error:\n",
    "                print(error)\n",
    "                continue\n",
    "        matrix_fields = [\n",
    "            'G_losses',\n",
    "            'G_class_losses',\n",
    "            'G_syn_losses',\n",
    "            'D_losses',\n",
    "            'D_real_losses',\n",
    "            'D_fake_losses',\n",
    "            'D_class_losses',\n",
    "            'D_syn_losses',\n",
    "            'Losses',\n",
    "            'Time_per_epoch',\n",
    "            'Total_time'\n",
    "        ]\n",
    "        self.metrics = {field: list() for field in matrix_fields}\n",
    "        early_stop_count = 0\n",
    "        early_stop_patient = len(self.train_loader) * 42\n",
    "        early_stop = False\n",
    "        best_batch_loss = -1\n",
    "        save_model = False\n",
    "        number_of_model_saved = 10\n",
    "        label_fixed_data = [0, 1, 2, 0, 1, 2, 0, 1, 2]\n",
    "        \n",
    "        def get_file_list(file_path, length):\n",
    "            dir_list = os.listdir(file_path)\n",
    "            dir_list = only_pth_file(dir_list)\n",
    "            if not dir_list:\n",
    "                return\n",
    "            else:\n",
    "                dir_list = sorted(dir_list,  key=lambda x: os.path.getmtime(os.path.join(file_path, x)))\n",
    "                print(\"files to be deleted < length =\", length ,\"> ->\", dir_list[0:length])\n",
    "                return dir_list[0:length]\n",
    "\n",
    "        def only_pth_file(file_list):\n",
    "            new_list = []\n",
    "            for file in file_list:\n",
    "                if file[-4:] == \".pth\":\n",
    "                    new_list.append(file)\n",
    "            return new_list\n",
    "            \n",
    "        start_time = time.time()\n",
    "        print(\"Start Testing ...\")\n",
    "        print(self.generator)\n",
    "        print(self.discriminator)\n",
    "        summary(self.generator, [[self.size_of_z], [self.num_of_ch]], batch_size=16, device=device)\n",
    "        summary(self.discriminator, (self.num_of_ch, self.img_size, self.img_size), batch_size=16, device=device)\n",
    "        if self.save_network_image:\n",
    "            self.modeltorchviz(self.generator, torch.randn(16, self.size_of_z).requires_grad_(True), \n",
    "                          torch.randn(16, self.num_of_label).requires_grad_(True))\n",
    "            self.modeltorchviz(self.discriminator, torch.randn(16, self.num_of_ch, self.img_size, self.img_size).requires_grad_(True))\n",
    "        g_output = self.generator(torch.rand((self.batch_size, self.size_of_z)).to(device), \n",
    "                                  torch.ones((self.batch_size, self.num_of_label)).to(device))\n",
    "        print(g_output.shape)\n",
    "        s_output, c_output = self.discriminator(torch.rand(g_output.shape).to(device))\n",
    "        print(s_output.shape)\n",
    "        print(c_output.shape)\n",
    "        del g_output, s_output, c_output\n",
    "        print(\"Testing Done in ->\", time.time() - start_time)\n",
    "        start_time = time.time()\n",
    "        print(\"Start Training ...\")\n",
    "\n",
    "        self.discriminator.train()\n",
    "        for epoch in range(self.num_epoch):\n",
    "            epoch += 1\n",
    "            log_in_data_loader = []\n",
    "            self.generator.train()\n",
    "            epoch_start_time = time.time()\n",
    "            for i, data in enumerate(tqdm(self.train_loader, 0)):\n",
    "                x_, y_ = data\n",
    "                batch_size_x_ = x_.size(0)\n",
    "                z_ = torch.rand((batch_size_x_, self.size_of_z))\n",
    "                y_vec_ = torch.zeros((batch_size_x_, self.num_of_label)).scatter_(1, y_.type(torch.LongTensor).unsqueeze(1), 1)\n",
    "                x_, z_, y_vec_ = x_.cuda(), z_.cuda(), y_vec_.cuda()\n",
    "                self.y_real_, self.y_fake_ = torch.ones(batch_size_x_, 1), torch.zeros(batch_size_x_, 1)\n",
    "                self.y_real_, self.y_fake_ = self.y_real_.to(device), self.y_fake_.to(device)\n",
    "        \n",
    "                # update D network\n",
    "                self.optimizer_d.zero_grad()\n",
    "\n",
    "                D_real, C_real = self.discriminator(x_)\n",
    "                D_real_loss = self.syn_criterion(D_real, self.y_real_)\n",
    "                C_real_loss = self.class_criterion(C_real, torch.max(y_vec_, 1)[1])\n",
    "\n",
    "                G_ = self.generator(z_, y_vec_)\n",
    "                D_fake, C_fake = self.discriminator(G_)\n",
    "                D_fake_loss = self.syn_criterion(D_fake, self.y_fake_)\n",
    "                C_fake_loss = self.class_criterion(C_fake, torch.max(y_vec_, 1)[1])\n",
    "\n",
    "                D_loss = D_real_loss + C_real_loss + D_fake_loss + C_fake_loss\n",
    "                self.metrics['D_losses'].append(D_loss.item())\n",
    "                self.metrics['D_real_losses'].append((D_real_loss + C_real_loss).item())\n",
    "                self.metrics['D_fake_losses'].append((D_fake_loss + C_fake_loss).item())\n",
    "                self.metrics['D_class_losses'].append((C_real_loss + C_fake_loss).item())\n",
    "                self.metrics['D_syn_losses'].append((D_real_loss + D_fake_loss).item())\n",
    "\n",
    "                D_loss.backward()\n",
    "                self.optimizer_d.step()\n",
    "\n",
    "                # update G network\n",
    "                self.optimizer_g.zero_grad()\n",
    "\n",
    "                G_ = self.generator(z_, y_vec_)\n",
    "                D_fake, C_fake = self.discriminator(G_)\n",
    "\n",
    "                D_fake_loss = self.syn_criterion(D_fake, self.y_real_)\n",
    "                C_fake_loss = self.class_criterion(C_fake, torch.max(y_vec_, 1)[1])\n",
    "\n",
    "                G_loss = D_fake_loss + C_fake_loss\n",
    "                self.metrics['G_losses'].append(G_loss.item())\n",
    "                self.metrics['G_syn_losses'].append(D_fake_loss.item())\n",
    "                self.metrics['G_class_losses'].append(C_fake_loss.item())\n",
    "                self.metrics['Losses'].append(self.metrics['G_losses'][-1] + self.metrics['D_losses'][-1])\n",
    "\n",
    "                G_loss.backward()\n",
    "                self.optimizer_g.step()\n",
    "\n",
    "                if best_batch_loss < 0:\n",
    "                    best_batch_loss = self.metrics['Losses'][-1]\n",
    "                    early_stop_count = 0\n",
    "                else:\n",
    "                    if best_batch_loss >= self.metrics['Losses'][-1]:\n",
    "                        best_batch_loss = self.metrics['Losses'][-1]\n",
    "                        early_stop_count = 0\n",
    "                        log_in_data_loader.append(\"---------------------<lowest loss update -> \" + str(best_batch_loss) + \" at -> \" + str(i + 1) + \">---------------------\")\n",
    "                        save_model = True\n",
    "                    else:\n",
    "                        early_stop_count += 1\n",
    "                        if early_stop_count >= early_stop_patient:\n",
    "                            log_in_data_loader.append(\"-----------------------------------< early stopping ... >-----------------------------------\")\n",
    "                            early_stop = True\n",
    "\n",
    "                if i % round(len(self.train_loader)/5) == 0:\n",
    "                    vutils.save_image(x_, './GANAug/output_images/ACGAN/real_samples_e' + str(epoch) + '_d' + str(i) + '.jpg', normalize=True)\n",
    "                    fake = self.generator(self.sample_z_fixed, self.sample_y_fixed)\n",
    "                    for j in range(len(fake)):\n",
    "                        vutils.save_image(fake[j].data,\n",
    "                                '%s/fake_samples_epoch_%03d.jpg' % ('./GANAug/output_images/ACGAN/' + self.label_dict[label_fixed_data[j]], epoch), \n",
    "                                          normalize=True)\n",
    "\n",
    "            self.metrics['Time_per_epoch'].append(time.time() - epoch_start_time)\n",
    "            with torch.no_grad():\n",
    "                self.visualize_results(epoch)\n",
    "    \n",
    "            for message in log_in_data_loader:\n",
    "                print(message)\n",
    "            print('[%d/%d] ======================================================================== \\nLoss_D: %.8f, Loss_G: %.8f\\nLoss_S_D: %.8f, Loss_C_D: %.8f, Loss_R_D: %.8f, Loss_F_D: %.8f\\nLoss_S_G: %.8f, Loss_C_G: %.8f'\n",
    "                  % (epoch, self.num_epoch, \n",
    "                     np.mean(self.metrics['D_losses'][-len(self.train_loader):]), \n",
    "                     np.mean(self.metrics['G_losses'][-len(self.train_loader):]),\n",
    "                     np.mean(self.metrics['D_syn_losses'][-len(self.train_loader):]), \n",
    "                     np.mean(self.metrics['D_class_losses'][-len(self.train_loader):]), \n",
    "                     np.mean(self.metrics['D_real_losses'][-len(self.train_loader):]), \n",
    "                     np.mean(self.metrics['D_fake_losses'][-len(self.train_loader):]), \n",
    "                     np.mean(self.metrics['G_syn_losses'][-len(self.train_loader):]),\n",
    "                     np.mean(self.metrics['G_class_losses'][-len(self.train_loader):])))\n",
    "\n",
    "            # do checkpointing\n",
    "            if save_model == True:\n",
    "                save_model = False\n",
    "                torch.save(self.generator.state_dict(), '%s/G_epoch_%d_save_model.pth' % (os.path.join('.', 'GANAug/model/ACGAN'), epoch))\n",
    "                torch.save(self.discriminator.state_dict(), '%s/D_epoch_%d_save_model.pth' % (os.path.join('.', 'GANAug/model/ACGAN'), epoch))\n",
    "            elif early_stop:\n",
    "                torch.save(self.generator.state_dict(), '%s/G_epoch_%d_early_stop.pth' % (os.path.join('.', 'GANAug/model/ACGAN'), epoch))\n",
    "                torch.save(self.discriminator.state_dict(), '%s/D_epoch_%d_early_stop.pth' % (os.path.join('.', 'GANAug/model/ACGAN'), epoch))\n",
    "                break\n",
    "            elif epoch % round(num_epochs/10) == 0:\n",
    "                torch.save(self.generator.state_dict(), '%s/G_epoch_%d.pth' % (os.path.join('.', 'GANAug/model/ACGAN'), epoch))\n",
    "                torch.save(self.discriminator.state_dict(), '%s/D_epoch_%d.pth' % (os.path.join('.', 'GANAug/model/ACGAN'), epoch))\n",
    "            else:\n",
    "                print(\"---------------------< no model saved at epoch:\", epoch, \">---------------------\")\n",
    "\n",
    "            if len(only_pth_file(os.listdir(os.path.join('.', 'GANAug/model/ACGAN')))) > number_of_model_saved:\n",
    "                delete_list = get_file_list(os.path.join('.', 'GANAug/model/ACGAN'), \n",
    "                                            len(only_pth_file(os.listdir(os.path.join('.', 'GANAug/model/ACGAN')))) - number_of_model_saved)\n",
    "                for file in delete_list:\n",
    "                    if os.path.exists(os.path.join(os.path.join('.', 'GANAug/model/ACGAN'), file)):\n",
    "                        os.remove(os.path.join(os.path.join('.', 'GANAug/model/ACGAN'), file))\n",
    "                    else:\n",
    "                        print(\"file ->\", os.path.join(os.path.join('.', 'GANAug/model/ACGAN'), file), \"does not exist\")\n",
    "\n",
    "        self.metrics['Total_time'].append(time.time() - start_time)\n",
    "        print(\"Average epoch time: %.2f, total %d epochs time: %.2f\" % (np.mean(self.metrics['Time_per_epoch']),\n",
    "                                                                        self.num_epoch, self.metrics['Total_time'][0]))\n",
    "        generate_animation(os.path.join('.', 'GANAug/model/ACGAN/') + self.model_name, self.num_epoch)\n",
    "        loss_plot(self.metrics, os.path.join('.', 'GANAug/plots/ACGAN'), self.model_name + \"_loss_plot\")\n",
    "        print(\"Training Done !\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "11433097",
   "metadata": {},
   "outputs": [],
   "source": [
    "acgan = ACGAN()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "53308270",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[WinError 183] 当文件已存在时，无法创建该文件。: '.\\\\GANAug'\n",
      "[WinError 183] 当文件已存在时，无法创建该文件。: '.\\\\GANAug/model'\n",
      "[WinError 183] 当文件已存在时，无法创建该文件。: '.\\\\GANAug/plots'\n",
      "[WinError 183] 当文件已存在时，无法创建该文件。: '.\\\\GANAug/model/ACGAN'\n",
      "[WinError 183] 当文件已存在时，无法创建该文件。: '.\\\\GANAug/plots/ACGAN'\n",
      "[WinError 183] 当文件已存在时，无法创建该文件。: '.\\\\GANAug/output_images'\n",
      "[WinError 183] 当文件已存在时，无法创建该文件。: '.\\\\GANAug/output_images/ACGAN'\n",
      "[WinError 183] 当文件已存在时，无法创建该文件。: '.\\\\GANAug/output_images/ACGAN/Covid'\n",
      "[WinError 183] 当文件已存在时，无法创建该文件。: '.\\\\GANAug/output_images/ACGAN/Normal'\n",
      "[WinError 183] 当文件已存在时，无法创建该文件。: '.\\\\GANAug/output_images/ACGAN/Viral Pneumonia'\n",
      "Start Testing ...\n",
      "Generator(\n",
      "  (fc): Sequential(\n",
      "    (0): Linear(in_features=259, out_features=1024, bias=True)\n",
      "    (1): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU()\n",
      "    (3): Linear(in_features=1024, out_features=524288, bias=True)\n",
      "    (4): BatchNorm1d(524288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (5): ReLU()\n",
      "  )\n",
      "  (deconv): Sequential(\n",
      "    (0): ConvTranspose2d(128, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU()\n",
      "    (3): ConvTranspose2d(64, 3, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "    (4): Tanh()\n",
      "  )\n",
      ")\n",
      "Discriminator(\n",
      "  (conv): Sequential(\n",
      "    (0): Conv2d(3, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "    (1): LeakyReLU(negative_slope=0.2)\n",
      "    (2): Conv2d(64, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "    (3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (4): LeakyReLU(negative_slope=0.2)\n",
      "  )\n",
      "  (fc1): Sequential(\n",
      "    (0): Linear(in_features=524288, out_features=1024, bias=True)\n",
      "    (1): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): LeakyReLU(negative_slope=0.2)\n",
      "  )\n",
      "  (dc): Sequential(\n",
      "    (0): Linear(in_features=1024, out_features=1, bias=True)\n",
      "    (1): Sigmoid()\n",
      "  )\n",
      "  (cl): Sequential(\n",
      "    (0): Linear(in_features=1024, out_features=3, bias=True)\n",
      "  )\n",
      ")\n",
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Linear-1                 [16, 1024]         266,240\n",
      "       BatchNorm1d-2                 [16, 1024]           2,048\n",
      "              ReLU-3                 [16, 1024]               0\n",
      "            Linear-4               [16, 524288]     537,395,200\n",
      "       BatchNorm1d-5               [16, 524288]       1,048,576\n",
      "              ReLU-6               [16, 524288]               0\n",
      "   ConvTranspose2d-7         [16, 64, 128, 128]         131,136\n",
      "       BatchNorm2d-8         [16, 64, 128, 128]             128\n",
      "              ReLU-9         [16, 64, 128, 128]               0\n",
      "  ConvTranspose2d-10          [16, 3, 256, 256]           3,075\n",
      "             Tanh-11          [16, 3, 256, 256]               0\n",
      "================================================================\n",
      "Total params: 538,846,403\n",
      "Trainable params: 538,846,403\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.05\n",
      "Forward/backward pass size (MB): 624.38\n",
      "Params size (MB): 2055.54\n",
      "Estimated Total Size (MB): 2679.96\n",
      "----------------------------------------------------------------\n",
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1         [16, 64, 128, 128]           3,136\n",
      "         LeakyReLU-2         [16, 64, 128, 128]               0\n",
      "            Conv2d-3          [16, 128, 64, 64]         131,200\n",
      "       BatchNorm2d-4          [16, 128, 64, 64]             256\n",
      "         LeakyReLU-5          [16, 128, 64, 64]               0\n",
      "            Linear-6                 [16, 1024]     536,871,936\n",
      "       BatchNorm1d-7                 [16, 1024]           2,048\n",
      "         LeakyReLU-8                 [16, 1024]               0\n",
      "            Linear-9                    [16, 1]           1,025\n",
      "          Sigmoid-10                    [16, 1]               0\n",
      "           Linear-11                    [16, 3]           3,075\n",
      "================================================================\n",
      "Total params: 537,012,676\n",
      "Trainable params: 537,012,676\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 12.00\n",
      "Forward/backward pass size (MB): 448.38\n",
      "Params size (MB): 2048.54\n",
      "Estimated Total Size (MB): 2508.92\n",
      "----------------------------------------------------------------\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA out of memory. Tried to allocate 512.00 MiB (GPU 0; 6.00 GiB total capacity; 5.02 GiB already allocated; 0 bytes free; 5.02 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Input \u001b[1;32mIn [10]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43macgan\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[1;32mIn [8]\u001b[0m, in \u001b[0;36mACGAN.train\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    184\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodeltorchviz(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgenerator, torch\u001b[38;5;241m.\u001b[39mrandn(\u001b[38;5;241m16\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msize_of_z)\u001b[38;5;241m.\u001b[39mrequires_grad_(\u001b[38;5;28;01mTrue\u001b[39;00m), \n\u001b[0;32m    185\u001b[0m                   torch\u001b[38;5;241m.\u001b[39mrandn(\u001b[38;5;241m16\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_of_label)\u001b[38;5;241m.\u001b[39mrequires_grad_(\u001b[38;5;28;01mTrue\u001b[39;00m))\n\u001b[0;32m    186\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodeltorchviz(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdiscriminator, torch\u001b[38;5;241m.\u001b[39mrandn(\u001b[38;5;241m16\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_of_ch, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mimg_size, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mimg_size)\u001b[38;5;241m.\u001b[39mrequires_grad_(\u001b[38;5;28;01mTrue\u001b[39;00m))\n\u001b[1;32m--> 187\u001b[0m g_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerator\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrand\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msize_of_z\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m    188\u001b[0m \u001b[43m                          \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mones\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnum_of_label\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    189\u001b[0m \u001b[38;5;28mprint\u001b[39m(g_output\u001b[38;5;241m.\u001b[39mshape)\n\u001b[0;32m    190\u001b[0m s_output, c_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdiscriminator(torch\u001b[38;5;241m.\u001b[39mrand(g_output\u001b[38;5;241m.\u001b[39mshape)\u001b[38;5;241m.\u001b[39mto(device))\n",
      "File \u001b[1;32mD:\\Miniconda3\\envs\\torch_1-11\\lib\\site-packages\\torch\\nn\\modules\\module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1106\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1107\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1108\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1109\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1110\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1111\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1112\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Input \u001b[1;32mIn [6]\u001b[0m, in \u001b[0;36mGenerator.forward\u001b[1;34m(self, input, label)\u001b[0m\n\u001b[0;32m     42\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc(x)\n\u001b[0;32m     43\u001b[0m x \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m128\u001b[39m, (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_size \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m \u001b[38;5;241m4\u001b[39m), (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_size \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m \u001b[38;5;241m4\u001b[39m))\n\u001b[1;32m---> 44\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdeconv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     46\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "File \u001b[1;32mD:\\Miniconda3\\envs\\torch_1-11\\lib\\site-packages\\torch\\nn\\modules\\module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1106\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1107\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1108\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1109\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1110\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1111\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1112\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32mD:\\Miniconda3\\envs\\torch_1-11\\lib\\site-packages\\torch\\nn\\modules\\container.py:141\u001b[0m, in \u001b[0;36mSequential.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    139\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[0;32m    140\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[1;32m--> 141\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    142\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[1;32mD:\\Miniconda3\\envs\\torch_1-11\\lib\\site-packages\\torch\\nn\\modules\\module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1106\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1107\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1108\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1109\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1110\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1111\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1112\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32mD:\\Miniconda3\\envs\\torch_1-11\\lib\\site-packages\\torch\\nn\\modules\\batchnorm.py:168\u001b[0m, in \u001b[0;36m_BatchNorm.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    161\u001b[0m     bn_training \u001b[38;5;241m=\u001b[39m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrunning_mean \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;129;01mand\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrunning_var \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m    163\u001b[0m \u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    164\u001b[0m \u001b[38;5;124;03mBuffers are only updated if they are to be tracked and we are in training mode. Thus they only need to be\u001b[39;00m\n\u001b[0;32m    165\u001b[0m \u001b[38;5;124;03mpassed when the update should occur (i.e. in training mode when they are tracked), or when buffer stats are\u001b[39;00m\n\u001b[0;32m    166\u001b[0m \u001b[38;5;124;03mused for normalization (i.e. in eval mode when buffers are not None).\u001b[39;00m\n\u001b[0;32m    167\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m--> 168\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbatch_norm\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    169\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    170\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# If buffers are not to be tracked, ensure that they won't be updated\u001b[39;49;00m\n\u001b[0;32m    171\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrunning_mean\u001b[49m\n\u001b[0;32m    172\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrack_running_stats\u001b[49m\n\u001b[0;32m    173\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    174\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrunning_var\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrack_running_stats\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    175\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    176\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    177\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbn_training\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    178\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexponential_average_factor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    179\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    180\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mD:\\Miniconda3\\envs\\torch_1-11\\lib\\site-packages\\torch\\nn\\functional.py:2421\u001b[0m, in \u001b[0;36mbatch_norm\u001b[1;34m(input, running_mean, running_var, weight, bias, training, momentum, eps)\u001b[0m\n\u001b[0;32m   2418\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m training:\n\u001b[0;32m   2419\u001b[0m     _verify_batch_size(\u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39msize())\n\u001b[1;32m-> 2421\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbatch_norm\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2422\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrunning_mean\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrunning_var\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtraining\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmomentum\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackends\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcudnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menabled\u001b[49m\n\u001b[0;32m   2423\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 512.00 MiB (GPU 0; 6.00 GiB total capacity; 5.02 GiB already allocated; 0 bytes free; 5.02 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "acgan.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09e18531",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_gan(generator, discriminator, num_epochs, metrics, loader):\n",
    "    now = datetime.datetime.now()\n",
    "    g_losses = metrics['G_losses'][-1]\n",
    "    d_losses = metrics['D_losses'][-1]\n",
    "    path='GANAug/output_images/ACGAN'\n",
    "    try:\n",
    "        os.mkdir(os.path.join('.', path))\n",
    "    except Exception as error:\n",
    "        print(error)\n",
    "\n",
    "    real_batch = next(iter(loader))\n",
    "    \n",
    "    test_img_list = []\n",
    "    test_noise = torch.randn(batch_size, nz, device=device)\n",
    "    test_label = torch.randn(batch_size, nb_label, device=device)\n",
    "    test_fake = generator(test_noise, test_label).detach().cpu()\n",
    "    test_img_list.append(vutils.make_grid(test_fake, padding=2, normalize=True))\n",
    "\n",
    "    fig = plt.figure(figsize=(20, 20))\n",
    "    ax1 = plt.subplot(1,2,1)\n",
    "    ax1 = plt.axis(\"off\")\n",
    "    ax1 = plt.title(\"Real Images\")\n",
    "    ax1 = plt.imshow(np.transpose(vutils.make_grid(real_batch[0].to(device)[:64], padding=5, normalize=True).cpu(),(1,2,0)))\n",
    "\n",
    "    ax2 = plt.subplot(1,2,2)\n",
    "    ax2 = plt.axis(\"off\")\n",
    "    ax2 = plt.title(\"Fake Images\")\n",
    "    ax2 = plt.imshow(np.transpose(test_img_list[-1],(1,2,0)))\n",
    "    plt.show()\n",
    "    fig.savefig('%s/image_%.3f_%.3f_%d_%s.png' %\n",
    "                   (path, g_losses, d_losses, num_epochs, now.strftime(\"%Y-%m-%d_%H:%M:%S\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03814c7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_gan(acgan.generator, acgan.discriminator, acgan.num_epochs, acgan.metrics, acgan.train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9cca5ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_gan(name, train_epoch, values, path, save):\n",
    "    clear_output(wait=True)\n",
    "    plt.close('all')\n",
    "    fig = plt.figure()\n",
    "    fig = plt.ion()\n",
    "    fig = plt.subplot(1, 1, 1)\n",
    "    fig = plt.title('epoch: %s -> %s: %s' % (train_epoch, name, values[-1]))\n",
    "    fig = plt.ylabel(name)\n",
    "    fig = plt.xlabel('train_set')\n",
    "    fig = plt.plot(values)\n",
    "    fig = plt.grid()\n",
    "    get_fig = plt.gcf()\n",
    "    fig = plt.draw()  # draw the plot\n",
    "    fig = plt.pause(1)  # show it for 1 second\n",
    "    plt.show()\n",
    "    if save:\n",
    "        now = datetime.datetime.now()\n",
    "        get_fig.savefig('%s/%s_%.3f_%d_%s.png' %\n",
    "                        (path, name, train_epoch, values[-1], now.strftime(\"%Y-%m-%d_%H:%M:%S\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d5e7892",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model(generator, discriminator, gen_optimizer, dis_optimizer, metrics, num_epochs):\n",
    "    now = datetime.datetime.now()\n",
    "    g_losses = metrics['G_losses'][-1]\n",
    "    d_losses = metrics['D_losses'][-1]\n",
    "    path='GANAug/plots/ACGAN/train_%+.3f_%+.3f_%s'% (g_losses, d_losses, now.strftime(\"%Y-%m-%d_%H-%M-%S\"))\n",
    "    try:\n",
    "        os.mkdir(os.path.join('.', path))\n",
    "    except Exception as error:\n",
    "        print(error)\n",
    "    plot_gan('G_losses', num_epochs, metrics['G_losses'], path, True)\n",
    "    plot_gan('D_losses', num_epochs, metrics['D_losses'], path, True)\n",
    "    plot_gan('G_syn_losses', num_epochs, metrics['G_syn_losses'], path, True)\n",
    "    plot_gan('G_class_losses', num_epochs, metrics['G_class_losses'], path, True)\n",
    "    plot_gan('G_syn_losses', num_epochs, metrics['G_syn_losses'], path, True)\n",
    "    plot_gan('D_class_losses', num_epochs, metrics['D_class_losses'], path, True)\n",
    "    plot_gan('D_syn_losses', num_epochs, metrics['D_syn_losses'], path, True)\n",
    "    plot_gan('Losses', num_epochs, metrics['Losses'], path, True)\n",
    "    plot_gan('Accuracy', num_epochs, metrics['Accuracy'], path, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f66ad873",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_model(acgan.generator, acgan.discriminator, acgan.optimizer_g, acgan.optimizer_d, acgan.metrics, acgan.epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c56cd9c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_img_list = []\n",
    "test_noise = torch.randn(batch_size, nz, device=device)\n",
    "test_label = torch.randn(batch_size, nb_label, device=device)\n",
    "test_img = acgan.generator(test_noise, test_label)\n",
    "\n",
    "s_output, c_label_op = acgan.discriminator(test_img.detach().to(device))\n",
    "print('Discriminator s', s_output)\n",
    "print('Discriminator c', c_label_op)\n",
    "\n",
    "test_img = test_img.detach().cpu()\n",
    "test_img_list.append(vutils.make_grid(test_img, padding=2, normalize=True))\n",
    "plt.imshow(np.transpose(test_img_list[-1],(1,2,0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d45fe52a",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = next(iter(test_loader))\n",
    "test_noise, test_class_lable = data\n",
    "test_img = test_noise\n",
    "print('class label for real', test_class_lable)\n",
    "\n",
    "s_output,c_label_op = acgan.discriminator(test_img.detach().to(device))\n",
    "print('Discriminator s', s_output)\n",
    "print('Discriminator c', c_label_op)\n",
    "\n",
    "test_img = test_img.detach().cpu()\n",
    "test_img_list.append(vutils.make_grid(test_img, padding=2, normalize=True))\n",
    "plt.imshow(np.transpose(test_img_list[-1],(1,2,0)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
