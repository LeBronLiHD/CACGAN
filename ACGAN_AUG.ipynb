{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "370513dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: http://mirrors.aliyun.com/pypi/simple\n",
      "Requirement already satisfied: opencv-python in /root/miniconda3/lib/python3.8/site-packages (4.5.5.64)\n",
      "Requirement already satisfied: torchsummary in /root/miniconda3/lib/python3.8/site-packages (1.5.1)\n",
      "Requirement already satisfied: scikit-learn in /root/miniconda3/lib/python3.8/site-packages (1.1.1)\n",
      "Requirement already satisfied: torchviz in /root/miniconda3/lib/python3.8/site-packages (0.0.2)\n",
      "Requirement already satisfied: utils in /root/miniconda3/lib/python3.8/site-packages (1.0.1)\n",
      "Requirement already satisfied: numpy>=1.17.3 in /root/miniconda3/lib/python3.8/site-packages (from opencv-python) (1.21.4)\n",
      "Requirement already satisfied: scipy>=1.3.2 in /root/miniconda3/lib/python3.8/site-packages (from scikit-learn) (1.8.1)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /root/miniconda3/lib/python3.8/site-packages (from scikit-learn) (3.1.0)\n",
      "Requirement already satisfied: joblib>=1.0.0 in /root/miniconda3/lib/python3.8/site-packages (from scikit-learn) (1.1.0)\n",
      "Requirement already satisfied: torch in /root/miniconda3/lib/python3.8/site-packages (from torchviz) (1.10.0+cu113)\n",
      "Requirement already satisfied: graphviz in /root/miniconda3/lib/python3.8/site-packages (from torchviz) (0.20)\n",
      "Requirement already satisfied: typing-extensions in /root/miniconda3/lib/python3.8/site-packages (from torch->torchviz) (4.0.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install opencv-python torchsummary scikit-learn torchviz utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6a6a2202",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.parallel\n",
    "import torch.nn.functional as F\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.optim as optim\n",
    "import torch.utils.data\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.utils as vutils\n",
    "import utils\n",
    "from torch.nn.functional import one_hot\n",
    "from torchvision.utils import save_image\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import SubsetRandomSampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6cf2ef10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['sm_37', 'sm_50', 'sm_60', 'sm_70', 'sm_75', 'sm_80', 'sm_86'] cuda\n"
     ]
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(torch.cuda.get_arch_list(), device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b8079c45",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import argparse\n",
    "import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import HTML\n",
    "from IPython.display import clear_output\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "from tqdm import notebook\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import gc\n",
    "import matplotlib.colors as mat_color\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import random\n",
    "import cv2\n",
    "from torchvision.datasets import ImageNet, ImageFolder\n",
    "from torch.utils.data import DataLoader\n",
    "from torchsummary import summary\n",
    "from torchviz import make_dot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3eb59879",
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_weights(net):\n",
    "    for m in net.modules():\n",
    "        if isinstance(m, nn.Conv2d):\n",
    "            m.weight.data.normal_(0, 0.02)\n",
    "            m.bias.data.zero_()\n",
    "        elif isinstance(m, nn.ConvTranspose2d):\n",
    "            m.weight.data.normal_(0, 0.02)\n",
    "            m.bias.data.zero_()\n",
    "        elif isinstance(m, nn.Linear):\n",
    "            m.weight.data.normal_(0, 0.02)\n",
    "            m.bias.data.zero_()\n",
    "\n",
    "class Generator(nn.Module):\n",
    "    # Network Architecture is exactly same as in infoGAN (https://arxiv.org/abs/1606.03657)\n",
    "    # Architecture : FC1024_BR-FC7x7x128_BR-(64)4dc2s_BR-(1)4dc2s_S\n",
    "    def __init__(self, input_dim=100, output_dim=1, input_size=32, class_num=10):\n",
    "        super(Generator, self).__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.input_size = input_size\n",
    "        self.class_num = class_num\n",
    "\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(self.input_dim + self.class_num, 1024),\n",
    "            nn.BatchNorm1d(1024),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(1024, 128 * (self.input_size // 4) * (self.input_size // 4)),\n",
    "            nn.BatchNorm1d(128 * (self.input_size // 4) * (self.input_size // 4)),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        self.deconv = nn.Sequential(\n",
    "            nn.ConvTranspose2d(128, 64, 4, 2, 1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(64, self.output_dim, 4, 2, 1),\n",
    "            nn.Tanh(),\n",
    "        )\n",
    "        initialize_weights(self)\n",
    "\n",
    "    def forward(self, input, label):\n",
    "        x = torch.cat([input, label], 1)\n",
    "        x = self.fc(x)\n",
    "        x = x.view(-1, 128, (self.input_size // 4), (self.input_size // 4))\n",
    "        x = self.deconv(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "    \n",
    "class Discriminator(nn.Module):\n",
    "    # Network Architecture is exactly same as in infoGAN (https://arxiv.org/abs/1606.03657)\n",
    "    # Architecture : (64)4c2s-(128)4c2s_BL-FC1024_BL-FC1_S\n",
    "    def __init__(self, input_dim=1, output_dim=1, input_size=32, class_num=10):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.input_size = input_size\n",
    "        self.class_num = class_num\n",
    "\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(self.input_dim, 64, 4, 2, 1),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Conv2d(64, 128, 4, 2, 1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.LeakyReLU(0.2),\n",
    "        )\n",
    "        self.fc1 = nn.Sequential(\n",
    "            nn.Linear(128 * (self.input_size // 4) * (self.input_size // 4), 1024),\n",
    "            nn.BatchNorm1d(1024),\n",
    "            nn.LeakyReLU(0.2),\n",
    "        )\n",
    "        self.dc = nn.Sequential(\n",
    "            nn.Linear(1024, self.output_dim),\n",
    "            nn.Sigmoid(),\n",
    "        )\n",
    "        self.cl = nn.Sequential(\n",
    "            nn.Linear(1024, self.class_num),\n",
    "        )\n",
    "        initialize_weights(self)\n",
    "\n",
    "    def forward(self, input):\n",
    "        x = self.conv(input)\n",
    "        x = x.view(-1, 128 * (self.input_size // 4) * (self.input_size // 4))\n",
    "        x = self.fc1(x)\n",
    "        d = self.dc(x)\n",
    "        c = self.cl(x)\n",
    "\n",
    "        return d, c\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "220b9685",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_path = './data'\n",
    "base_folder = \"Covid-19 Image Dataset\"\n",
    "classic_folder = 'Coivd-19_Classic'\n",
    "synthetic_folder = 'Coivd-19_Synthetic'\n",
    "data_dir = os.path.join(base_path, classic_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9b6ea5a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "train_path = os.path.join(data_dir, \"train\")\n",
    "test_path = os.path.join(data_dir, \"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1bf35c4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Covid', 'Normal', 'Viral Pneumonia']\n"
     ]
    }
   ],
   "source": [
    "labels = os.listdir(train_path)\n",
    "print(labels)\n",
    "no_norm = mat_color.Normalize(vmin=0, vmax=255, clip=False)\n",
    "label_dict = {\n",
    "    0 : 'Covid', \n",
    "    1 : 'Normal', \n",
    "    2 : 'Viral Pneumonia'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "28c7ac6b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32])\n",
      "torch.Size([32, 3])\n",
      "torch.Size([32, 256])\n",
      "torch.Size([3, 256])\n",
      "torch.Size([3, 3])\n"
     ]
    }
   ],
   "source": [
    "# Number of training epochs\n",
    "num_epochs = 200\n",
    "#All images will be resized to this size using a transformer.\n",
    "img_size = 128 * 2\n",
    "# Number of channels in the training images. For color images this is 3\n",
    "nc = 3\n",
    "# Size of z latent vector (i.e. size of generator input)\n",
    "nz = img_size\n",
    "# Size of feature maps in generator\n",
    "ngf = img_size\n",
    "# Size of feature maps in discriminator\n",
    "ndf = img_size\n",
    "# No of labels\n",
    "nb_label = len(labels)\n",
    "# Learning rate for optimizers\n",
    "lr = 0.00000010\n",
    "lr_d = 0.00000002\n",
    "# Beta1 hyperparam for Adam optimizers\n",
    "beta1 = 0.5\n",
    "# Beta2 hyperparam for Adam optimizers\n",
    "beta2 = 0.999\n",
    "\n",
    "real_label = 1.\n",
    "fake_label = 0.\n",
    "s_criterion = nn.BCELoss().to(device) #For synthesizing\n",
    "c_criterion = nn.CrossEntropyLoss().to(device) #For classification\n",
    "\n",
    "s_label = torch.FloatTensor(batch_size).to(device)\n",
    "c_label = torch.FloatTensor(batch_size, nb_label).to(device)\n",
    "noise = torch.FloatTensor(batch_size, nz).to(device)\n",
    "\n",
    "s_label = Variable(s_label)\n",
    "c_label = Variable(c_label)\n",
    "print(s_label.shape)\n",
    "print(c_label.shape)\n",
    "print(noise.shape)\n",
    "\n",
    "noise = Variable(noise)\n",
    "\n",
    "noise_fixed = torch.FloatTensor(3, nz).to(device)\n",
    "noise_data = np.random.normal(0, 1, (3, nz))\n",
    "noise_fixed.data.copy_(torch.tensor(noise_data))\n",
    "noise_fixed = Variable(noise_fixed)\n",
    "\n",
    "label_fixed = torch.FloatTensor(3, 3).to(device)\n",
    "label_onehot = np.zeros((3, nb_label))\n",
    "label_fixed_data = [0, 1, 2]\n",
    "label_onehot[np.arange(1), label_fixed_data] = 1\n",
    "label_fixed.data.copy_(torch.tensor(label_onehot))\n",
    "label_fixed = Variable(label_fixed)\n",
    "\n",
    "print(noise_fixed.shape)\n",
    "print(label_fixed.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bd17020a",
   "metadata": {},
   "outputs": [],
   "source": [
    "for func in [\n",
    "    lambda: os.mkdir(os.path.join('.', 'GANAug')),\n",
    "    lambda: os.mkdir(os.path.join('.', 'GANAug/model')),\n",
    "    lambda: os.mkdir(os.path.join('.', 'GANAug/plots')),\n",
    "    lambda: os.mkdir(os.path.join('.', 'GANAug/model/ACGAN')),\n",
    "    lambda: os.mkdir(os.path.join('.', 'GANAug/plots/ACGAN')),\n",
    "    lambda: os.mkdir(os.path.join('.', 'GANAug/output_images')),\n",
    "    lambda: os.mkdir(os.path.join('.', 'GANAug/output_images/ACGAN')),\n",
    "    lambda: os.mkdir(os.path.join('.', 'GANAug/output_images/ACGAN/' + label_dict[0])),\n",
    "    lambda: os.mkdir(os.path.join('.', 'GANAug/output_images/ACGAN/' + label_dict[1])),\n",
    "    lambda: os.mkdir(os.path.join('.', 'GANAug/output_images/ACGAN/' + label_dict[2]))]:  # create directories\n",
    "    try:\n",
    "        func()\n",
    "    except Exception as error:\n",
    "        print(error)\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "74dda24a",
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix_fields = [\n",
    "    'G_losses',\n",
    "    'G_class_losses',\n",
    "    'G_syn_losses',\n",
    "    'D_losses',\n",
    "    'D_real_losses',\n",
    "    'D_fake_losses',\n",
    "    'D_class_losses',\n",
    "    'D_syn_losses',\n",
    "    'Accuracy',\n",
    "    'Losses'\n",
    "]\n",
    "metrics = {field: list() for field in matrix_fields}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a26a75d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset(train_dir=train_path, test_dir=test_path):\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((img_size, img_size)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "        ])\n",
    "    train_data = datasets.ImageFolder(train_dir, transform=transform)\n",
    "    train_loader = DataLoader(train_data, batch_size, shuffle=True, num_workers=1)\n",
    "\n",
    "    test_data = datasets.ImageFolder(test_dir ,transform=transform)\n",
    "    test_loader = DataLoader(test_data, batch_size, shuffle=True, num_workers=1)\n",
    "\n",
    "    return train_loader, test_loader, train_data, test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bbb0b552",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader, test_loader, train_data, test_data = load_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0360680c",
   "metadata": {},
   "outputs": [],
   "source": [
    "generator = Generator(input_dim=nz, output_dim=nc, input_size=ngf, class_num=nb_label).to(device)\n",
    "discriminator = Discriminator(input_dim=nc, output_dim=1, input_size=ndf, class_num=nb_label).to(device)\n",
    "\n",
    "# setup optimizer\n",
    "optimizerD = optim.Adam(discriminator.parameters(), lr=lr_d, betas=(beta1, beta2))\n",
    "optimizerG = optim.Adam(generator.parameters(), lr=lr, betas=(beta1, beta2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8d141b1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 3, 256, 256])\n"
     ]
    }
   ],
   "source": [
    "g_output = generator(torch.rand((batch_size, nz)).to(device), torch.ones((batch_size, nb_label)).to(device))\n",
    "print(g_output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8244b33d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 1])\n",
      "torch.Size([32, 3])\n"
     ]
    }
   ],
   "source": [
    "s_output, c_output = discriminator(torch.rand(g_output.shape).to(device))\n",
    "print(s_output.shape)\n",
    "print(c_output.shape)\n",
    "del g_output, s_output, c_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "065eed00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: http://mirrors.aliyun.com/pypi/simple\n",
      "Requirement already satisfied: torchviz in /root/miniconda3/lib/python3.8/site-packages (0.0.2)\n",
      "Requirement already satisfied: torch in /root/miniconda3/lib/python3.8/site-packages (from torchviz) (1.10.0+cu113)\n",
      "Requirement already satisfied: graphviz in /root/miniconda3/lib/python3.8/site-packages (from torchviz) (0.20)\n",
      "Requirement already satisfied: typing-extensions in /root/miniconda3/lib/python3.8/site-packages (from torch->torchviz) (4.0.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install torchviz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "81981a02",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generator(\n",
      "  (fc): Sequential(\n",
      "    (0): Linear(in_features=259, out_features=1024, bias=True)\n",
      "    (1): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU()\n",
      "    (3): Linear(in_features=1024, out_features=524288, bias=True)\n",
      "    (4): BatchNorm1d(524288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (5): ReLU()\n",
      "  )\n",
      "  (deconv): Sequential(\n",
      "    (0): ConvTranspose2d(128, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU()\n",
      "    (3): ConvTranspose2d(64, 3, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "    (4): Tanh()\n",
      "  )\n",
      ")\n",
      "Discriminator(\n",
      "  (conv): Sequential(\n",
      "    (0): Conv2d(3, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "    (1): LeakyReLU(negative_slope=0.2)\n",
      "    (2): Conv2d(64, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "    (3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (4): LeakyReLU(negative_slope=0.2)\n",
      "  )\n",
      "  (fc1): Sequential(\n",
      "    (0): Linear(in_features=524288, out_features=1024, bias=True)\n",
      "    (1): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): LeakyReLU(negative_slope=0.2)\n",
      "  )\n",
      "  (dc): Sequential(\n",
      "    (0): Linear(in_features=1024, out_features=1, bias=True)\n",
      "    (1): Sigmoid()\n",
      "  )\n",
      "  (cl): Sequential(\n",
      "    (0): Linear(in_features=1024, out_features=3, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(generator)\n",
    "print(discriminator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5e2592f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Linear-1                 [16, 1024]         266,240\n",
      "       BatchNorm1d-2                 [16, 1024]           2,048\n",
      "              ReLU-3                 [16, 1024]               0\n",
      "            Linear-4               [16, 524288]     537,395,200\n",
      "       BatchNorm1d-5               [16, 524288]       1,048,576\n",
      "              ReLU-6               [16, 524288]               0\n",
      "   ConvTranspose2d-7         [16, 64, 128, 128]         131,136\n",
      "       BatchNorm2d-8         [16, 64, 128, 128]             128\n",
      "              ReLU-9         [16, 64, 128, 128]               0\n",
      "  ConvTranspose2d-10          [16, 3, 256, 256]           3,075\n",
      "             Tanh-11          [16, 3, 256, 256]               0\n",
      "================================================================\n",
      "Total params: 538,846,403\n",
      "Trainable params: 538,846,403\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.05\n",
      "Forward/backward pass size (MB): 624.38\n",
      "Params size (MB): 2055.54\n",
      "Estimated Total Size (MB): 2679.96\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "SHOW = True\n",
    "if SHOW:\n",
    "    summary(generator, [[nz], [nc]], batch_size=16, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "57c2031b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1         [16, 64, 128, 128]           3,136\n",
      "         LeakyReLU-2         [16, 64, 128, 128]               0\n",
      "            Conv2d-3          [16, 128, 64, 64]         131,200\n",
      "       BatchNorm2d-4          [16, 128, 64, 64]             256\n",
      "         LeakyReLU-5          [16, 128, 64, 64]               0\n",
      "            Linear-6                 [16, 1024]     536,871,936\n",
      "       BatchNorm1d-7                 [16, 1024]           2,048\n",
      "         LeakyReLU-8                 [16, 1024]               0\n",
      "            Linear-9                    [16, 1]           1,025\n",
      "          Sigmoid-10                    [16, 1]               0\n",
      "           Linear-11                    [16, 3]           3,075\n",
      "================================================================\n",
      "Total params: 537,012,676\n",
      "Trainable params: 537,012,676\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 12.00\n",
      "Forward/backward pass size (MB): 448.38\n",
      "Params size (MB): 2048.54\n",
      "Estimated Total Size (MB): 2508.92\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "if SHOW:\n",
    "    summary(discriminator, (nc, img_size, img_size), batch_size=16, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0580f119",
   "metadata": {},
   "outputs": [],
   "source": [
    "SHOW_IMG = False\n",
    "if SHOW_IMG:\n",
    "    def modeltorchviz(model, input_1, input_2):\n",
    "        if input_2 != None:\n",
    "            y = model(input_1.to(device), input_2.to(device))\n",
    "        else:\n",
    "            y = model(input_1to(device))\n",
    "        if input_2 != None:\n",
    "            MyConvNetVis = make_dot(y, params=dict(list(model.named_parameters()) + [('x', input_1)] + [('x', input_2)]))\n",
    "        else:\n",
    "            MyConvNetVis = make_dot(y, params=dict(list(model.named_parameters()) + [('x', input_1)]))\n",
    "        MyConvNetVis.format = \"png\"\n",
    "        MyConvNetVis.directory = \"images\"\n",
    "        MyConvNetVis.view() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f0cd9edc",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "if SHOW_IMG:\n",
    "    modeltorchviz(generator, torch.randn(16, nz).requires_grad_(True), torch.randn(16, nb_label).requires_grad_(True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d02b9f92",
   "metadata": {},
   "outputs": [],
   "source": [
    "if SHOW_IMG:\n",
    "    modeltorchviz(discriminator, torch.randn(1, nc, ndf, ndf).requires_grad_(True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0d1b6459",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(predict, labels):\n",
    "    correct = 0\n",
    "    predict = torch.argmax(predict, dim=1)\n",
    "    labels = torch.argmax(labels, dim=1)\n",
    "    correct = predict.eq(labels.data).cpu().sum()\n",
    "    return correct, len(labels.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c2f610db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7938\n"
     ]
    }
   ],
   "source": [
    "early_stop_count = 0\n",
    "early_stop_patient = len(train_loader) * 42\n",
    "early_stop = False\n",
    "best_batch_loss = -1\n",
    "save_model = False\n",
    "print(early_stop_patient)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "33a52726",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_file_list(file_path, length):\n",
    "    dir_list = os.listdir(file_path)\n",
    "    dir_list = only_pth_file(dir_list)\n",
    "    if not dir_list:\n",
    "        return\n",
    "    else:\n",
    "        dir_list = sorted(dir_list,  key=lambda x: os.path.getmtime(os.path.join(file_path, x)))\n",
    "        print(\"files to be deleted < length =\", length ,\"> ->\", dir_list[0:length])\n",
    "        return dir_list[0:length]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "8e220d84",
   "metadata": {},
   "outputs": [],
   "source": [
    "def only_pth_file(file_list):\n",
    "    new_list = []\n",
    "    for file in file_list:\n",
    "        if file[-4:] == \".pth\":\n",
    "            new_list.append(file)\n",
    "    return new_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0714f81e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|█▍                                                                                      | 3/189 [00:04<04:32,  1.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------< lowest loss update -> 3.2875759601593018 >---------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|██████████████████████████████████▌                                                    | 75/189 [01:32<02:27,  1.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------< lowest loss update -> 3.272176504135132 >---------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████| 189/189 [03:52<00:00,  1.23s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1/200] ======================== Loss_D: 1.44594955, Loss_G: 1.97420926, Loss_S_D: 1.44594955, \n",
      "Loss_S_G: 0.83177433, Loss_C_D: 2.26282662, Loss_C_G: 1.14243493, Accuracy: 0.33978173\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 11%|█████████▋                                                                             | 21/189 [00:25<03:05,  1.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------< lowest loss update -> 3.252357006072998 >---------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 38%|████████████████████████████████▋                                                      | 71/189 [01:27<02:15,  1.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------< lowest loss update -> 3.2364087104797363 >---------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████| 189/189 [03:52<00:00,  1.23s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2/200] ======================== Loss_D: 1.44529248, Loss_G: 1.96073469, Loss_S_D: 1.44529248, \n",
      "Loss_S_G: 0.83156228, Loss_C_D: 2.15845279, Loss_C_G: 1.12917240, Accuracy: 0.45138890\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 53%|█████████████████████████████████████████████▉                                        | 101/189 [02:02<01:41,  1.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------< lowest loss update -> 3.2250683307647705 >---------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████| 189/189 [03:53<00:00,  1.24s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3/200] ======================== Loss_D: 1.44261373, Loss_G: 1.95369644, Loss_S_D: 1.44261373, \n",
      "Loss_S_G: 0.82997838, Loss_C_D: 2.07397673, Loss_C_G: 1.12371806, Accuracy: 0.55654764\n",
      "files to be deleted < length = 2 > -> ['G_epoch_1_save_model.pth', 'D_epoch_1_save_model.pth']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|████████████████████████████████████████████████████████████                          | 132/189 [02:47<01:16,  1.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------< lowest loss update -> 3.1888527870178223 >---------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████| 189/189 [04:00<00:00,  1.27s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4/200] ======================== Loss_D: 1.44083946, Loss_G: 1.94471928, Loss_S_D: 1.44083946, \n",
      "Loss_S_G: 0.82990065, Loss_C_D: 2.00241334, Loss_C_G: 1.11481863, Accuracy: 0.62880290\n",
      "files to be deleted < length = 2 > -> ['G_epoch_2_save_model.pth', 'D_epoch_2_save_model.pth']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████| 189/189 [04:05<00:00,  1.30s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5/200] ======================== Loss_D: 1.43792827, Loss_G: 1.94066020, Loss_S_D: 1.43792827, \n",
      "Loss_S_G: 0.82789720, Loss_C_D: 1.94205433, Loss_C_G: 1.11276300, Accuracy: 0.68369710\n",
      "---------------------< no model saved at epoch: 5 >---------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████| 189/189 [04:06<00:00,  1.30s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[6/200] ======================== Loss_D: 1.43907144, Loss_G: 1.92886265, Loss_S_D: 1.43907144, \n",
      "Loss_S_G: 0.82827250, Loss_C_D: 1.88533617, Loss_C_G: 1.10059015, Accuracy: 0.72222221\n",
      "---------------------< no model saved at epoch: 6 >---------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████| 189/189 [04:06<00:00,  1.31s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[7/200] ======================== Loss_D: 1.43623152, Loss_G: 1.91358161, Loss_S_D: 1.43623152, \n",
      "Loss_S_G: 0.82733666, Loss_C_D: 1.82594922, Loss_C_G: 1.08624495, Accuracy: 0.75363755\n",
      "---------------------< no model saved at epoch: 7 >---------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 47%|████████████████████████████████████████▌                                              | 88/189 [01:57<02:17,  1.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------< lowest loss update -> 3.184760808944702 >---------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████| 189/189 [04:07<00:00,  1.31s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[8/200] ======================== Loss_D: 1.43566877, Loss_G: 1.90456860, Loss_S_D: 1.43566877, \n",
      "Loss_S_G: 0.82573623, Loss_C_D: 1.78442139, Loss_C_G: 1.07883236, Accuracy: 0.77066797\n",
      "files to be deleted < length = 2 > -> ['G_epoch_3_save_model.pth', 'D_epoch_3_save_model.pth']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|█▍                                                                                      | 3/189 [00:04<04:52,  1.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------< lowest loss update -> 3.177516460418701 >---------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████| 189/189 [04:08<00:00,  1.32s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[9/200] ======================== Loss_D: 1.43219242, Loss_G: 1.90035679, Loss_S_D: 1.43219242, \n",
      "Loss_S_G: 0.82530537, Loss_C_D: 1.74660320, Loss_C_G: 1.07505143, Accuracy: 0.78984791\n",
      "files to be deleted < length = 2 > -> ['G_epoch_4_save_model.pth', 'D_epoch_4_save_model.pth']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 93%|████████████████████████████████████████████████████████████████████████████████      | 176/189 [03:47<00:15,  1.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------< lowest loss update -> 3.1726505756378174 >---------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████| 189/189 [04:04<00:00,  1.29s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10/200] ======================== Loss_D: 1.43188684, Loss_G: 1.89425401, Loss_S_D: 1.43188684, \n",
      "Loss_S_G: 0.82563682, Loss_C_D: 1.71519771, Loss_C_G: 1.06861719, Accuracy: 0.80803573\n",
      "files to be deleted < length = 2 > -> ['G_epoch_8_save_model.pth', 'D_epoch_8_save_model.pth']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████| 189/189 [04:02<00:00,  1.29s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[11/200] ======================== Loss_D: 1.43104121, Loss_G: 1.88653650, Loss_S_D: 1.43104121, \n",
      "Loss_S_G: 0.82464161, Loss_C_D: 1.68440943, Loss_C_G: 1.06189489, Accuracy: 0.81365740\n",
      "---------------------< no model saved at epoch: 11 >---------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|██████████████████████                                                                 | 48/189 [01:04<03:01,  1.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------< lowest loss update -> 3.1690800189971924 >---------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████| 189/189 [04:02<00:00,  1.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------< lowest loss update -> 3.159302234649658 >---------------------\n",
      "[12/200] ======================== Loss_D: 1.43055465, Loss_G: 1.87667721, Loss_S_D: 1.43055465, \n",
      "Loss_S_G: 0.82485360, Loss_C_D: 1.65111330, Loss_C_G: 1.05182362, Accuracy: 0.82242066\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "files to be deleted < length = 2 > -> ['G_epoch_9_save_model.pth', 'D_epoch_9_save_model.pth']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 44%|██████████████████████████████████████▋                                                | 84/189 [01:46<02:13,  1.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------< lowest loss update -> 3.143864631652832 >---------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████| 189/189 [04:02<00:00,  1.28s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13/200] ======================== Loss_D: 1.42990029, Loss_G: 1.86557891, Loss_S_D: 1.42990029, \n",
      "Loss_S_G: 0.82336319, Loss_C_D: 1.62259034, Loss_C_G: 1.04221572, Accuracy: 0.82275134\n",
      "files to be deleted < length = 2 > -> ['G_epoch_10_save_model.pth', 'D_epoch_10_save_model.pth']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 64%|███████████████████████████████████████████████████████                               | 121/189 [02:36<01:23,  1.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------< lowest loss update -> 3.1355767250061035 >---------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████| 189/189 [04:02<00:00,  1.28s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[14/200] ======================== Loss_D: 1.42883120, Loss_G: 1.85468498, Loss_S_D: 1.42883120, \n",
      "Loss_S_G: 0.82126134, Loss_C_D: 1.59713945, Loss_C_G: 1.03342363, Accuracy: 0.83564812\n",
      "files to be deleted < length = 2 > -> ['G_epoch_12_save_model.pth', 'D_epoch_12_save_model.pth']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████| 189/189 [04:01<00:00,  1.28s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[15/200] ======================== Loss_D: 1.42752271, Loss_G: 1.85570974, Loss_S_D: 1.42752271, \n",
      "Loss_S_G: 0.82067890, Loss_C_D: 1.57998658, Loss_C_G: 1.03503084, Accuracy: 0.83349866\n",
      "---------------------< no model saved at epoch: 15 >---------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|█████████████████████████████████████████████████████████▊                            | 127/189 [02:44<01:15,  1.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------< lowest loss update -> 3.1320576667785645 >---------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 69%|███████████████████████████████████████████████████████████▏                          | 130/189 [02:48<01:09,  1.17s/it]"
     ]
    }
   ],
   "source": [
    "for epoch in range(num_epochs):\n",
    "    epoch += 1\n",
    "    for i, data in enumerate(tqdm(train_loader, 0)):\n",
    "        ###########################\n",
    "        # (1) Update D network\n",
    "        ###########################\n",
    "        # train with real\n",
    "        discriminator.zero_grad()\n",
    "        img, label = data\n",
    "        batch_size = img.size(0)\n",
    "        with torch.no_grad():\n",
    "            img, label = img.to(device), label.to(device)\n",
    "            label = one_hot(label, num_classes=nb_label)\n",
    "            s_label.resize_(batch_size).fill_(real_label)\n",
    "            c_label.resize_(batch_size, 3).copy_(label)\n",
    "        s_output, c_output = discriminator(img)\n",
    "        s_errD_real = s_criterion(s_output[:,0], s_label.resize_(batch_size))\n",
    "        c_errD_real = c_criterion(c_output, c_label)\n",
    "        errD_real = s_errD_real + c_errD_real\n",
    "        errD_real.backward()\n",
    "        \n",
    "        correct, length = test(c_output, c_label)\n",
    "\n",
    "        # train with fake\n",
    "        noise_data = np.random.normal(0, 1, (batch_size, nz))\n",
    "        noise.resize_(batch_size, nz).data.copy_(torch.tensor(noise_data))\n",
    "        label = np.random.randint(0, nb_label, batch_size)\n",
    "        noise_ = np.random.normal(0, 1, (batch_size, nz))\n",
    "        label_onehot = np.zeros((batch_size, nb_label))\n",
    "        label_onehot[np.arange(batch_size), label] = 1\n",
    "        c_label.resize_(batch_size, nb_label).data.copy_(torch.tensor(label_onehot))\n",
    "        fake = generator(noise, c_label)\n",
    "        s_label.data.fill_(fake_label)\n",
    "        s_output,c_output = discriminator(fake)\n",
    "        s_errD_fake = s_criterion(s_output[:,0], s_label.resize_(batch_size))\n",
    "        c_errD_fake = c_criterion(c_output, c_label)\n",
    "        errD_fake = s_errD_fake + c_errD_fake\n",
    "        errD_fake.backward()\n",
    "\n",
    "        errD = s_errD_real + s_errD_fake\n",
    "        # errD.backward()\n",
    "        optimizerD.step()\n",
    "\n",
    "        ###########################\n",
    "        # (2) Update G network\n",
    "        ###########################\n",
    "        generator.zero_grad()\n",
    "        s_label.resize_(batch_size, 1).data.fill_(real_label)\n",
    "        fake = generator(noise, c_label)\n",
    "        s_output, c_output = discriminator(fake)\n",
    "        s_errG = s_criterion(s_output[:,0], s_label.resize_(batch_size))\n",
    "        c_errG = c_criterion(c_output, c_label)\n",
    "\n",
    "        errG = s_errG + c_errG\n",
    "        errG.backward()\n",
    "        optimizerG.step()\n",
    "        metrics['G_losses'].append(errG.item())\n",
    "        metrics['G_class_losses'].append(c_errG.item())\n",
    "        metrics['G_syn_losses'].append(s_errG.item())\n",
    "        metrics['D_losses'].append(errD.item())\n",
    "        metrics['D_real_losses'].append(errD_real.item())\n",
    "        metrics['D_fake_losses'].append(errD_fake.item())\n",
    "        metrics['D_class_losses'].append((c_errD_real + c_errD_fake).item())\n",
    "        metrics['D_syn_losses'].append((s_errD_real + s_errD_fake).item())\n",
    "        metrics['Losses'].append((errG + errD).item())\n",
    "        metrics['Accuracy'].append(correct/length)\n",
    "    \n",
    "        if best_batch_loss < 0:\n",
    "            best_batch_loss = (errG + errD).item()\n",
    "            early_stop_count = 0\n",
    "        else:\n",
    "            if best_batch_loss >= (errG + errD).item():\n",
    "                best_batch_loss = (errG + errD).item()\n",
    "                early_stop_count = 0\n",
    "                print(\"---------------------< lowest loss update ->\", best_batch_loss, \">---------------------\")\n",
    "                save_model = True\n",
    "            else:\n",
    "                early_stop_count += 1\n",
    "                if early_stop_count >= early_stop_patient:\n",
    "                    print(\"---------------------< early stopping ... >---------------------\")\n",
    "                    early_stop = True\n",
    "        \n",
    "        if i % round(len(train_loader)/5) == 0:\n",
    "            vutils.save_image(img, './GANAug/output_images/ACGAN/real_samples_e' + str(epoch) + '_d' + str(i) + '.jpg', normalize=True)\n",
    "            fake = generator(noise_fixed, label_fixed)\n",
    "            for j in range(len(fake)):\n",
    "                vutils.save_image(fake[j].data,\n",
    "                        '%s/fake_samples_epoch_%03d.jpg' % ('./GANAug/output_images/ACGAN/' + label_dict[label_fixed_data[j]], epoch), \n",
    "                                  normalize=True)\n",
    "    print('[%d/%d] ======================== Loss_D: %.8f, Loss_G: %.8f, Loss_S_D: %.8f, \\nLoss_S_G: %.8f, Loss_C_D: %.8f, Loss_C_G: %.8f, Accuracy: %.8f'\n",
    "          % (epoch, num_epochs, \n",
    "             np.mean(metrics['D_losses'][-len(train_loader):]), \n",
    "             np.mean(metrics['G_losses'][-len(train_loader):]),\n",
    "             np.mean(metrics['D_syn_losses'][-len(train_loader):]), \n",
    "             np.mean(metrics['G_syn_losses'][-len(train_loader):]),\n",
    "             np.mean(metrics['D_class_losses'][-len(train_loader):]), \n",
    "             np.mean(metrics['G_class_losses'][-len(train_loader):]),\n",
    "             np.mean(metrics['Accuracy'][-len(train_loader):])))\n",
    "\n",
    "    # do checkpointing\n",
    "    if save_model == True:\n",
    "        save_model = False\n",
    "        torch.save(generator.state_dict(), '%s/G_epoch_%d_save_model.pth' % (os.path.join('.', 'GANAug/model/ACGAN'), epoch))\n",
    "        torch.save(discriminator.state_dict(), '%s/D_epoch_%d_save_model.pth' % (os.path.join('.', 'GANAug/model/ACGAN'), epoch))\n",
    "    elif early_stop:\n",
    "        torch.save(generator.state_dict(), '%s/G_epoch_%d_early_stop.pth' % (os.path.join('.', 'GANAug/model/ACGAN'), epoch))\n",
    "        torch.save(discriminator.state_dict(), '%s/D_epoch_%d_early_stop.pth' % (os.path.join('.', 'GANAug/model/ACGAN'), epoch))\n",
    "        break\n",
    "    elif epoch % round(num_epochs/10) == 0:\n",
    "        torch.save(generator.state_dict(), '%s/G_epoch_%d.pth' % (os.path.join('.', 'GANAug/model/ACGAN'), epoch))\n",
    "        torch.save(discriminator.state_dict(), '%s/D_epoch_%d.pth' % (os.path.join('.', 'GANAug/model/ACGAN'), epoch))\n",
    "    else:\n",
    "        print(\"---------------------< no model saved at epoch:\", epoch, \">---------------------\")\n",
    "    \n",
    "    if len(only_pth_file(os.listdir(os.path.join('.', 'GANAug/model/ACGAN')))) > 4:\n",
    "        delete_list = get_file_list(os.path.join('.', 'GANAug/model/ACGAN'), \n",
    "                                    len(only_pth_file(os.listdir(os.path.join('.', 'GANAug/model/ACGAN')))) - 4)\n",
    "        for file in delete_list:\n",
    "            if os.path.exists(os.path.join(os.path.join('.', 'GANAug/model/ACGAN'), file)):\n",
    "                os.remove(os.path.join(os.path.join('.', 'GANAug/model/ACGAN'), file))\n",
    "            else:\n",
    "                print(\"file ->\", os.path.join(os.path.join('.', 'GANAug/model/ACGAN'), file), \"does not exist\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b6d9c4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tr_plot(tr_data, start_epoch):\n",
    "    Epoch_count=len(metrics['G_losses']) + start_epoch\n",
    "    Epochs=[i + 1 for i in range (start_epoch ,Epoch_count)]  \n",
    "    plt.figure(figsize=(20, 20))\n",
    "    plt.style.use('fivethirtyeight')\n",
    "    \n",
    "    plt.subplot(2, 2, 1)\n",
    "    plt.plot(Epochs, metrics['G_losses'], 'r', label='G_losses')\n",
    "    plt.plot(Epochs, metrics['D_losses'], 'g',label='D_losses' )\n",
    "    plt.title('G&D_losses')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.subplot(2, 2, 2)\n",
    "    plt.plot(Epochs, metrics['G_class_losses'], 'r', label='G_class_losses')\n",
    "    plt.plot(Epochs, metrics['G_syn_losses'], 'g',label='G_syn_losses' )\n",
    "    plt.title('G_class&syn_losses')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "\n",
    "    plt.subplot(2, 2, 3)\n",
    "    plt.plot(Epochs, metrics['D_class_losses'], 'r', label='D_class_losses')\n",
    "    plt.plot(Epochs, metrics['D_syn_losses'], 'g',label='D_syn_losses' )\n",
    "    plt.title('D_class&syn_losses')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "\n",
    "    plt.subplot(2, 2, 4)\n",
    "    plt.plot(Epochs, metrics['D_real_losses'], 'r', label='D_real_losses')\n",
    "    plt.plot(Epochs, metrics['D_fake_losses'], 'g',label='D_fake_losses' )\n",
    "    plt.title('D_real&fake_losses')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.tight_layout\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "235fede4",
   "metadata": {},
   "outputs": [],
   "source": [
    "tr_plot(metrics, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ec852c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tr_plot(tr_data, start_epoch):\n",
    "    Epoch_count=len(metrics['Losses']) + start_epoch\n",
    "    Epochs=[i + 1 for i in range (start_epoch ,Epoch_count)]  \n",
    "    plt.figure(figsize=(20, 10))\n",
    "    plt.style.use('fivethirtyeight')\n",
    "    \n",
    "    plt.subplot(2, 2, 1)\n",
    "    plt.plot(Epochs, metrics['Losses'], 'blue', label='Loss')\n",
    "    plt.title('Losses')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.subplot(2, 2, 2)\n",
    "    plt.plot(Epochs, metrics['Accuracy'], 'orange', label='Accuracy')\n",
    "    plt.title('Accuracy')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.tight_layout\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44c508b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "tr_plot(metrics, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09e18531",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_gan(generator, discriminator, num_epochs, metrics, loader):\n",
    "    now = datetime.datetime.now()\n",
    "    g_losses = metrics['G_losses'][-1]\n",
    "    d_losses = metrics['D_losses'][-1]\n",
    "    path='GANAug/output_images/ACGAN'\n",
    "    try:\n",
    "        os.mkdir(os.path.join('.', path))\n",
    "    except Exception as error:\n",
    "        print(error)\n",
    "\n",
    "    real_batch = next(iter(loader))\n",
    "    \n",
    "    test_img_list = []\n",
    "    test_noise = torch.randn(batch_size, nz, device=device)\n",
    "    test_label = torch.randn(batch_size, nb_label, device=device)\n",
    "    test_fake = generator(test_noise, test_label).detach().cpu()\n",
    "    test_img_list.append(vutils.make_grid(test_fake, padding=2, normalize=True))\n",
    "\n",
    "    fig = plt.figure(figsize=(20, 20))\n",
    "    ax1 = plt.subplot(1,2,1)\n",
    "    ax1 = plt.axis(\"off\")\n",
    "    ax1 = plt.title(\"Real Images\")\n",
    "    ax1 = plt.imshow(np.transpose(vutils.make_grid(real_batch[0].to(device)[:64], padding=5, normalize=True).cpu(),(1,2,0)))\n",
    "\n",
    "    ax2 = plt.subplot(1,2,2)\n",
    "    ax2 = plt.axis(\"off\")\n",
    "    ax2 = plt.title(\"Fake Images\")\n",
    "    ax2 = plt.imshow(np.transpose(test_img_list[-1],(1,2,0)))\n",
    "    plt.show()\n",
    "    fig.savefig('%s/image_%.3f_%.3f_%d_%s.png' %\n",
    "                   (path, g_losses, d_losses, num_epochs, now.strftime(\"%Y-%m-%d_%H:%M:%S\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03814c7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_gan(generator, discriminator, num_epochs, metrics, train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9cca5ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_gan(name, train_epoch, values, path, save):\n",
    "    clear_output(wait=True)\n",
    "    plt.close('all')\n",
    "    fig = plt.figure()\n",
    "    fig = plt.ion()\n",
    "    fig = plt.subplot(1, 1, 1)\n",
    "    fig = plt.title('epoch: %s -> %s: %s' % (train_epoch, name, values[-1]))\n",
    "    fig = plt.ylabel(name)\n",
    "    fig = plt.xlabel('train_set')\n",
    "    fig = plt.plot(values)\n",
    "    fig = plt.grid()\n",
    "    get_fig = plt.gcf()\n",
    "    fig = plt.draw()  # draw the plot\n",
    "    fig = plt.pause(1)  # show it for 1 second\n",
    "    plt.show()\n",
    "    if save:\n",
    "        now = datetime.datetime.now()\n",
    "        get_fig.savefig('%s/%s_%.3f_%d_%s.png' %\n",
    "                        (path, name, train_epoch, values[-1], now.strftime(\"%Y-%m-%d_%H:%M:%S\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d5e7892",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model(generator, discriminator, gen_optimizer, dis_optimizer, metrics, num_epochs):\n",
    "    now = datetime.datetime.now()\n",
    "    g_losses = metrics['G_losses'][-1]\n",
    "    d_losses = metrics['D_losses'][-1]\n",
    "    path='GANAug/plots/ACGAN/train_%+.3f_%+.3f_%s'% (g_losses, d_losses, now.strftime(\"%Y-%m-%d_%H-%M-%S\"))\n",
    "    try:\n",
    "        os.mkdir(os.path.join('.', path))\n",
    "    except Exception as error:\n",
    "        print(error)\n",
    "    plot_gan('G_losses', num_epochs, metrics['G_losses'], path, True)\n",
    "    plot_gan('D_losses', num_epochs, metrics['D_losses'], path, True)\n",
    "    plot_gan('G_syn_losses', num_epochs, metrics['G_syn_losses'], path, True)\n",
    "    plot_gan('G_class_losses', num_epochs, metrics['G_class_losses'], path, True)\n",
    "    plot_gan('G_syn_losses', num_epochs, metrics['G_syn_losses'], path, True)\n",
    "    plot_gan('D_class_losses', num_epochs, metrics['D_class_losses'], path, True)\n",
    "    plot_gan('D_syn_losses', num_epochs, metrics['D_syn_losses'], path, True)\n",
    "    plot_gan('Losses', num_epochs, metrics['Losses'], path, True)\n",
    "    plot_gan('Accuracy', num_epochs, metrics['Accuracy'], path, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f66ad873",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_model(generator, discriminator, optimizerG, optimizerD, metrics, num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c56cd9c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_img_list = []\n",
    "test_noise = torch.randn(batch_size, nz, device=device)\n",
    "test_label = torch.randn(batch_size, nb_label, device=device)\n",
    "test_img = generator(test_noise, test_label)\n",
    "\n",
    "s_output, c_label_op = discriminator(test_img.detach().to(device))\n",
    "print('Discriminator s', s_output)\n",
    "print('Discriminator c', c_label_op)\n",
    "\n",
    "test_img = test_img.detach().cpu()\n",
    "test_img_list.append(vutils.make_grid(test_img, padding=2, normalize=True))\n",
    "plt.imshow(np.transpose(test_img_list[-1],(1,2,0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d45fe52a",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = next(iter(test_loader))\n",
    "test_noise, test_class_lable = data\n",
    "test_img = test_noise\n",
    "print('class label for real', test_class_lable)\n",
    "\n",
    "s_output,c_label_op = discriminator(test_img.detach().to(device))\n",
    "print('Discriminator s', s_output)\n",
    "print('Discriminator c', c_label_op)\n",
    "\n",
    "test_img = test_img.detach().cpu()\n",
    "test_img_list.append(vutils.make_grid(test_img, padding=2, normalize=True))\n",
    "plt.imshow(np.transpose(test_img_list[-1],(1,2,0)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
