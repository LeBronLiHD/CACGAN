{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "370513dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: http://mirrors.aliyun.com/pypi/simple\n",
      "Requirement already satisfied: opencv-python in /root/miniconda3/lib/python3.8/site-packages (4.5.5.64)\n",
      "Requirement already satisfied: torchsummary in /root/miniconda3/lib/python3.8/site-packages (1.5.1)\n",
      "Requirement already satisfied: scikit-learn in /root/miniconda3/lib/python3.8/site-packages (1.1.1)\n",
      "Requirement already satisfied: torchviz in /root/miniconda3/lib/python3.8/site-packages (0.0.2)\n",
      "Requirement already satisfied: utils in /root/miniconda3/lib/python3.8/site-packages (1.0.1)\n",
      "Requirement already satisfied: numpy>=1.17.3 in /root/miniconda3/lib/python3.8/site-packages (from opencv-python) (1.21.4)\n",
      "Requirement already satisfied: scipy>=1.3.2 in /root/miniconda3/lib/python3.8/site-packages (from scikit-learn) (1.8.1)\n",
      "Requirement already satisfied: joblib>=1.0.0 in /root/miniconda3/lib/python3.8/site-packages (from scikit-learn) (1.1.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /root/miniconda3/lib/python3.8/site-packages (from scikit-learn) (3.1.0)\n",
      "Requirement already satisfied: graphviz in /root/miniconda3/lib/python3.8/site-packages (from torchviz) (0.20)\n",
      "Requirement already satisfied: torch in /root/miniconda3/lib/python3.8/site-packages (from torchviz) (1.10.0+cu113)\n",
      "Requirement already satisfied: typing-extensions in /root/miniconda3/lib/python3.8/site-packages (from torch->torchviz) (4.0.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install opencv-python torchsummary scikit-learn torchviz utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6a6a2202",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.parallel\n",
    "import torch.nn.functional as F\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.optim as optim\n",
    "import torch.utils.data\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.utils as vutils\n",
    "import utils\n",
    "from torch.nn.functional import one_hot\n",
    "from torchvision.utils import save_image\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import SubsetRandomSampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6cf2ef10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['sm_37', 'sm_50', 'sm_60', 'sm_70', 'sm_75', 'sm_80', 'sm_86'] cuda\n"
     ]
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(torch.cuda.get_arch_list(), device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b8079c45",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import argparse\n",
    "import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import HTML\n",
    "from IPython.display import clear_output\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "from tqdm import notebook\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import gc\n",
    "import matplotlib.colors as mat_color\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import random\n",
    "import cv2\n",
    "from torchvision.datasets import ImageNet, ImageFolder\n",
    "from torch.utils.data import DataLoader\n",
    "from torchsummary import summary\n",
    "from torchviz import make_dot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3eb59879",
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_weights(net):\n",
    "    for m in net.modules():\n",
    "        if isinstance(m, nn.Conv2d):\n",
    "            m.weight.data.normal_(0, 0.02)\n",
    "            m.bias.data.zero_()\n",
    "        elif isinstance(m, nn.ConvTranspose2d):\n",
    "            m.weight.data.normal_(0, 0.02)\n",
    "            m.bias.data.zero_()\n",
    "        elif isinstance(m, nn.Linear):\n",
    "            m.weight.data.normal_(0, 0.02)\n",
    "            m.bias.data.zero_()\n",
    "\n",
    "class Generator(nn.Module):\n",
    "    # Network Architecture is exactly same as in infoGAN (https://arxiv.org/abs/1606.03657)\n",
    "    # Architecture : FC1024_BR-FC7x7x128_BR-(64)4dc2s_BR-(1)4dc2s_S\n",
    "    def __init__(self, input_dim=100, output_dim=1, input_size=32, class_num=10):\n",
    "        super(Generator, self).__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.input_size = input_size\n",
    "        self.class_num = class_num\n",
    "\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(self.input_dim + self.class_num, 1024),\n",
    "            nn.BatchNorm1d(1024),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(1024, 128 * (self.input_size // 4) * (self.input_size // 4)),\n",
    "            nn.BatchNorm1d(128 * (self.input_size // 4) * (self.input_size // 4)),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        self.deconv = nn.Sequential(\n",
    "            nn.ConvTranspose2d(128, 64, 4, 2, 1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(64, self.output_dim, 4, 2, 1),\n",
    "            nn.Tanh(),\n",
    "        )\n",
    "        initialize_weights(self)\n",
    "\n",
    "    def forward(self, input, label):\n",
    "        x = torch.cat([input, label], 1)\n",
    "        x = self.fc(x)\n",
    "        x = x.view(-1, 128, (self.input_size // 4), (self.input_size // 4))\n",
    "        x = self.deconv(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "    \n",
    "class Discriminator(nn.Module):\n",
    "    # Network Architecture is exactly same as in infoGAN (https://arxiv.org/abs/1606.03657)\n",
    "    # Architecture : (64)4c2s-(128)4c2s_BL-FC1024_BL-FC1_S\n",
    "    def __init__(self, input_dim=1, output_dim=1, input_size=32, class_num=10):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.input_size = input_size\n",
    "        self.class_num = class_num\n",
    "\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(self.input_dim, 64, 4, 2, 1),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Conv2d(64, 128, 4, 2, 1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.LeakyReLU(0.2),\n",
    "        )\n",
    "        self.fc1 = nn.Sequential(\n",
    "            nn.Linear(128 * (self.input_size // 4) * (self.input_size // 4), 1024),\n",
    "            nn.BatchNorm1d(1024),\n",
    "            nn.LeakyReLU(0.2),\n",
    "        )\n",
    "        self.dc = nn.Sequential(\n",
    "            nn.Linear(1024, self.output_dim),\n",
    "            nn.Sigmoid(),\n",
    "        )\n",
    "        self.cl = nn.Sequential(\n",
    "            nn.Linear(1024, self.class_num),\n",
    "        )\n",
    "        initialize_weights(self)\n",
    "\n",
    "    def forward(self, input):\n",
    "        x = self.conv(input)\n",
    "        x = x.view(-1, 128 * (self.input_size // 4) * (self.input_size // 4))\n",
    "        x = self.fc1(x)\n",
    "        d = self.dc(x)\n",
    "        c = self.cl(x)\n",
    "\n",
    "        return d, c\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "220b9685",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_path = './data'\n",
    "base_folder = \"Covid-19 Image Dataset\"\n",
    "classic_folder = 'Coivd-19_Classic'\n",
    "synthetic_folder = 'Coivd-19_Synthetic'\n",
    "data_dir = os.path.join(base_path, classic_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9b6ea5a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_size = 512\n",
    "batch_size = 16\n",
    "train_path = os.path.join(data_dir, \"train\")\n",
    "test_path = os.path.join(data_dir, \"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1bf35c4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Covid', 'Normal', 'Viral Pneumonia']\n"
     ]
    }
   ],
   "source": [
    "labels = os.listdir(train_path)\n",
    "print(labels)\n",
    "no_norm = mat_color.Normalize(vmin=0, vmax=255, clip=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "28c7ac6b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([16])\n",
      "torch.Size([16, 3])\n",
      "tensor([4.2706e-20, 4.5717e-41, 7.7899e-24, 3.0737e-41, 4.4842e-44, 0.0000e+00,\n",
      "        8.9683e-44, 0.0000e+00, 7.8375e-24, 3.0737e-41, 2.2421e-44, 0.0000e+00,\n",
      "               nan, 0.0000e+00, 2.6800e+20, 1.7288e+28], device='cuda:0')\n",
      "tensor([[ 0.0000e+00,  0.0000e+00,  7.4351e-25],\n",
      "        [ 3.0737e-41, -5.2732e-14,  0.0000e+00],\n",
      "        [ 3.6013e-43,  0.0000e+00,  1.6120e-23],\n",
      "        [ 3.0737e-41,  4.2706e-20,  4.5717e-41],\n",
      "        [ 2.1943e-38,  0.0000e+00,  3.1529e-43],\n",
      "        [ 0.0000e+00,  1.6120e-23,  3.0737e-41],\n",
      "        [ 4.2706e-20,  4.5717e-41,  6.4423e-30],\n",
      "        [ 0.0000e+00,  2.7045e-43,  0.0000e+00],\n",
      "        [ 1.6120e-23,  3.0737e-41,  4.2706e-20],\n",
      "        [ 4.5717e-41, -2.6384e+26,  0.0000e+00],\n",
      "        [ 2.2561e-43,  0.0000e+00,  1.6120e-23],\n",
      "        [ 3.0737e-41,  4.2706e-20,  4.5717e-41],\n",
      "        [-1.9493e-13,  0.0000e+00,  1.8077e-43],\n",
      "        [ 0.0000e+00,  1.6120e-23,  3.0737e-41],\n",
      "        [ 4.2706e-20,  4.5717e-41, -9.1864e-23],\n",
      "        [ 0.0000e+00,  1.3593e-43,  0.0000e+00]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "# Number of training epochs\n",
    "num_epochs = 100\n",
    "\n",
    "#All images will be resized to this size using a transformer.\n",
    "#image_size = 64\n",
    "imageSize = 512\n",
    "\n",
    "# Number of channels in the training images. For color images this is 3\n",
    "nc = 3\n",
    "\n",
    "# Size of z latent vector (i.e. size of generator input)\n",
    "nz = 128\n",
    "\n",
    "# Size of feature maps in generator\n",
    "ngf = imageSize\n",
    "\n",
    "# Size of feature maps in discriminator\n",
    "ndf = imageSize\n",
    "\n",
    "# No of labels\n",
    "nb_label = len(labels)\n",
    "\n",
    "# Learning rate for optimizers\n",
    "lr = 0.002\n",
    "lr_d = 0.0002\n",
    "# Beta1 hyperparam for Adam optimizers\n",
    "beta1 = 0.5\n",
    "# Beta2 hyperparam for Adam optimizers\n",
    "beta2 = 0.999\n",
    "\n",
    "real_label = 1.\n",
    "fake_label = 0.\n",
    "# Input to generator\n",
    "# fixed_noise = torch.randn(64, nz, 1, 1, device=device) #batch of 64\n",
    "# Define Loss function\n",
    "s_criterion = nn.BCELoss().to(device) #For synthesizing\n",
    "c_criterion = nn.CrossEntropyLoss().to(device) #For classification\n",
    "\n",
    "# input = torch.FloatTensor(batch_size, nc, imageSize, imageSize).to(device)\n",
    "# noise = torch.FloatTensor(batch_size, nz, 1, 1).to(device)\n",
    "# fixed_noise = torch.FloatTensor(batch_size, nz, 1, 1).normal_(0, 1).to(device)\n",
    "s_label = torch.FloatTensor(batch_size).to(device)\n",
    "c_label = torch.FloatTensor(batch_size, 3).to(device)\n",
    "# s_label = torch.FloatTensor(batch_size).to(device)\n",
    "# c_label = torch.LongTensor(batch_size).to(device)\n",
    "\n",
    "# input = Variable(input)\n",
    "# s_label = Variable(s_label)\n",
    "# c_label = Variable(c_label)\n",
    "print(s_label.shape)\n",
    "print(c_label.shape)\n",
    "print(s_label)\n",
    "print(c_label)\n",
    "# noise = Variable(noise)\n",
    "# fixed_noise = Variable(fixed_noise)\n",
    "# fixed_noise_ = np.random.normal(0, 1, (batch_size, nz))\n",
    "# random_label = np.random.randint(0, nb_label, batch_size)\n",
    "# #print('fixed label:{}'.format(random_label))\n",
    "# random_onehot = np.zeros((batch_size, nb_label))\n",
    "# random_onehot[np.arange(batch_size), random_label] = 1\n",
    "# fixed_noise_[np.arange(batch_size), :nb_label] = random_onehot[np.arange(batch_size)]\n",
    "\n",
    "\n",
    "# fixed_noise_ = (torch.from_numpy(fixed_noise_))\n",
    "# fixed_noise_ = fixed_noise_.resize_(batch_size, nz, 1, 1)\n",
    "# fixed_noise.data.copy_(fixed_noise_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bd17020a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Errno 17] File exists: './GANAug'\n",
      "[Errno 17] File exists: './GANAug/model'\n",
      "[Errno 17] File exists: './GANAug/plots'\n",
      "[Errno 17] File exists: './GANAug/model/ACGAN'\n",
      "[Errno 17] File exists: './GANAug/plots/ACGAN'\n",
      "[Errno 17] File exists: './GANAug/output_images'\n",
      "[Errno 17] File exists: './GANAug/output_images/ACGAN'\n"
     ]
    }
   ],
   "source": [
    "for func in [\n",
    "    lambda: os.mkdir(os.path.join('.', 'GANAug')),\n",
    "    lambda: os.mkdir(os.path.join('.', 'GANAug/model')),\n",
    "    lambda: os.mkdir(os.path.join('.', 'GANAug/plots')),\n",
    "    lambda: os.mkdir(os.path.join('.', 'GANAug/model/ACGAN')),\n",
    "    lambda: os.mkdir(os.path.join('.', 'GANAug/plots/ACGAN')),\n",
    "    lambda: os.mkdir(os.path.join('.', 'GANAug/output_images')),\n",
    "    lambda: os.mkdir(os.path.join('.', 'GANAug/output_images/ACGAN'))]:  # create directories\n",
    "    try:\n",
    "        func()\n",
    "    except Exception as error:\n",
    "        print(error)\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "74dda24a",
   "metadata": {},
   "outputs": [],
   "source": [
    "METRIC_FIELDS = [\n",
    "    'train.D_x',\n",
    "    'train.D_G_z1',\n",
    "    'train.D_G_z2',\n",
    "    'train.G_losses',\n",
    "    'train.D_losses',\n",
    "]\n",
    "metrics = {field: list() for field in METRIC_FIELDS}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a26a75d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset(train_dir=train_path, test_dir=test_path):\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((img_size, img_size)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "        ])\n",
    "    train_data = datasets.ImageFolder(train_dir, transform=transform)\n",
    "    train_loader = DataLoader(train_data, batch_size, shuffle=True, num_workers=1)\n",
    "\n",
    "    test_data = datasets.ImageFolder(test_dir ,transform=transform)\n",
    "    test_loader = DataLoader(test_data, batch_size, shuffle=True, num_workers=1)\n",
    "\n",
    "    return train_loader, test_loader, train_data, test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bbb0b552",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader, test_loader, train_data, test_data = load_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0360680c",
   "metadata": {},
   "outputs": [],
   "source": [
    "generator = Generator(input_dim=nz, output_dim=nc, input_size=ngf, class_num=nb_label).to(device)\n",
    "discriminator = Discriminator(input_dim=nc, output_dim=1, input_size=ndf, class_num=nb_label).to(device)\n",
    "\n",
    "# setup optimizer\n",
    "optimizerD = optim.Adam(discriminator.parameters(), lr=lr_d, betas=(beta1, beta2))\n",
    "optimizerG = optim.Adam(generator.parameters(), lr=lr, betas=(beta1, beta2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8d141b1c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16, 3, 512, 512])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "g_output = generator(torch.rand((batch_size, nz)).to(device), torch.ones((batch_size, nb_label)).to(device))\n",
    "g_output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8244b33d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16, 1])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s_output, c_output = discriminator(torch.rand(g_output.shape).to(device))\n",
    "s_output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8ae20157",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16, 3])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c_output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e0160689",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.5457],\n",
       "        [0.3780],\n",
       "        [0.4582],\n",
       "        [0.5439],\n",
       "        [0.3552],\n",
       "        [0.3951],\n",
       "        [0.4220],\n",
       "        [0.5855],\n",
       "        [0.3634],\n",
       "        [0.5419],\n",
       "        [0.3884],\n",
       "        [0.3118],\n",
       "        [0.3516],\n",
       "        [0.3942],\n",
       "        [0.3329],\n",
       "        [0.4155]], device='cuda:0', grad_fn=<SigmoidBackward0>)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "47633398",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.2222,  0.2849,  0.0235],\n",
       "        [-0.3323,  0.3322,  0.7577],\n",
       "        [-0.1640,  0.9260, -0.1262],\n",
       "        [ 0.6716, -0.2795,  0.2536],\n",
       "        [ 0.4336,  0.0124, -0.2183],\n",
       "        [-0.5017,  0.4373,  0.9848],\n",
       "        [-0.0200,  0.1337, -0.2580],\n",
       "        [ 0.3637,  0.0554, -0.1926],\n",
       "        [-0.3055,  0.2880, -0.2432],\n",
       "        [-0.1434,  0.4411,  0.1116],\n",
       "        [ 0.6108, -0.7236,  0.3336],\n",
       "        [ 0.4692, -0.1671, -0.2834],\n",
       "        [-0.7962,  0.8066,  0.2995],\n",
       "        [-0.8408, -0.2449, -0.3654],\n",
       "        [ 0.1333,  0.0596, -0.6922],\n",
       "        [ 0.4729,  0.1008,  0.2158]], device='cuda:0',\n",
       "       grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "94ee3b40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# exit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "065eed00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: http://mirrors.aliyun.com/pypi/simple\n",
      "Requirement already satisfied: torchviz in /root/miniconda3/lib/python3.8/site-packages (0.0.2)\n",
      "Requirement already satisfied: graphviz in /root/miniconda3/lib/python3.8/site-packages (from torchviz) (0.20)\n",
      "Requirement already satisfied: torch in /root/miniconda3/lib/python3.8/site-packages (from torchviz) (1.10.0+cu113)\n",
      "Requirement already satisfied: typing-extensions in /root/miniconda3/lib/python3.8/site-packages (from torch->torchviz) (4.0.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install torchviz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "81981a02",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generator(\n",
      "  (fc): Sequential(\n",
      "    (0): Linear(in_features=131, out_features=1024, bias=True)\n",
      "    (1): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU()\n",
      "    (3): Linear(in_features=1024, out_features=2097152, bias=True)\n",
      "    (4): BatchNorm1d(2097152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (5): ReLU()\n",
      "  )\n",
      "  (deconv): Sequential(\n",
      "    (0): ConvTranspose2d(128, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU()\n",
      "    (3): ConvTranspose2d(64, 3, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "    (4): Tanh()\n",
      "  )\n",
      ")\n",
      "Discriminator(\n",
      "  (conv): Sequential(\n",
      "    (0): Conv2d(3, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "    (1): LeakyReLU(negative_slope=0.2)\n",
      "    (2): Conv2d(64, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "    (3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (4): LeakyReLU(negative_slope=0.2)\n",
      "  )\n",
      "  (fc1): Sequential(\n",
      "    (0): Linear(in_features=2097152, out_features=1024, bias=True)\n",
      "    (1): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): LeakyReLU(negative_slope=0.2)\n",
      "  )\n",
      "  (dc): Sequential(\n",
      "    (0): Linear(in_features=1024, out_features=1, bias=True)\n",
      "    (1): Sigmoid()\n",
      "  )\n",
      "  (cl): Sequential(\n",
      "    (0): Linear(in_features=1024, out_features=3, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(generator)\n",
    "print(discriminator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5e2592f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "SHOW = False\n",
    "if SHOW:\n",
    "    summary(generator, (nz, ngf, nc), batch_size=1, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "57c2031b",
   "metadata": {},
   "outputs": [],
   "source": [
    "if SHOW:\n",
    "    summary(discriminator, (nc, ndf, ndf), batch_size=1, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "0580f119",
   "metadata": {},
   "outputs": [],
   "source": [
    "SHOW_IMG = False\n",
    "if SHOW_IMG:\n",
    "    def modeltorchviz(model,input2):\n",
    "        y = model(input2.cuda())    # 获取网络的预测值\n",
    "        MyConvNetVis = make_dot(y, params=dict(list(model.named_parameters()) + [('x', input2)]))\n",
    "        MyConvNetVis.format = \"png\"\n",
    "        # 指定文件生成的文件夹\n",
    "        MyConvNetVis.directory = \"images\"\n",
    "        # 生成文件\n",
    "        MyConvNetVis.view() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f0cd9edc",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "if SHOW_IMG:\n",
    "    modeltorchviz(generator, torch.randn(1, nz, ngf, nc).requires_grad_(True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d02b9f92",
   "metadata": {},
   "outputs": [],
   "source": [
    "if SHOW_IMG:\n",
    "    modeltorchviz(discriminator, torch.randn(1, nc, ndf, ndf).requires_grad_(True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "0d1b6459",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(predict, labels):\n",
    "    correct = 0\n",
    "    pred = predict.data.max(1)[1]\n",
    "    correct = pred.eq(labels.data).cpu().sum()\n",
    "    return correct, len(labels.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "0714f81e",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                                | 0/189 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "img.shape torch.Size([16, 3, 512, 512])\n",
      "label_ori tensor([0, 0, 0, 2, 1, 1, 1, 1, 2, 0, 0, 0, 1, 2, 2, 0], device='cuda:0')\n",
      "label_ori.shape torch.Size([16])\n",
      "label tensor([[1, 0, 0],\n",
      "        [1, 0, 0],\n",
      "        [1, 0, 0],\n",
      "        [0, 0, 1],\n",
      "        [0, 1, 0],\n",
      "        [0, 1, 0],\n",
      "        [0, 1, 0],\n",
      "        [0, 1, 0],\n",
      "        [0, 0, 1],\n",
      "        [1, 0, 0],\n",
      "        [1, 0, 0],\n",
      "        [1, 0, 0],\n",
      "        [0, 1, 0],\n",
      "        [0, 0, 1],\n",
      "        [0, 0, 1],\n",
      "        [1, 0, 0]], device='cuda:0')\n",
      "s_label.shape torch.Size([16])\n",
      "c_label.shape torch.Size([16, 3])\n",
      "real_label 1.0\n",
      "s_label.shape torch.Size([16])\n",
      "c_label.shape torch.Size([16, 3])\n",
      "s_output.shape torch.Size([16, 1])\n",
      "s_label.shape torch.Size([16])\n",
      "================================================================================\n",
      "s_output tensor([[0.3702],\n",
      "        [0.4009],\n",
      "        [0.5535],\n",
      "        [0.4673],\n",
      "        [0.5046],\n",
      "        [0.2711],\n",
      "        [0.4713],\n",
      "        [0.5688],\n",
      "        [0.3739],\n",
      "        [0.1712],\n",
      "        [0.3621],\n",
      "        [0.4306],\n",
      "        [0.5800],\n",
      "        [0.4749],\n",
      "        [0.4907],\n",
      "        [0.3888]], device='cuda:0', grad_fn=<SigmoidBackward0>)\n",
      "s_label tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
      "       device='cuda:0')\n",
      "================================================================================\n",
      "c_label tensor([[1., 0., 0.],\n",
      "        [1., 0., 0.],\n",
      "        [1., 0., 0.],\n",
      "        [0., 0., 1.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 0., 1.],\n",
      "        [1., 0., 0.],\n",
      "        [1., 0., 0.],\n",
      "        [1., 0., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 0., 1.],\n",
      "        [0., 0., 1.],\n",
      "        [1., 0., 0.]], device='cuda:0')\n",
      "c_output tensor([[ 0.1366,  0.3918,  0.2658],\n",
      "        [ 0.2582,  0.0183,  0.4103],\n",
      "        [ 0.1757,  0.7103, -0.1626],\n",
      "        [-0.7469, -0.4673,  0.8512],\n",
      "        [-0.0408,  0.5012,  0.3850],\n",
      "        [ 0.6518, -0.2284,  0.2446],\n",
      "        [-0.1221, -0.3004,  0.5978],\n",
      "        [-0.9255, -0.0075, -0.3856],\n",
      "        [-0.6429,  0.3482, -0.1703],\n",
      "        [ 0.8718, -0.3105, -0.0759],\n",
      "        [-0.3565,  0.7707, -0.7064],\n",
      "        [ 0.3033,  0.2944, -0.0687],\n",
      "        [ 0.9954,  0.5213, -0.0450],\n",
      "        [-0.3361,  0.1422, -0.0749],\n",
      "        [-0.1101, -0.0276, -0.2126],\n",
      "        [-0.5791,  0.0708, -0.1823]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "================================================================================\n",
      "c_label.shape torch.Size([16, 3])\n",
      "c_output.shape torch.Size([16, 3])\n",
      "================================================================================\n",
      "c_label tensor([[1., 0., 0.],\n",
      "        [1., 0., 0.],\n",
      "        [1., 0., 0.],\n",
      "        [0., 0., 1.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 0., 1.],\n",
      "        [1., 0., 0.],\n",
      "        [1., 0., 0.],\n",
      "        [1., 0., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 0., 1.],\n",
      "        [0., 0., 1.],\n",
      "        [1., 0., 0.]], device='cuda:0')\n",
      "c_output tensor([[ 0.1366,  0.3918,  0.2658],\n",
      "        [ 0.2582,  0.0183,  0.4103],\n",
      "        [ 0.1757,  0.7103, -0.1626],\n",
      "        [-0.7469, -0.4673,  0.8512],\n",
      "        [-0.0408,  0.5012,  0.3850],\n",
      "        [ 0.6518, -0.2284,  0.2446],\n",
      "        [-0.1221, -0.3004,  0.5978],\n",
      "        [-0.9255, -0.0075, -0.3856],\n",
      "        [-0.6429,  0.3482, -0.1703],\n",
      "        [ 0.8718, -0.3105, -0.0759],\n",
      "        [-0.3565,  0.7707, -0.7064],\n",
      "        [ 0.3033,  0.2944, -0.0687],\n",
      "        [ 0.9954,  0.5213, -0.0450],\n",
      "        [-0.3361,  0.1422, -0.0749],\n",
      "        [-0.1101, -0.0276, -0.2126],\n",
      "        [-0.5791,  0.0708, -0.1823]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "================================================================================\n",
      "tensor([[ 0.1366,  0.3918,  0.2658],\n",
      "        [ 0.2582,  0.0183,  0.4103],\n",
      "        [ 0.1757,  0.7103, -0.1626],\n",
      "        [-0.7469, -0.4673,  0.8512],\n",
      "        [-0.0408,  0.5012,  0.3850],\n",
      "        [ 0.6518, -0.2284,  0.2446],\n",
      "        [-0.1221, -0.3004,  0.5978],\n",
      "        [-0.9255, -0.0075, -0.3856],\n",
      "        [-0.6429,  0.3482, -0.1703],\n",
      "        [ 0.8718, -0.3105, -0.0759],\n",
      "        [-0.3565,  0.7707, -0.7064],\n",
      "        [ 0.3033,  0.2944, -0.0687],\n",
      "        [ 0.9954,  0.5213, -0.0450],\n",
      "        [-0.3361,  0.1422, -0.0749],\n",
      "        [-0.1101, -0.0276, -0.2126],\n",
      "        [-0.5791,  0.0708, -0.1823]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "tensor([[1., 0., 0.],\n",
      "        [1., 0., 0.],\n",
      "        [1., 0., 0.],\n",
      "        [0., 0., 1.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 0., 1.],\n",
      "        [1., 0., 0.],\n",
      "        [1., 0., 0.],\n",
      "        [1., 0., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 0., 1.],\n",
      "        [0., 0., 1.],\n",
      "        [1., 0., 0.]], device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                                | 0/189 [00:02<?, ?it/s]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA out of memory. Tried to allocate 8.00 GiB (GPU 0; 23.69 GiB total capacity; 18.82 GiB already allocated; 2.69 GiB free; 18.88 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_79975/1924876768.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     46\u001b[0m         \u001b[0mc_errD_real\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mc_criterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mc_label\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m         \u001b[0merrD_real\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0ms_errD_real\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mc_errD_real\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 48\u001b[0;31m         \u001b[0merrD_real\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     49\u001b[0m         \u001b[0mD_x\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0ms_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.8/site-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    305\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    306\u001b[0m                 inputs=inputs)\n\u001b[0;32m--> 307\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    308\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    309\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.8/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    152\u001b[0m         \u001b[0mretain_graph\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    153\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 154\u001b[0;31m     Variable._execution_engine.run_backward(\n\u001b[0m\u001b[1;32m    155\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    156\u001b[0m         allow_unreachable=True, accumulate_grad=True)  # allow_unreachable flag\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 8.00 GiB (GPU 0; 23.69 GiB total capacity; 18.82 GiB already allocated; 2.69 GiB free; 18.88 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "for epoch in range(2):\n",
    "    for i, data in enumerate(tqdm(train_loader, 0)):\n",
    "        ###########################\n",
    "        # (1) Update D network\n",
    "        ###########################\n",
    "        # train with real\n",
    "        discriminator.zero_grad()\n",
    "        img, label = data\n",
    "        batch_size = img.size(0)\n",
    "        with torch.no_grad():\n",
    "            img, label = img.to(device), label.to(device)\n",
    "#             input.resize_(img.size()).copy_(img)\n",
    "            print(\"img.shape\", img.shape)\n",
    "            print(\"label_ori\", label)\n",
    "            print(\"label_ori.shape\", label.shape)\n",
    "            label = one_hot(label)\n",
    "            print(\"label\", label)\n",
    "            print(\"s_label.shape\", s_label.shape)\n",
    "            print(\"c_label.shape\", c_label.shape)\n",
    "            print(\"real_label\", real_label)\n",
    "            s_label.resize_(batch_size).fill_(real_label)\n",
    "            c_label.resize_(batch_size, 3).copy_(label)\n",
    "            print(\"s_label.shape\", s_label.shape)\n",
    "            print(\"c_label.shape\", c_label.shape)\n",
    "        s_output, c_output = discriminator(img)\n",
    "        print(\"s_output.shape\", s_output.shape)\n",
    "        print(\"s_label.shape\", s_label.shape)\n",
    "        print(\"=\"*80)\n",
    "        print(\"s_output\", s_output)\n",
    "        print(\"s_label\", s_label)\n",
    "        print(\"=\"*80)\n",
    "        print(\"c_label\", c_label)\n",
    "        print(\"c_output\", c_output)\n",
    "        print(\"=\"*80)\n",
    "        s_errD_real = s_criterion(s_output[:,0], s_label.resize_(batch_size))\n",
    "#         c_output = torch.argmax(c_output, dim=1)\n",
    "#         c_label = torch.argmax(c_label, dim=1)\n",
    "        print(\"c_label.shape\", c_label.shape)\n",
    "        print(\"c_output.shape\", c_output.shape)\n",
    "        print(\"=\"*80)\n",
    "        print(\"c_label\", c_label)\n",
    "        print(\"c_output\", c_output)\n",
    "        print(\"=\"*80)\n",
    "        print(c_output.float())\n",
    "        print(c_label.float())\n",
    "        c_errD_real = c_criterion(c_output.float(), c_label.float())\n",
    "        errD_real = s_errD_real + c_errD_real\n",
    "        errD_real.backward()\n",
    "        D_x = s_output.data.mean()\n",
    "        \n",
    "        correct, length = test(c_output, c_label)\n",
    "\n",
    "        # train with fake\n",
    "        with torch.no_grad():\n",
    "            noise.resize_(batch_size, nz, 1, 1)\n",
    "            noise.normal_(0, 1)\n",
    "\n",
    "        label = np.random.randint(0, nb_label, batch_size)\n",
    "        noise_ = np.random.normal(0, 1, (batch_size, nz))\n",
    "        label_onehot = np.zeros((batch_size, nb_label))\n",
    "        label_onehot[np.arange(batch_size), label] = 1\n",
    "        noise_[np.arange(batch_size), :nb_label] = label_onehot[np.arange(batch_size)]\n",
    "        \n",
    "        noise_ = (torch.from_numpy(noise_))\n",
    "        noise_ = noise_.resize_(batch_size, nz, 1, 1)\n",
    "        noise.data.copy_(noise_)\n",
    "\n",
    "        c_label.data.resize_(batch_size).copy_(torch.from_numpy(label))\n",
    "\n",
    "        fake = generator(noise)\n",
    "        s_label.data.fill_(fake_label)\n",
    "        s_output,c_output = discriminator(fake.detach())\n",
    "        s_errD_fake = s_criterion(s_output, s_label)\n",
    "        c_errD_fake = c_criterion(c_output, c_label)\n",
    "        errD_fake = s_errD_fake + c_errD_fake\n",
    "\n",
    "        errD_fake.backward()\n",
    "        D_G_z1 = s_output.data.mean()\n",
    "        errD = s_errD_real + s_errD_fake\n",
    "        optimizerD.step()\n",
    "\n",
    "        ###########################\n",
    "        # (2) Update G network\n",
    "        ###########################\n",
    "        generator.zero_grad()\n",
    "        s_label.data.fill_(real_label)  # fake labels are real for generator cost\n",
    "        s_output,c_output = discriminator(fake)\n",
    "        s_errG = s_criterion(s_output, s_label)\n",
    "        c_errG = c_criterion(c_output, c_label)\n",
    "        \n",
    "        errG = s_errG + c_errG\n",
    "        errG.backward()\n",
    "        D_G_z2 = s_output.data.mean()\n",
    "        optimizerG.step()\n",
    "        metrics['train.G_losses'].append(errG.item())\n",
    "        metrics['train.D_losses'].append(errD.item())\n",
    "\n",
    "        print('[%d/%d][%d/%d] Loss_D: %.4f Loss_G: %.4f D(x): %.4f D(G(z)): %.4f / %.4f, Accuracy: %.4f / %.4f = %.4f'\n",
    "              % (epoch, num_epochs, i, len(train_loader),\n",
    "                 errD.data, errG.data, D_x, D_G_z1, D_G_z2,\n",
    "                 correct, length, 100.* correct / length))\n",
    "        if i % 100 == 0:\n",
    "            vutils.save_image(img,\n",
    "                    '%s/real_samples.jpg' % './augGAN/output_images/ACGAN', normalize=True)\n",
    "            #fake = netG(fixed_cat)\n",
    "            fake = generator(fixed_noise)\n",
    "            vutils.save_image(fake.data,\n",
    "                    '%s/fake_samples_epoch_%03d.jpg' % ('./augGAN/output_images/ACGAN', epoch), normalize=True)\n",
    "\n",
    "    # do checkpointing\n",
    "    #torch.save(generator.state_dict(), '%s/netG_epoch_%d.pth' % (os.path.join('.', 'augGAN/model/ACGAN'), epoch))\n",
    "    #torch.save(discriminator.state_dict(), '%s/netD_epoch_%d.pth' % (os.path.join('.', 'augGAN/model/ACGAN'), epoch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09e18531",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test2(generator, discriminator, num_epochs, metrics, loader):\n",
    "    print('Testing Block.........')\n",
    "    now = datetime.datetime.now()\n",
    "    #g_losses = metrics['train.G_losses'][-1]\n",
    "    #d_losses = metrics['train.D_losses'][-1]\n",
    "    path='augGAN/output_images/ACGAN'\n",
    "    try:\n",
    "      os.mkdir(os.path.join('.', path))\n",
    "    except Exception as error:\n",
    "      print(error)\n",
    "\n",
    "    real_batch = next(iter(loader))\n",
    "    \n",
    "    test_img_list = []\n",
    "    test_noise = torch.randn(batch_size, nz, 1, 1, device=device)\n",
    "    test_fake = generator(test_noise).detach().cpu()\n",
    "    test_img_list.append(vutils.make_grid(test_fake, padding=2, normalize=True))\n",
    "\n",
    "    fig = plt.figure(figsize=(15,15))\n",
    "    ax1 = plt.subplot(1,2,1)\n",
    "    ax1 = plt.axis(\"off\")\n",
    "    ax1 = plt.title(\"Real Images\")\n",
    "    ax1 = plt.imshow(np.transpose(vutils.make_grid(real_batch[0].to(device)[:64], padding=5, normalize=True).cpu(),(1,2,0)))\n",
    "\n",
    "    ax2 = plt.subplot(1,2,2)\n",
    "    ax2 = plt.axis(\"off\")\n",
    "    ax2 = plt.title(\"Fake Images\")\n",
    "    ax2 = plt.imshow(np.transpose(test_img_list[-1],(1,2,0)))\n",
    "    #ax2 = plt.show()\n",
    "    #fig.savefig('%s/image_%.3f_%.3f_%d_%s.png' %\n",
    "    #                (path, g_losses, d_losses, num_epochs, now.strftime(\"%Y-%m-%d_%H:%M:%S\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9cca5ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot(name, train_epoch, values, path, save):\n",
    "    clear_output(wait=True)\n",
    "    plt.close('all')\n",
    "    fig = plt.figure()\n",
    "    fig = plt.ion()\n",
    "    fig = plt.subplot(1, 1, 1)\n",
    "    fig = plt.title('epoch: %s -> %s: %s' % (train_epoch, name, values[-1]))\n",
    "    fig = plt.ylabel(name)\n",
    "    fig = plt.xlabel('train_set')\n",
    "    fig = plt.plot(values)\n",
    "    fig = plt.grid()\n",
    "    get_fig = plt.gcf()\n",
    "    fig = plt.draw()  # draw the plot\n",
    "    fig = plt.pause(1)  # show it for 1 second\n",
    "    if save:\n",
    "        now = datetime.datetime.now()\n",
    "        get_fig.savefig('%s/%s_%.3f_%d_%s.png' %\n",
    "                        (path, name, train_epoch, values[-1], now.strftime(\"%Y-%m-%d_%H:%M:%S\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d5e7892",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model(generator, discriminator, gen_optimizer, dis_optimizer, metrics, num_epochs):\n",
    "    now = datetime.datetime.now()\n",
    "    g_losses = metrics['train.G_losses'][-1]\n",
    "    d_losses = metrics['train.D_losses'][-1]\n",
    "    name = \"%+.3f_%+.3f_%d_%s.dat\" % (g_losses, d_losses, num_epochs, now.strftime(\"%Y-%m-%d_%H:%M:%S\"))\n",
    "    # fname = os.path.join('.', 'augGAN/model', name)\n",
    "    # states = {\n",
    "    #         'state_dict_generator': generator.state_dict(),\n",
    "    #         'state_dict_discriminator': discriminator.state_dict(),\n",
    "    #         'gen_optimizer': gen_optimizer.state_dict(),\n",
    "    #         'dis_optimizer': dis_optimizer.state_dict(),\n",
    "    #         'metrics': metrics,\n",
    "    #         'train_epoch': num_epochs,\n",
    "    #         'date': now.strftime(\"%Y-%m-%d_%H:%M:%S\"),\n",
    "    # }\n",
    "    # torch.save(states, fname)\n",
    "    path='augGAN/plots/ACGAN/train_%+.3f_%+.3f_%s'% (g_losses, d_losses, now.strftime(\"%Y-%m-%d_%H:%M:%S\"))\n",
    "    try:\n",
    "        os.mkdir(os.path.join('.', path))\n",
    "    except Exception as error:\n",
    "        print(error)\n",
    "\n",
    "    plot('G_losses', num_epochs, metrics['train.G_losses'], path, True)\n",
    "    plot('D_losses', num_epochs, metrics['train.D_losses'], path, True)\n",
    "    plot('D_x', num_epochs, metrics['train.D_x'], path, True)\n",
    "    plot('D_G_z1', num_epochs, metrics['train.D_G_z1'], path, True)\n",
    "    plot('D_G_z2', num_epochs, metrics['train.D_G_z2'], path, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a280fe2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "test2(generator, discriminator, num_epochs, metrics, train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f66ad873",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_model(generator, discriminator, optimizerG, optimizerD, metrics, num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c56cd9c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_batch = 16\n",
    "test_fake = 1\n",
    "\n",
    "if test_fake:\n",
    "    #check for fake image\n",
    "    test_img_list = []\n",
    "    test_noise = torch.randn(test_batch, nz, 1, 1, device=device)\n",
    "    test_img = generator(test_noise)#.detach().cpu()\n",
    "\n",
    "else:\n",
    "    #check for real image\n",
    "    test_loader = torch.utils.data.DataLoader(train_set, batch_size=test_batch,\n",
    "                                            shuffle=True)\n",
    "    data = next(iter(test_loader))\n",
    "    test_noise, test_class_lable = data\n",
    "    test_img.data.resize_(test_noise.size()).copy_(test_noise)\n",
    "    #print(data[0].size())\n",
    "    print('class label for real', test_class_lable)\n",
    "\n",
    "s_output,c_label_op = discriminator(test_img.detach().to(device))\n",
    "print('Discriminator s o/p', s_output)\n",
    "print('Discriminator c o/p', c_label_op)\n",
    "\n",
    "# label = np.random.randint(0, nb_label, batch_size)\n",
    "# c_label.data.resize_(batch_size).copy_(torch.from_numpy(label))\n",
    "# print(c_label)\n",
    "\n",
    "test_img = test_img.detach().cpu()\n",
    "test_img_list.append(vutils.make_grid(test_img, padding=2, normalize=True))\n",
    "plt.imshow(np.transpose(test_img_list[-1],(1,2,0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d45fe52a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
