{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "370513dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: opencv-python in d:\\miniconda3\\envs\\torch_1-11\\lib\\site-packages (4.5.5.64)\n",
      "Requirement already satisfied: torchsummary in d:\\miniconda3\\envs\\torch_1-11\\lib\\site-packages (1.5.1)\n",
      "Requirement already satisfied: scikit-learn in d:\\miniconda3\\envs\\torch_1-11\\lib\\site-packages (1.1.0)\n",
      "Requirement already satisfied: torchviz in d:\\miniconda3\\envs\\torch_1-11\\lib\\site-packages (0.0.2)\n",
      "Requirement already satisfied: utils in d:\\miniconda3\\envs\\torch_1-11\\lib\\site-packages (1.0.1)\n",
      "Requirement already satisfied: numpy>=1.19.3 in d:\\miniconda3\\envs\\torch_1-11\\lib\\site-packages (from opencv-python) (1.21.5)\n",
      "Requirement already satisfied: scipy>=1.3.2 in d:\\miniconda3\\envs\\torch_1-11\\lib\\site-packages (from scikit-learn) (1.7.3)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in d:\\miniconda3\\envs\\torch_1-11\\lib\\site-packages (from scikit-learn) (3.1.0)\n",
      "Requirement already satisfied: joblib>=1.0.0 in d:\\miniconda3\\envs\\torch_1-11\\lib\\site-packages (from scikit-learn) (1.1.0)\n",
      "Requirement already satisfied: graphviz in d:\\miniconda3\\envs\\torch_1-11\\lib\\site-packages (from torchviz) (0.20)\n",
      "Requirement already satisfied: torch in d:\\miniconda3\\envs\\torch_1-11\\lib\\site-packages (from torchviz) (1.11.0)\n",
      "Requirement already satisfied: typing_extensions in d:\\miniconda3\\envs\\torch_1-11\\lib\\site-packages (from torch->torchviz) (4.1.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install opencv-python torchsummary scikit-learn torchviz utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6a6a2202",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.parallel\n",
    "import torch.nn.functional as F\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.optim as optim\n",
    "import torch.utils.data\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.utils as vutils\n",
    "import utils\n",
    "from torchvision.utils import save_image\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import SubsetRandomSampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6cf2ef10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['sm_37', 'sm_50', 'sm_60', 'sm_61', 'sm_70', 'sm_75', 'sm_80', 'sm_86', 'compute_37'] cuda\n"
     ]
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(torch.cuda.get_arch_list(), device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b8079c45",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import argparse\n",
    "import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import HTML\n",
    "from IPython.display import clear_output\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "from tqdm import notebook\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from tabulate import tabulate\n",
    "import gc\n",
    "import matplotlib.colors as mat_color\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import random\n",
    "import cv2\n",
    "from torchvision.datasets import ImageNet, ImageFolder\n",
    "from torch.utils.data import DataLoader\n",
    "from torchsummary import summary\n",
    "from torchviz import make_dot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3eb59879",
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_weights(net):\n",
    "    for m in net.modules():\n",
    "        if isinstance(m, nn.Conv2d):\n",
    "            m.weight.data.normal_(0, 0.02)\n",
    "            m.bias.data.zero_()\n",
    "        elif isinstance(m, nn.ConvTranspose2d):\n",
    "            m.weight.data.normal_(0, 0.02)\n",
    "            m.bias.data.zero_()\n",
    "        elif isinstance(m, nn.Linear):\n",
    "            m.weight.data.normal_(0, 0.02)\n",
    "            m.bias.data.zero_()\n",
    "\n",
    "class Generator(nn.Module):\n",
    "    # Network Architecture is exactly same as in infoGAN (https://arxiv.org/abs/1606.03657)\n",
    "    # Architecture : FC1024_BR-FC7x7x128_BR-(64)4dc2s_BR-(1)4dc2s_S\n",
    "    def __init__(self, input_dim=100, output_dim=1, input_size=32, class_num=10):\n",
    "        super(Generator, self).__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.input_size = input_size\n",
    "        self.class_num = class_num\n",
    "\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(self.input_dim + self.class_num, 1024),\n",
    "            nn.BatchNorm1d(1024),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(1024, 128 * (self.input_size // 4) * (self.input_size // 4)),\n",
    "            nn.BatchNorm1d(128 * (self.input_size // 4) * (self.input_size // 4)),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        self.deconv = nn.Sequential(\n",
    "            nn.ConvTranspose2d(128, 64, 4, 2, 1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(64, self.output_dim, 4, 2, 1),\n",
    "            nn.Tanh(),\n",
    "        )\n",
    "        initialize_weights(self)\n",
    "\n",
    "    def forward(self, input, label):\n",
    "        x = torch.cat([input, label], 1)\n",
    "        x = self.fc(x)\n",
    "        x = x.view(-1, 128, (self.input_size // 4), (self.input_size // 4))\n",
    "        x = self.deconv(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "    \n",
    "class Discriminator(nn.Module):\n",
    "    # Network Architecture is exactly same as in infoGAN (https://arxiv.org/abs/1606.03657)\n",
    "    # Architecture : (64)4c2s-(128)4c2s_BL-FC1024_BL-FC1_S\n",
    "    def __init__(self, input_dim=1, output_dim=1, input_size=32, class_num=10):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.input_size = input_size\n",
    "        self.class_num = class_num\n",
    "\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(self.input_dim, 64, 4, 2, 1),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Conv2d(64, 128, 4, 2, 1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.LeakyReLU(0.2),\n",
    "        )\n",
    "        self.fc1 = nn.Sequential(\n",
    "            nn.Linear(128 * (self.input_size // 4) * (self.input_size // 4), 1024),\n",
    "            nn.BatchNorm1d(1024),\n",
    "            nn.LeakyReLU(0.2),\n",
    "        )\n",
    "        self.dc = nn.Sequential(\n",
    "            nn.Linear(1024, self.output_dim),\n",
    "            nn.Sigmoid(),\n",
    "        )\n",
    "        self.cl = nn.Sequential(\n",
    "            nn.Linear(1024, self.class_num),\n",
    "        )\n",
    "        initialize_weights(self)\n",
    "\n",
    "    def forward(self, input):\n",
    "        x = self.conv(input)\n",
    "        x = x.view(-1, 128 * (self.input_size // 4) * (self.input_size // 4))\n",
    "        x = self.fc1(x)\n",
    "        d = self.dc(x)\n",
    "        c = self.cl(x)\n",
    "\n",
    "        return d, c\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6dcbda96",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "\n",
    "    def __init__(self, nz, ngf, nc):\n",
    "\n",
    "        super(Generator, self).__init__()\n",
    "        self.ReLU = nn.ReLU(True)\n",
    "        self.Tanh = nn.Tanh()\n",
    "        self.DropOut = nn.Dropout(p=0.5)\n",
    "        self.conv1 = nn.ConvTranspose2d(nz, ngf * 8, 4, 1, 0, bias=False)\n",
    "        self.BatchNorm1 = nn.BatchNorm2d(ngf * 8)\n",
    "\n",
    "        self.conv2 = nn.ConvTranspose2d(ngf * 8, ngf * 4, 4, 2, 1, bias=False)\n",
    "        self.BatchNorm2 = nn.BatchNorm2d(ngf * 4)\n",
    "\n",
    "        self.conv3 = nn.ConvTranspose2d(ngf * 4, ngf * 2, 4, 2, 1, bias=False)\n",
    "        self.BatchNorm3 = nn.BatchNorm2d(ngf * 2)\n",
    "\n",
    "        self.conv4 = nn.ConvTranspose2d(ngf * 2, ngf * 1, 4, 2, 1, bias=False)\n",
    "        self.BatchNorm4 = nn.BatchNorm2d(ngf * 1)\n",
    "\n",
    "        self.conv5 = nn.ConvTranspose2d(ngf * 1, nc, 4, 2, 1, bias=False)\n",
    "\n",
    "        self.apply(weights_init)\n",
    "\n",
    "\n",
    "    def forward(self, input):\n",
    "        print(\"input\", input.shape)\n",
    "        x = self.conv1(input)\n",
    "        print(\"conv1\", x.shape)\n",
    "        x = self.BatchNorm1(x)\n",
    "        print(\"BatchNorm1\", x.shape)\n",
    "        x = self.ReLU(x)\n",
    "        print(\"ReLU\", x.shape)\n",
    "        #x = self.DropOut(x)\n",
    "\n",
    "        x = self.conv2(x)\n",
    "        print(\"conv2\", x.shape)\n",
    "        x = self.BatchNorm2(x)\n",
    "        print(\"BatchNorm2\", x.shape)\n",
    "        x = self.ReLU(x)\n",
    "        print(\"ReLU\", x.shape)\n",
    "        #x = self.DropOut(x)\n",
    "\n",
    "        x = self.conv3(x)\n",
    "        print(\"conv3\", x.shape)\n",
    "        x = self.BatchNorm3(x)\n",
    "        print(\"BatchNorm3\", x.shape)\n",
    "        x = self.ReLU(x)\n",
    "        print(\"ReLU\", x.shape)\n",
    "        #x = self.DropOut(x)\n",
    "\n",
    "        x = self.conv4(x)\n",
    "        print(\"conv4\", x.shape)\n",
    "        x = self.BatchNorm4(x)\n",
    "        print(\"BatchNorm4\", x.shape)\n",
    "        x = self.ReLU(x)\n",
    "        print(\"ReLU\", x.shape)\n",
    "        #x = self.DropOut(x)\n",
    "\n",
    "        x = self.conv5(x)\n",
    "        print(\"conv5\", x.shape)\n",
    "        output = self.Tanh(x)\n",
    "        print(\"Tanh\", x.shape)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "62fd43ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "\n",
    "    def __init__(self, nc, ndf, nb_label):\n",
    "\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.LeakyReLU = nn.LeakyReLU(0.2, inplace=True)\n",
    "        self.DropOut1 = nn.Dropout(p=0.5)\n",
    "        self.DropOut2 = nn.Dropout(p=0.25)\n",
    "        self.conv1 = nn.Conv2d(nc, ndf, 4, 2, 1, bias=False)\n",
    "        self.conv2 = nn.Conv2d(ndf, ndf * 2, 4, 2, 1, bias=False)\n",
    "        self.BatchNorm2 = nn.BatchNorm2d(ndf * 2)\n",
    "        self.conv3 = nn.Conv2d(ndf * 2, ndf * 4, 4, 2, 1, bias=False)\n",
    "        self.BatchNorm3 = nn.BatchNorm2d(ndf * 4)\n",
    "        self.conv4 = nn.Conv2d(ndf * 4, ndf * 8, 4, 2, 1, bias=False)\n",
    "        self.BatchNorm4 = nn.BatchNorm2d(ndf * 8)\n",
    "        self.conv5 = nn.Conv2d(ndf * 8, ndf * 1, 4, 1, 0, bias=False)\n",
    "        self.disc_linear = nn.Linear(ndf * 1, 1)\n",
    "        self.aux_linear = nn.Linear(ndf * 1, nb_label)\n",
    "        self.softmax = nn.Softmax()\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        self.ndf = ndf\n",
    "        self.apply(weights_init)\n",
    "\n",
    "    def forward(self, input):\n",
    "\n",
    "        x = self.conv1(input)\n",
    "        print(\"conv1\", x.shape)\n",
    "        x = self.LeakyReLU(x)\n",
    "        print(\"LeakyReLU\", x.shape)\n",
    "        x = self.DropOut1(x)\n",
    "        print(\"DropOut1\", x.shape)\n",
    "\n",
    "        x = self.conv2(x)\n",
    "        print(\"conv2\", x.shape)\n",
    "        x = self.BatchNorm2(x)\n",
    "        print(\"BatchNorm2\", x.shape)\n",
    "        x = self.LeakyReLU(x)\n",
    "        print(\"LeakyReLU\", x.shape)\n",
    "\n",
    "        x = self.conv3(x)\n",
    "        print(\"conv3\", x.shape)\n",
    "        x = self.BatchNorm3(x)\n",
    "        print(\"BatchNorm3\", x.shape)\n",
    "        x = self.LeakyReLU(x)\n",
    "        print(\"LeakyReLU\", x.shape)\n",
    "        x = self.DropOut1(x)\n",
    "        print(\"DropOut1\", x.shape)\n",
    "\n",
    "        x = self.conv4(x)\n",
    "        print(\"conv4\", x.shape)\n",
    "        x = self.BatchNorm4(x)\n",
    "        print(\"BatchNorm4\", x.shape)\n",
    "        x = self.LeakyReLU(x)\n",
    "        print(\"LeakyReLU\", x.shape)\n",
    "        x = self.DropOut2(x)\n",
    "        print(\"DropOut2\", x.shape)\n",
    "\n",
    "        x = self.conv5(x)\n",
    "        print(\"conv5\", x.shape)\n",
    "        x = x.view(-1, self.ndf * 1)\n",
    "        print(\"view ===>\", x.shape)\n",
    "        c = self.aux_linear(x)\n",
    "        print(\"aux_linear\", c.shape)\n",
    "#         c = self.softmax(c)\n",
    "        c = self.sigmoid(c)\n",
    "        print(\"sigmoid\", c.shape)\n",
    "        s = self.disc_linear(x)\n",
    "        print(\"disc_linear\", s.shape)\n",
    "        s = self.sigmoid(s)\n",
    "        print(\"sigmoid\", s.shape)\n",
    "        return s, c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6707e895",
   "metadata": {},
   "outputs": [],
   "source": [
    "def weights_init(m):\n",
    "    classname = m.__class__.__name__\n",
    "    if classname.find('Conv') != -1:\n",
    "        m.weight.data.normal_(0.0, 0.05)\n",
    "    elif classname.find('BatchNorm') != -1:\n",
    "        m.weight.data.normal_(1.0, 0.05)\n",
    "        m.bias.data.fill_(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "220b9685",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_path = './data'\n",
    "base_folder = \"Covid-19 Image Dataset\"\n",
    "classic_folder = 'Coivd-19_Classic'\n",
    "synthetic_folder = 'Coivd-19_Synthetic'\n",
    "data_dir = os.path.join(base_path, classic_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9b6ea5a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_size = 512\n",
    "batch_size = 16\n",
    "train_path = os.path.join(data_dir, \"train\")\n",
    "test_path = os.path.join(data_dir, \"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1bf35c4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Covid', 'Normal', 'Viral Pneumonia']\n"
     ]
    }
   ],
   "source": [
    "labels = os.listdir(train_path)\n",
    "print(labels)\n",
    "no_norm = mat_color.Normalize(vmin=0, vmax=255, clip=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "28c7ac6b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([16])\n",
      "torch.Size([16])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[[ 1.0000]],\n",
       "\n",
       "         [[ 0.0000]],\n",
       "\n",
       "         [[ 0.0000]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[ 0.5422]],\n",
       "\n",
       "         [[-0.0727]],\n",
       "\n",
       "         [[-0.0704]]],\n",
       "\n",
       "\n",
       "        [[[ 0.0000]],\n",
       "\n",
       "         [[ 0.0000]],\n",
       "\n",
       "         [[ 1.0000]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[-0.9718]],\n",
       "\n",
       "         [[-0.6538]],\n",
       "\n",
       "         [[ 0.7630]]],\n",
       "\n",
       "\n",
       "        [[[ 0.0000]],\n",
       "\n",
       "         [[ 0.0000]],\n",
       "\n",
       "         [[ 1.0000]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[ 0.5645]],\n",
       "\n",
       "         [[-0.0455]],\n",
       "\n",
       "         [[ 1.1549]]],\n",
       "\n",
       "\n",
       "        ...,\n",
       "\n",
       "\n",
       "        [[[ 0.0000]],\n",
       "\n",
       "         [[ 1.0000]],\n",
       "\n",
       "         [[ 0.0000]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[-0.7931]],\n",
       "\n",
       "         [[-1.8942]],\n",
       "\n",
       "         [[-0.3067]]],\n",
       "\n",
       "\n",
       "        [[[ 0.0000]],\n",
       "\n",
       "         [[ 1.0000]],\n",
       "\n",
       "         [[ 0.0000]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[ 2.1853]],\n",
       "\n",
       "         [[ 0.4596]],\n",
       "\n",
       "         [[ 1.6194]]],\n",
       "\n",
       "\n",
       "        [[[ 1.0000]],\n",
       "\n",
       "         [[ 0.0000]],\n",
       "\n",
       "         [[ 0.0000]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[ 2.5612]],\n",
       "\n",
       "         [[-1.9233]],\n",
       "\n",
       "         [[ 0.8950]]]], device='cuda:0')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Number of training epochs\n",
    "num_epochs = 100\n",
    "\n",
    "#All images will be resized to this size using a transformer.\n",
    "#image_size = 64\n",
    "imageSize = 512\n",
    "\n",
    "# Number of channels in the training images. For color images this is 3\n",
    "nc = 3\n",
    "\n",
    "# Size of z latent vector (i.e. size of generator input)\n",
    "nz = 128\n",
    "\n",
    "# Size of feature maps in generator\n",
    "ngf = imageSize\n",
    "\n",
    "# Size of feature maps in discriminator\n",
    "ndf = imageSize\n",
    "\n",
    "# No of labels\n",
    "nb_label = len(labels)\n",
    "\n",
    "# Learning rate for optimizers\n",
    "lr = 0.002\n",
    "lr_d = 0.0002\n",
    "# Beta1 hyperparam for Adam optimizers\n",
    "beta1 = 0.5\n",
    "# Beta2 hyperparam for Adam optimizers\n",
    "beta2 = 0.999\n",
    "\n",
    "real_label = 1.\n",
    "fake_label = 0.\n",
    "# Input to generator\n",
    "fixed_noise = torch.randn(64, nz, 1, 1, device=device) #batch of 64\n",
    "# Define Loss function\n",
    "s_criterion = nn.BCELoss().to(device) #For synthesizing\n",
    "c_criterion = nn.NLLLoss().to(device) #For classification\n",
    "\n",
    "input = torch.FloatTensor(batch_size, nc, imageSize, imageSize).to(device)\n",
    "noise = torch.FloatTensor(batch_size, nz, 1, 1).to(device)\n",
    "fixed_noise = torch.FloatTensor(batch_size, nz, 1, 1).normal_(0, 1).to(device)\n",
    "s_label = torch.FloatTensor(batch_size).to(device)\n",
    "c_label = torch.LongTensor(batch_size).to(device)\n",
    "# s_label = torch.FloatTensor(batch_size).to(device)\n",
    "# c_label = torch.LongTensor(batch_size).to(device)\n",
    "\n",
    "input = Variable(input)\n",
    "s_label = Variable(s_label)\n",
    "c_label = Variable(c_label)\n",
    "print(s_label.shape)\n",
    "print(c_label.shape)\n",
    "noise = Variable(noise)\n",
    "fixed_noise = Variable(fixed_noise)\n",
    "fixed_noise_ = np.random.normal(0, 1, (batch_size, nz))\n",
    "random_label = np.random.randint(0, nb_label, batch_size)\n",
    "#print('fixed label:{}'.format(random_label))\n",
    "random_onehot = np.zeros((batch_size, nb_label))\n",
    "random_onehot[np.arange(batch_size), random_label] = 1\n",
    "fixed_noise_[np.arange(batch_size), :nb_label] = random_onehot[np.arange(batch_size)]\n",
    "\n",
    "\n",
    "fixed_noise_ = (torch.from_numpy(fixed_noise_))\n",
    "fixed_noise_ = fixed_noise_.resize_(batch_size, nz, 1, 1)\n",
    "fixed_noise.data.copy_(fixed_noise_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bd17020a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[WinError 183] 当文件已存在时，无法创建该文件。: '.\\\\GANAug'\n",
      "[WinError 183] 当文件已存在时，无法创建该文件。: '.\\\\GANAug/model'\n",
      "[WinError 183] 当文件已存在时，无法创建该文件。: '.\\\\GANAug/plots'\n",
      "[WinError 183] 当文件已存在时，无法创建该文件。: '.\\\\GANAug/model/ACGAN'\n",
      "[WinError 183] 当文件已存在时，无法创建该文件。: '.\\\\GANAug/plots/ACGAN'\n",
      "[WinError 183] 当文件已存在时，无法创建该文件。: '.\\\\GANAug/output_images'\n",
      "[WinError 183] 当文件已存在时，无法创建该文件。: '.\\\\GANAug/output_images/ACGAN'\n"
     ]
    }
   ],
   "source": [
    "for func in [\n",
    "    lambda: os.mkdir(os.path.join('.', 'GANAug')),\n",
    "    lambda: os.mkdir(os.path.join('.', 'GANAug/model')),\n",
    "    lambda: os.mkdir(os.path.join('.', 'GANAug/plots')),\n",
    "    lambda: os.mkdir(os.path.join('.', 'GANAug/model/ACGAN')),\n",
    "    lambda: os.mkdir(os.path.join('.', 'GANAug/plots/ACGAN')),\n",
    "    lambda: os.mkdir(os.path.join('.', 'GANAug/output_images')),\n",
    "    lambda: os.mkdir(os.path.join('.', 'GANAug/output_images/ACGAN'))]:  # create directories\n",
    "    try:\n",
    "        func()\n",
    "    except Exception as error:\n",
    "        print(error)\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "74dda24a",
   "metadata": {},
   "outputs": [],
   "source": [
    "METRIC_FIELDS = [\n",
    "    'train.D_x',\n",
    "    'train.D_G_z1',\n",
    "    'train.D_G_z2',\n",
    "    'train.G_losses',\n",
    "    'train.D_losses',\n",
    "]\n",
    "metrics = {field: list() for field in METRIC_FIELDS}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a26a75d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset(train_dir=train_path, test_dir=test_path):\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((img_size, img_size)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "        ])\n",
    "    train_data = datasets.ImageFolder(train_dir, transform=transform)\n",
    "    train_loader = DataLoader(train_data, batch_size, shuffle=True, num_workers=1)\n",
    "\n",
    "    test_data = datasets.ImageFolder(test_dir ,transform=transform)\n",
    "    test_loader = DataLoader(test_data, batch_size, shuffle=True, num_workers=1)\n",
    "\n",
    "    return train_loader, test_loader, train_data, test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bbb0b552",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader, test_loader, train_data, test_data = load_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0360680c",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "CUDA out of memory. Tried to allocate 8.00 GiB (GPU 0; 6.00 GiB total capacity; 48.55 MiB already allocated; 4.61 GiB free; 50.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Input \u001b[1;32mIn [14]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[0m generator \u001b[38;5;241m=\u001b[39m \u001b[43mGenerator\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_dim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnz\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_dim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mngf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclass_num\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnb_label\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      2\u001b[0m discriminator \u001b[38;5;241m=\u001b[39m Discriminator(input_dim\u001b[38;5;241m=\u001b[39mnc, output_dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, input_size\u001b[38;5;241m=\u001b[39mndf, class_num\u001b[38;5;241m=\u001b[39mnb_label)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# setup optimizer\u001b[39;00m\n",
      "File \u001b[1;32mD:\\Miniconda3\\envs\\torch_1-11\\lib\\site-packages\\torch\\nn\\modules\\module.py:907\u001b[0m, in \u001b[0;36mModule.to\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    903\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(device, dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    904\u001b[0m                     non_blocking, memory_format\u001b[38;5;241m=\u001b[39mconvert_to_format)\n\u001b[0;32m    905\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(device, dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, non_blocking)\n\u001b[1;32m--> 907\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mD:\\Miniconda3\\envs\\torch_1-11\\lib\\site-packages\\torch\\nn\\modules\\module.py:578\u001b[0m, in \u001b[0;36mModule._apply\u001b[1;34m(self, fn)\u001b[0m\n\u001b[0;32m    576\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_apply\u001b[39m(\u001b[38;5;28mself\u001b[39m, fn):\n\u001b[0;32m    577\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[1;32m--> 578\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    580\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[0;32m    581\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[0;32m    582\u001b[0m             \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[0;32m    583\u001b[0m             \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    588\u001b[0m             \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[0;32m    589\u001b[0m             \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[1;32mD:\\Miniconda3\\envs\\torch_1-11\\lib\\site-packages\\torch\\nn\\modules\\module.py:578\u001b[0m, in \u001b[0;36mModule._apply\u001b[1;34m(self, fn)\u001b[0m\n\u001b[0;32m    576\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_apply\u001b[39m(\u001b[38;5;28mself\u001b[39m, fn):\n\u001b[0;32m    577\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[1;32m--> 578\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    580\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[0;32m    581\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[0;32m    582\u001b[0m             \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[0;32m    583\u001b[0m             \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    588\u001b[0m             \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[0;32m    589\u001b[0m             \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[1;32mD:\\Miniconda3\\envs\\torch_1-11\\lib\\site-packages\\torch\\nn\\modules\\module.py:601\u001b[0m, in \u001b[0;36mModule._apply\u001b[1;34m(self, fn)\u001b[0m\n\u001b[0;32m    597\u001b[0m \u001b[38;5;66;03m# Tensors stored in modules are graph leaves, and we don't want to\u001b[39;00m\n\u001b[0;32m    598\u001b[0m \u001b[38;5;66;03m# track autograd history of `param_applied`, so we have to use\u001b[39;00m\n\u001b[0;32m    599\u001b[0m \u001b[38;5;66;03m# `with torch.no_grad():`\u001b[39;00m\n\u001b[0;32m    600\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m--> 601\u001b[0m     param_applied \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparam\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    602\u001b[0m should_use_set_data \u001b[38;5;241m=\u001b[39m compute_should_use_set_data(param, param_applied)\n\u001b[0;32m    603\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m should_use_set_data:\n",
      "File \u001b[1;32mD:\\Miniconda3\\envs\\torch_1-11\\lib\\site-packages\\torch\\nn\\modules\\module.py:905\u001b[0m, in \u001b[0;36mModule.to.<locals>.convert\u001b[1;34m(t)\u001b[0m\n\u001b[0;32m    902\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m convert_to_format \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m t\u001b[38;5;241m.\u001b[39mdim() \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;241m4\u001b[39m, \u001b[38;5;241m5\u001b[39m):\n\u001b[0;32m    903\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(device, dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    904\u001b[0m                 non_blocking, memory_format\u001b[38;5;241m=\u001b[39mconvert_to_format)\n\u001b[1;32m--> 905\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_floating_point\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_complex\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnon_blocking\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 8.00 GiB (GPU 0; 6.00 GiB total capacity; 48.55 MiB already allocated; 4.61 GiB free; 50.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "generator = Generator(input_dim=nz, output_dim=nc, input_size=ngf, class_num=nb_label).to(device)\n",
    "discriminator = Discriminator(input_dim=nc, output_dim=1, input_size=ndf, class_num=nb_label).to(device)\n",
    "\n",
    "# setup optimizer\n",
    "optimizerD = optim.Adam(discriminator.parameters(), lr=lr_d, betas=(beta1, beta2))\n",
    "optimizerG = optim.Adam(generator.parameters(), lr=lr, betas=(beta1, beta2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8d141b1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input torch.Size([1, 128, 512, 3])\n",
      "conv1 torch.Size([1, 4096, 515, 6])\n",
      "BatchNorm1 torch.Size([1, 4096, 515, 6])\n",
      "ReLU torch.Size([1, 4096, 515, 6])\n",
      "conv2 torch.Size([1, 2048, 1030, 12])\n",
      "BatchNorm2 torch.Size([1, 2048, 1030, 12])\n",
      "ReLU torch.Size([1, 2048, 1030, 12])\n",
      "conv3 torch.Size([1, 1024, 2060, 24])\n",
      "BatchNorm3 torch.Size([1, 1024, 2060, 24])\n",
      "ReLU torch.Size([1, 1024, 2060, 24])\n",
      "conv4 torch.Size([1, 512, 4120, 48])\n",
      "BatchNorm4 torch.Size([1, 512, 4120, 48])\n",
      "ReLU torch.Size([1, 512, 4120, 48])\n",
      "conv5 torch.Size([1, 3, 8240, 96])\n",
      "Tanh torch.Size([1, 3, 8240, 96])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 3, 8240, 96])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "g_output = generator(torch.randn(1, nz, ngf, nc, device=device))\n",
    "g_output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8244b33d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conv1 torch.Size([16, 512, 256, 256])\n",
      "LeakyReLU torch.Size([16, 512, 256, 256])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA out of memory. Tried to allocate 512.00 MiB (GPU 0; 6.00 GiB total capacity; 4.99 GiB already allocated; 0 bytes free; 5.20 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Input \u001b[1;32mIn [18]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[0m s_output, c_output \u001b[38;5;241m=\u001b[39m \u001b[43mdiscriminator\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrandn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m16\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m512\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m512\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      2\u001b[0m s_output\n",
      "File \u001b[1;32mD:\\Miniconda3\\envs\\torch_1-11\\lib\\site-packages\\torch\\nn\\modules\\module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1106\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1107\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1108\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1109\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1110\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1111\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1112\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Input \u001b[1;32mIn [6]\u001b[0m, in \u001b[0;36mDiscriminator.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m     28\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mLeakyReLU(x)\n\u001b[0;32m     29\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLeakyReLU\u001b[39m\u001b[38;5;124m\"\u001b[39m, x\u001b[38;5;241m.\u001b[39mshape)\n\u001b[1;32m---> 30\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mDropOut1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     31\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDropOut1\u001b[39m\u001b[38;5;124m\"\u001b[39m, x\u001b[38;5;241m.\u001b[39mshape)\n\u001b[0;32m     33\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv2(x)\n",
      "File \u001b[1;32mD:\\Miniconda3\\envs\\torch_1-11\\lib\\site-packages\\torch\\nn\\modules\\module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1106\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1107\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1108\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1109\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1110\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1111\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1112\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32mD:\\Miniconda3\\envs\\torch_1-11\\lib\\site-packages\\torch\\nn\\modules\\dropout.py:58\u001b[0m, in \u001b[0;36mDropout.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m     57\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m---> 58\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdropout\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minplace\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mD:\\Miniconda3\\envs\\torch_1-11\\lib\\site-packages\\torch\\nn\\functional.py:1279\u001b[0m, in \u001b[0;36mdropout\u001b[1;34m(input, p, training, inplace)\u001b[0m\n\u001b[0;32m   1277\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m p \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m0.0\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m p \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1.0\u001b[39m:\n\u001b[0;32m   1278\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdropout probability has to be between 0 and 1, \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbut got \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(p))\n\u001b[1;32m-> 1279\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _VF\u001b[38;5;241m.\u001b[39mdropout_(\u001b[38;5;28minput\u001b[39m, p, training) \u001b[38;5;28;01mif\u001b[39;00m inplace \u001b[38;5;28;01melse\u001b[39;00m \u001b[43m_VF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdropout\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtraining\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 512.00 MiB (GPU 0; 6.00 GiB total capacity; 4.99 GiB already allocated; 0 bytes free; 5.20 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "s_output, c_output = discriminator(torch.randn(16, 3, 512, 512, device=device))\n",
    "s_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94ee3b40",
   "metadata": {},
   "outputs": [],
   "source": [
    "exit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "065eed00",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install torchviz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81981a02",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print(generator)\n",
    "print(discriminator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e2592f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "SHOW = True\n",
    "if SHOW:\n",
    "    summary(generator, (nz, ngf, nc), batch_size=1, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57c2031b",
   "metadata": {},
   "outputs": [],
   "source": [
    "if SHOW:\n",
    "    summary(discriminator, (nc, ndf, ndf), batch_size=1, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0580f119",
   "metadata": {},
   "outputs": [],
   "source": [
    "SHOW_IMG = False\n",
    "if SHOW_IMG:\n",
    "    def modeltorchviz(model,input2):\n",
    "        y = model(input2.cuda())    # 获取网络的预测值\n",
    "        MyConvNetVis = make_dot(y, params=dict(list(model.named_parameters()) + [('x', input2)]))\n",
    "        MyConvNetVis.format = \"png\"\n",
    "        # 指定文件生成的文件夹\n",
    "        MyConvNetVis.directory = \"images\"\n",
    "        # 生成文件\n",
    "        MyConvNetVis.view() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0cd9edc",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "if SHOW_IMG:\n",
    "    modeltorchviz(generator, torch.randn(1, nz, ngf, nc).requires_grad_(True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d02b9f92",
   "metadata": {},
   "outputs": [],
   "source": [
    "if SHOW_IMG:\n",
    "    modeltorchviz(discriminator, torch.randn(1, nc, ndf, ndf).requires_grad_(True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d1b6459",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(predict, labels):\n",
    "    correct = 0\n",
    "    pred = predict.data.max(1)[1]\n",
    "    correct = pred.eq(labels.data).cpu().sum()\n",
    "    return correct, len(labels.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0714f81e",
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(2):\n",
    "    for i, data in enumerate(tqdm(train_loader, 0)):\n",
    "        ###########################\n",
    "        # (1) Update D network\n",
    "        ###########################\n",
    "        # train with real\n",
    "        discriminator.zero_grad()\n",
    "        img, label = data\n",
    "        batch_size = img.size(0)\n",
    "        with torch.no_grad():\n",
    "            input.resize_(img.size()).copy_(img)\n",
    "            print(input.shape)\n",
    "            print(label)\n",
    "            print(label.shape)\n",
    "            print(s_label.shape)\n",
    "            print(c_label.shape)\n",
    "            s_label.resize_(batch_size).fill_(real_label)\n",
    "            c_label.resize_(batch_size).copy_(label)\n",
    "            print(s_label.shape)\n",
    "            print(c_label.shape)\n",
    "        s_output, c_output = discriminator(input)\n",
    "        print(s_output.shape)\n",
    "        print(c_output.shape)\n",
    "        \n",
    "        s_errD_real = s_criterion(s_output, s_label)\n",
    "        c_errD_real = c_criterion(c_output, c_label)\n",
    "        errD_real = s_errD_real + c_errD_real\n",
    "        errD_real.backward()\n",
    "        D_x = s_output.data.mean()\n",
    "        \n",
    "        correct, length = test(c_output, c_label)\n",
    "\n",
    "        # train with fake\n",
    "        with torch.no_grad():\n",
    "            noise.resize_(batch_size, nz, 1, 1)\n",
    "            noise.normal_(0, 1)\n",
    "\n",
    "        label = np.random.randint(0, nb_label, batch_size)\n",
    "        noise_ = np.random.normal(0, 1, (batch_size, nz))\n",
    "        label_onehot = np.zeros((batch_size, nb_label))\n",
    "        label_onehot[np.arange(batch_size), label] = 1\n",
    "        noise_[np.arange(batch_size), :nb_label] = label_onehot[np.arange(batch_size)]\n",
    "        \n",
    "        noise_ = (torch.from_numpy(noise_))\n",
    "        noise_ = noise_.resize_(batch_size, nz, 1, 1)\n",
    "        noise.data.copy_(noise_)\n",
    "\n",
    "        c_label.data.resize_(batch_size).copy_(torch.from_numpy(label))\n",
    "\n",
    "        fake = generator(noise)\n",
    "        s_label.data.fill_(fake_label)\n",
    "        s_output,c_output = discriminator(fake.detach())\n",
    "        s_errD_fake = s_criterion(s_output, s_label)\n",
    "        c_errD_fake = c_criterion(c_output, c_label)\n",
    "        errD_fake = s_errD_fake + c_errD_fake\n",
    "\n",
    "        errD_fake.backward()\n",
    "        D_G_z1 = s_output.data.mean()\n",
    "        errD = s_errD_real + s_errD_fake\n",
    "        optimizerD.step()\n",
    "\n",
    "        ###########################\n",
    "        # (2) Update G network\n",
    "        ###########################\n",
    "        generator.zero_grad()\n",
    "        s_label.data.fill_(real_label)  # fake labels are real for generator cost\n",
    "        s_output,c_output = discriminator(fake)\n",
    "        s_errG = s_criterion(s_output, s_label)\n",
    "        c_errG = c_criterion(c_output, c_label)\n",
    "        \n",
    "        errG = s_errG + c_errG\n",
    "        errG.backward()\n",
    "        D_G_z2 = s_output.data.mean()\n",
    "        optimizerG.step()\n",
    "        metrics['train.G_losses'].append(errG.item())\n",
    "        metrics['train.D_losses'].append(errD.item())\n",
    "\n",
    "        print('[%d/%d][%d/%d] Loss_D: %.4f Loss_G: %.4f D(x): %.4f D(G(z)): %.4f / %.4f, Accuracy: %.4f / %.4f = %.4f'\n",
    "              % (epoch, num_epochs, i, len(train_loader),\n",
    "                 errD.data, errG.data, D_x, D_G_z1, D_G_z2,\n",
    "                 correct, length, 100.* correct / length))\n",
    "        if i % 100 == 0:\n",
    "            vutils.save_image(img,\n",
    "                    '%s/real_samples.jpg' % './augGAN/output_images/ACGAN', normalize=True)\n",
    "            #fake = netG(fixed_cat)\n",
    "            fake = generator(fixed_noise)\n",
    "            vutils.save_image(fake.data,\n",
    "                    '%s/fake_samples_epoch_%03d.jpg' % ('./augGAN/output_images/ACGAN', epoch), normalize=True)\n",
    "\n",
    "    # do checkpointing\n",
    "    #torch.save(generator.state_dict(), '%s/netG_epoch_%d.pth' % (os.path.join('.', 'augGAN/model/ACGAN'), epoch))\n",
    "    #torch.save(discriminator.state_dict(), '%s/netD_epoch_%d.pth' % (os.path.join('.', 'augGAN/model/ACGAN'), epoch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09e18531",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test2(generator, discriminator, num_epochs, metrics, loader):\n",
    "    print('Testing Block.........')\n",
    "    now = datetime.datetime.now()\n",
    "    #g_losses = metrics['train.G_losses'][-1]\n",
    "    #d_losses = metrics['train.D_losses'][-1]\n",
    "    path='augGAN/output_images/ACGAN'\n",
    "    try:\n",
    "      os.mkdir(os.path.join('.', path))\n",
    "    except Exception as error:\n",
    "      print(error)\n",
    "\n",
    "    real_batch = next(iter(loader))\n",
    "    \n",
    "    test_img_list = []\n",
    "    test_noise = torch.randn(batch_size, nz, 1, 1, device=device)\n",
    "    test_fake = generator(test_noise).detach().cpu()\n",
    "    test_img_list.append(vutils.make_grid(test_fake, padding=2, normalize=True))\n",
    "\n",
    "    fig = plt.figure(figsize=(15,15))\n",
    "    ax1 = plt.subplot(1,2,1)\n",
    "    ax1 = plt.axis(\"off\")\n",
    "    ax1 = plt.title(\"Real Images\")\n",
    "    ax1 = plt.imshow(np.transpose(vutils.make_grid(real_batch[0].to(device)[:64], padding=5, normalize=True).cpu(),(1,2,0)))\n",
    "\n",
    "    ax2 = plt.subplot(1,2,2)\n",
    "    ax2 = plt.axis(\"off\")\n",
    "    ax2 = plt.title(\"Fake Images\")\n",
    "    ax2 = plt.imshow(np.transpose(test_img_list[-1],(1,2,0)))\n",
    "    #ax2 = plt.show()\n",
    "    #fig.savefig('%s/image_%.3f_%.3f_%d_%s.png' %\n",
    "    #                (path, g_losses, d_losses, num_epochs, now.strftime(\"%Y-%m-%d_%H:%M:%S\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9cca5ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot(name, train_epoch, values, path, save):\n",
    "    clear_output(wait=True)\n",
    "    plt.close('all')\n",
    "    fig = plt.figure()\n",
    "    fig = plt.ion()\n",
    "    fig = plt.subplot(1, 1, 1)\n",
    "    fig = plt.title('epoch: %s -> %s: %s' % (train_epoch, name, values[-1]))\n",
    "    fig = plt.ylabel(name)\n",
    "    fig = plt.xlabel('train_set')\n",
    "    fig = plt.plot(values)\n",
    "    fig = plt.grid()\n",
    "    get_fig = plt.gcf()\n",
    "    fig = plt.draw()  # draw the plot\n",
    "    fig = plt.pause(1)  # show it for 1 second\n",
    "    if save:\n",
    "        now = datetime.datetime.now()\n",
    "        get_fig.savefig('%s/%s_%.3f_%d_%s.png' %\n",
    "                        (path, name, train_epoch, values[-1], now.strftime(\"%Y-%m-%d_%H:%M:%S\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d5e7892",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model(generator, discriminator, gen_optimizer, dis_optimizer, metrics, num_epochs):\n",
    "    now = datetime.datetime.now()\n",
    "    g_losses = metrics['train.G_losses'][-1]\n",
    "    d_losses = metrics['train.D_losses'][-1]\n",
    "    name = \"%+.3f_%+.3f_%d_%s.dat\" % (g_losses, d_losses, num_epochs, now.strftime(\"%Y-%m-%d_%H:%M:%S\"))\n",
    "    # fname = os.path.join('.', 'augGAN/model', name)\n",
    "    # states = {\n",
    "    #         'state_dict_generator': generator.state_dict(),\n",
    "    #         'state_dict_discriminator': discriminator.state_dict(),\n",
    "    #         'gen_optimizer': gen_optimizer.state_dict(),\n",
    "    #         'dis_optimizer': dis_optimizer.state_dict(),\n",
    "    #         'metrics': metrics,\n",
    "    #         'train_epoch': num_epochs,\n",
    "    #         'date': now.strftime(\"%Y-%m-%d_%H:%M:%S\"),\n",
    "    # }\n",
    "    # torch.save(states, fname)\n",
    "    path='augGAN/plots/ACGAN/train_%+.3f_%+.3f_%s'% (g_losses, d_losses, now.strftime(\"%Y-%m-%d_%H:%M:%S\"))\n",
    "    try:\n",
    "        os.mkdir(os.path.join('.', path))\n",
    "    except Exception as error:\n",
    "        print(error)\n",
    "\n",
    "    plot('G_losses', num_epochs, metrics['train.G_losses'], path, True)\n",
    "    plot('D_losses', num_epochs, metrics['train.D_losses'], path, True)\n",
    "    plot('D_x', num_epochs, metrics['train.D_x'], path, True)\n",
    "    plot('D_G_z1', num_epochs, metrics['train.D_G_z1'], path, True)\n",
    "    plot('D_G_z2', num_epochs, metrics['train.D_G_z2'], path, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a280fe2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "test2(generator, discriminator, num_epochs, metrics, train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f66ad873",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_model(generator, discriminator, optimizerG, optimizerD, metrics, num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c56cd9c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_batch = 16\n",
    "test_fake = 1\n",
    "\n",
    "if test_fake:\n",
    "    #check for fake image\n",
    "    test_img_list = []\n",
    "    test_noise = torch.randn(test_batch, nz, 1, 1, device=device)\n",
    "    test_img = generator(test_noise)#.detach().cpu()\n",
    "\n",
    "else:\n",
    "    #check for real image\n",
    "    test_loader = torch.utils.data.DataLoader(train_set, batch_size=test_batch,\n",
    "                                            shuffle=True)\n",
    "    data = next(iter(test_loader))\n",
    "    test_noise, test_class_lable = data\n",
    "    test_img.data.resize_(test_noise.size()).copy_(test_noise)\n",
    "    #print(data[0].size())\n",
    "    print('class label for real', test_class_lable)\n",
    "\n",
    "s_output,c_label_op = discriminator(test_img.detach().to(device))\n",
    "print('Discriminator s o/p', s_output)\n",
    "print('Discriminator c o/p', c_label_op)\n",
    "\n",
    "# label = np.random.randint(0, nb_label, batch_size)\n",
    "# c_label.data.resize_(batch_size).copy_(torch.from_numpy(label))\n",
    "# print(c_label)\n",
    "\n",
    "test_img = test_img.detach().cpu()\n",
    "test_img_list.append(vutils.make_grid(test_img, padding=2, normalize=True))\n",
    "plt.imshow(np.transpose(test_img_list[-1],(1,2,0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d45fe52a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
