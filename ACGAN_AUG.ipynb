{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "370513dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: http://mirrors.aliyun.com/pypi/simple\n",
      "Requirement already satisfied: opencv-python in /root/miniconda3/lib/python3.8/site-packages (4.5.5.64)\n",
      "Requirement already satisfied: torchsummary in /root/miniconda3/lib/python3.8/site-packages (1.5.1)\n",
      "Requirement already satisfied: scikit-learn in /root/miniconda3/lib/python3.8/site-packages (1.1.1)\n",
      "Requirement already satisfied: torchviz in /root/miniconda3/lib/python3.8/site-packages (0.0.2)\n",
      "Requirement already satisfied: utils in /root/miniconda3/lib/python3.8/site-packages (1.0.1)\n",
      "Requirement already satisfied: numpy>=1.14.5 in /root/miniconda3/lib/python3.8/site-packages (from opencv-python) (1.21.4)\n",
      "Requirement already satisfied: scipy>=1.3.2 in /root/miniconda3/lib/python3.8/site-packages (from scikit-learn) (1.8.1)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /root/miniconda3/lib/python3.8/site-packages (from scikit-learn) (3.1.0)\n",
      "Requirement already satisfied: joblib>=1.0.0 in /root/miniconda3/lib/python3.8/site-packages (from scikit-learn) (1.1.0)\n",
      "Requirement already satisfied: torch in /root/miniconda3/lib/python3.8/site-packages (from torchviz) (1.10.0+cu113)\n",
      "Requirement already satisfied: graphviz in /root/miniconda3/lib/python3.8/site-packages (from torchviz) (0.20)\n",
      "Requirement already satisfied: typing-extensions in /root/miniconda3/lib/python3.8/site-packages (from torch->torchviz) (4.0.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install opencv-python torchsummary scikit-learn torchviz utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6a6a2202",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.parallel\n",
    "import torch.nn.functional as F\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.optim as optim\n",
    "import torch.utils.data\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.utils as vutils\n",
    "import utils\n",
    "from torch.nn.functional import one_hot\n",
    "from torchvision.utils import save_image\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import SubsetRandomSampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6cf2ef10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['sm_37', 'sm_50', 'sm_60', 'sm_70', 'sm_75', 'sm_80', 'sm_86'] cuda\n"
     ]
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(torch.cuda.get_arch_list(), device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b8079c45",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import argparse\n",
    "import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import HTML\n",
    "from IPython.display import clear_output\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "from tqdm import notebook\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import gc\n",
    "import matplotlib.colors as mat_color\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import random\n",
    "import cv2\n",
    "from torchvision.datasets import ImageNet, ImageFolder\n",
    "from torch.utils.data import DataLoader\n",
    "from torchsummary import summary\n",
    "from torchviz import make_dot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3eb59879",
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_weights(net):\n",
    "    for m in net.modules():\n",
    "        if isinstance(m, nn.Conv2d):\n",
    "            m.weight.data.normal_(0, 0.02)\n",
    "            m.bias.data.zero_()\n",
    "        elif isinstance(m, nn.ConvTranspose2d):\n",
    "            m.weight.data.normal_(0, 0.02)\n",
    "            m.bias.data.zero_()\n",
    "        elif isinstance(m, nn.Linear):\n",
    "            m.weight.data.normal_(0, 0.02)\n",
    "            m.bias.data.zero_()\n",
    "\n",
    "class Generator(nn.Module):\n",
    "    # Network Architecture is exactly same as in infoGAN (https://arxiv.org/abs/1606.03657)\n",
    "    # Architecture : FC1024_BR-FC7x7x128_BR-(64)4dc2s_BR-(1)4dc2s_S\n",
    "    def __init__(self, input_dim=100, output_dim=1, input_size=32, class_num=10):\n",
    "        super(Generator, self).__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.input_size = input_size\n",
    "        self.class_num = class_num\n",
    "\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(self.input_dim + self.class_num, 1024),\n",
    "            nn.BatchNorm1d(1024),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(1024, 128 * (self.input_size // 4) * (self.input_size // 4)),\n",
    "            nn.BatchNorm1d(128 * (self.input_size // 4) * (self.input_size // 4)),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        self.deconv = nn.Sequential(\n",
    "            nn.ConvTranspose2d(128, 64, 4, 2, 1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(64, self.output_dim, 4, 2, 1),\n",
    "            nn.Tanh(),\n",
    "        )\n",
    "        initialize_weights(self)\n",
    "\n",
    "    def forward(self, input, label):\n",
    "        x = torch.cat([input, label], 1)\n",
    "        x = self.fc(x)\n",
    "        x = x.view(-1, 128, (self.input_size // 4), (self.input_size // 4))\n",
    "        x = self.deconv(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "    \n",
    "class Discriminator(nn.Module):\n",
    "    # Network Architecture is exactly same as in infoGAN (https://arxiv.org/abs/1606.03657)\n",
    "    # Architecture : (64)4c2s-(128)4c2s_BL-FC1024_BL-FC1_S\n",
    "    def __init__(self, input_dim=1, output_dim=1, input_size=32, class_num=10):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.input_size = input_size\n",
    "        self.class_num = class_num\n",
    "\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(self.input_dim, 64, 4, 2, 1),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Conv2d(64, 128, 4, 2, 1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.LeakyReLU(0.2),\n",
    "        )\n",
    "        self.fc1 = nn.Sequential(\n",
    "            nn.Linear(128 * (self.input_size // 4) * (self.input_size // 4), 1024),\n",
    "            nn.BatchNorm1d(1024),\n",
    "            nn.LeakyReLU(0.2),\n",
    "        )\n",
    "        self.dc = nn.Sequential(\n",
    "            nn.Linear(1024, self.output_dim),\n",
    "            nn.Sigmoid(),\n",
    "        )\n",
    "        self.cl = nn.Sequential(\n",
    "            nn.Linear(1024, self.class_num),\n",
    "        )\n",
    "        initialize_weights(self)\n",
    "\n",
    "    def forward(self, input):\n",
    "        x = self.conv(input)\n",
    "        x = x.view(-1, 128 * (self.input_size // 4) * (self.input_size // 4))\n",
    "        x = self.fc1(x)\n",
    "        d = self.dc(x)\n",
    "        c = self.cl(x)\n",
    "\n",
    "        return d, c\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "220b9685",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_path = './data'\n",
    "base_folder = \"Covid-19 Image Dataset\"\n",
    "classic_folder = 'Coivd-19_Classic'\n",
    "synthetic_folder = 'Coivd-19_Synthetic'\n",
    "data_dir = os.path.join(base_path, classic_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9b6ea5a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "train_path = os.path.join(data_dir, \"train\")\n",
    "test_path = os.path.join(data_dir, \"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1bf35c4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Covid', 'Normal', 'Viral Pneumonia']\n"
     ]
    }
   ],
   "source": [
    "labels = os.listdir(train_path)\n",
    "print(labels)\n",
    "no_norm = mat_color.Normalize(vmin=0, vmax=255, clip=False)\n",
    "label_dict = {\n",
    "    0 : 'Covid', \n",
    "    1 : 'Normal', \n",
    "    2 : 'Viral Pneumonia'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "28c7ac6b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64])\n",
      "torch.Size([64, 3])\n",
      "torch.Size([64, 512])\n",
      "torch.Size([3, 512])\n",
      "torch.Size([3, 3])\n"
     ]
    }
   ],
   "source": [
    "# Number of training epochs\n",
    "num_epochs = 10\n",
    "\n",
    "#All images will be resized to this size using a transformer.\n",
    "img_size = 512\n",
    "\n",
    "# Number of channels in the training images. For color images this is 3\n",
    "nc = 3\n",
    "\n",
    "# Size of z latent vector (i.e. size of generator input)\n",
    "nz = img_size\n",
    "\n",
    "# Size of feature maps in generator\n",
    "ngf = img_size\n",
    "\n",
    "# Size of feature maps in discriminator\n",
    "ndf = img_size\n",
    "\n",
    "# No of labels\n",
    "nb_label = len(labels)\n",
    "\n",
    "# Learning rate for optimizers\n",
    "lr = 0.00003\n",
    "lr_d = 0.00001\n",
    "# Beta1 hyperparam for Adam optimizers\n",
    "beta1 = 0.5\n",
    "# Beta2 hyperparam for Adam optimizers\n",
    "beta2 = 0.999\n",
    "\n",
    "real_label = 1.\n",
    "fake_label = 0.\n",
    "s_criterion = nn.BCELoss().to(device) #For synthesizing\n",
    "c_criterion = nn.CrossEntropyLoss().to(device) #For classification\n",
    "\n",
    "s_label = torch.FloatTensor(batch_size).to(device)\n",
    "c_label = torch.FloatTensor(batch_size, nb_label).to(device)\n",
    "noise = torch.FloatTensor(batch_size, nz).to(device)\n",
    "\n",
    "s_label = Variable(s_label)\n",
    "c_label = Variable(c_label)\n",
    "print(s_label.shape)\n",
    "print(c_label.shape)\n",
    "print(noise.shape)\n",
    "\n",
    "noise = Variable(noise)\n",
    "\n",
    "noise_fixed = torch.FloatTensor(3, nz).to(device)\n",
    "noise_data = np.random.normal(0, 1, (3, nz))\n",
    "noise_fixed.data.copy_(torch.tensor(noise_data))\n",
    "noise_fixed = Variable(noise_fixed)\n",
    "\n",
    "label_fixed = torch.FloatTensor(3, 3).to(device)\n",
    "label_onehot = np.zeros((3, nb_label))\n",
    "label_fixed_data = [0, 1, 2]\n",
    "label_onehot[np.arange(1), label_fixed_data] = 1\n",
    "label_fixed.data.copy_(torch.tensor(label_onehot))\n",
    "label_fixed = Variable(label_fixed)\n",
    "\n",
    "print(noise_fixed.shape)\n",
    "print(label_fixed.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bd17020a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Errno 17] File exists: './GANAug'\n",
      "[Errno 17] File exists: './GANAug/model'\n",
      "[Errno 17] File exists: './GANAug/plots'\n",
      "[Errno 17] File exists: './GANAug/model/ACGAN'\n",
      "[Errno 17] File exists: './GANAug/plots/ACGAN'\n",
      "[Errno 17] File exists: './GANAug/output_images'\n",
      "[Errno 17] File exists: './GANAug/output_images/ACGAN'\n",
      "[Errno 17] File exists: './GANAug/output_images/ACGAN/Covid'\n",
      "[Errno 17] File exists: './GANAug/output_images/ACGAN/Normal'\n",
      "[Errno 17] File exists: './GANAug/output_images/ACGAN/Viral Pneumonia'\n"
     ]
    }
   ],
   "source": [
    "for func in [\n",
    "    lambda: os.mkdir(os.path.join('.', 'GANAug')),\n",
    "    lambda: os.mkdir(os.path.join('.', 'GANAug/model')),\n",
    "    lambda: os.mkdir(os.path.join('.', 'GANAug/plots')),\n",
    "    lambda: os.mkdir(os.path.join('.', 'GANAug/model/ACGAN')),\n",
    "    lambda: os.mkdir(os.path.join('.', 'GANAug/plots/ACGAN')),\n",
    "    lambda: os.mkdir(os.path.join('.', 'GANAug/output_images')),\n",
    "    lambda: os.mkdir(os.path.join('.', 'GANAug/output_images/ACGAN')),\n",
    "    lambda: os.mkdir(os.path.join('.', 'GANAug/output_images/ACGAN/' + label_dict[0])),\n",
    "    lambda: os.mkdir(os.path.join('.', 'GANAug/output_images/ACGAN/' + label_dict[1])),\n",
    "    lambda: os.mkdir(os.path.join('.', 'GANAug/output_images/ACGAN/' + label_dict[2]))]:  # create directories\n",
    "    try:\n",
    "        func()\n",
    "    except Exception as error:\n",
    "        print(error)\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "74dda24a",
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix_fields = [\n",
    "    'G_losses',\n",
    "    'G_class_losses',\n",
    "    'G_syn_losses',\n",
    "    'D_losses',\n",
    "    'D_real_losses',\n",
    "    'D_fake_losses',\n",
    "    'D_class_losses',\n",
    "    'D_syn_losses',\n",
    "    'Accuracy',\n",
    "    'Losses'\n",
    "]\n",
    "metrics = {field: list() for field in matrix_fields}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a26a75d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset(train_dir=train_path, test_dir=test_path):\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((img_size, img_size)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "        ])\n",
    "    train_data = datasets.ImageFolder(train_dir, transform=transform)\n",
    "    train_loader = DataLoader(train_data, batch_size, shuffle=True, num_workers=1)\n",
    "\n",
    "    test_data = datasets.ImageFolder(test_dir ,transform=transform)\n",
    "    test_loader = DataLoader(test_data, batch_size, shuffle=True, num_workers=1)\n",
    "\n",
    "    return train_loader, test_loader, train_data, test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bbb0b552",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader, test_loader, train_data, test_data = load_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0360680c",
   "metadata": {},
   "outputs": [],
   "source": [
    "generator = Generator(input_dim=nz, output_dim=nc, input_size=ngf, class_num=nb_label).to(device)\n",
    "discriminator = Discriminator(input_dim=nc, output_dim=1, input_size=ndf, class_num=nb_label).to(device)\n",
    "\n",
    "# setup optimizer\n",
    "optimizerD = optim.Adam(discriminator.parameters(), lr=lr_d, betas=(beta1, beta2))\n",
    "optimizerG = optim.Adam(generator.parameters(), lr=lr, betas=(beta1, beta2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8d141b1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 3, 512, 512])\n"
     ]
    }
   ],
   "source": [
    "g_output = generator(torch.rand((batch_size, nz)).to(device), torch.ones((batch_size, nb_label)).to(device))\n",
    "print(g_output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8244b33d",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "CUDA out of memory. Tried to allocate 1024.00 MiB (GPU 0; 23.69 GiB total capacity; 20.43 GiB already allocated; 514.56 MiB free; 21.08 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_157879/217816239.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0ms_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mc_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdiscriminator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrand\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mg_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mdel\u001b[0m \u001b[0mg_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mc_output\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_157879/3150458747.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     79\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 81\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     82\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m128\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput_size\u001b[0m \u001b[0;34m//\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput_size\u001b[0m \u001b[0;34m//\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.8/site-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    139\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 141\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    142\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    143\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.8/site-packages/torch/nn/modules/activation.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    736\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    737\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 738\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mleaky_relu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnegative_slope\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minplace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    739\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    740\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.8/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mleaky_relu\u001b[0;34m(input, negative_slope, inplace)\u001b[0m\n\u001b[1;32m   1473\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mleaky_relu_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnegative_slope\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1474\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1475\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mleaky_relu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnegative_slope\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1476\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1477\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 1024.00 MiB (GPU 0; 23.69 GiB total capacity; 20.43 GiB already allocated; 514.56 MiB free; 21.08 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "s_output, c_output = discriminator(torch.rand(g_output.shape).to(device))\n",
    "print(s_output.shape)\n",
    "print(c_output.shape)\n",
    "del g_output, s_output, c_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "065eed00",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install torchviz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81981a02",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print(generator)\n",
    "print(discriminator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e2592f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "SHOW = True\n",
    "if SHOW:\n",
    "    summary(generator, [[nz], [nc]], batch_size=16, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57c2031b",
   "metadata": {},
   "outputs": [],
   "source": [
    "if SHOW:\n",
    "    summary(discriminator, (nc, img_size, img_size), batch_size=16, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0580f119",
   "metadata": {},
   "outputs": [],
   "source": [
    "SHOW_IMG = False\n",
    "if SHOW_IMG:\n",
    "    def modeltorchviz(model, input_1, input_2):\n",
    "        if input_2 != None:\n",
    "            y = model(input_1.to(device), input_2.to(device))\n",
    "        else:\n",
    "            y = model(input_1to(device))\n",
    "        if input_2 != None:\n",
    "            MyConvNetVis = make_dot(y, params=dict(list(model.named_parameters()) + [('x', input_1)] + [('x', input_2)]))\n",
    "        else:\n",
    "            MyConvNetVis = make_dot(y, params=dict(list(model.named_parameters()) + [('x', input_1)]))\n",
    "        MyConvNetVis.format = \"png\"\n",
    "        MyConvNetVis.directory = \"images\"\n",
    "        MyConvNetVis.view() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0cd9edc",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "if SHOW_IMG:\n",
    "    modeltorchviz(generator, torch.randn(16, nz).requires_grad_(True), torch.randn(16, nb_label).requires_grad_(True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d02b9f92",
   "metadata": {},
   "outputs": [],
   "source": [
    "if SHOW_IMG:\n",
    "    modeltorchviz(discriminator, torch.randn(1, nc, ndf, ndf).requires_grad_(True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d1b6459",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(predict, labels):\n",
    "    correct = 0\n",
    "    predict = torch.argmax(predict, dim=1)\n",
    "    labels = torch.argmax(labels, dim=1)\n",
    "    correct = predict.eq(labels.data).cpu().sum()\n",
    "    return correct, len(labels.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0714f81e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for epoch in range(num_epochs):\n",
    "    for i, data in enumerate(tqdm(train_loader, 0)):\n",
    "        ###########################\n",
    "        # (1) Update D network\n",
    "        ###########################\n",
    "        # train with real\n",
    "        discriminator.zero_grad()\n",
    "        img, label = data\n",
    "        batch_size = img.size(0)\n",
    "        with torch.no_grad():\n",
    "            img, label = img.to(device), label.to(device)\n",
    "            label = one_hot(label, num_classes=nb_label)\n",
    "            s_label.resize_(batch_size).fill_(real_label)\n",
    "            c_label.resize_(batch_size, 3).copy_(label)\n",
    "        s_output, c_output = discriminator(img)\n",
    "        s_errD_real = s_criterion(s_output[:,0], s_label.resize_(batch_size))\n",
    "        c_errD_real = c_criterion(c_output, c_label)\n",
    "        errD_real = s_errD_real + c_errD_real\n",
    "        \n",
    "        correct, length = test(c_output, c_label)\n",
    "\n",
    "        # train with fake\n",
    "        noise_data = np.random.normal(0, 1, (batch_size, nz))\n",
    "        noise.resize_(batch_size, nz).data.copy_(torch.tensor(noise_data))\n",
    "        label = np.random.randint(0, nb_label, batch_size)\n",
    "        noise_ = np.random.normal(0, 1, (batch_size, nz))\n",
    "        label_onehot = np.zeros((batch_size, nb_label))\n",
    "        label_onehot[np.arange(batch_size), label] = 1\n",
    "        c_label.resize_(batch_size, nb_label).data.copy_(torch.tensor(label_onehot))\n",
    "        fake = generator(noise, c_label)\n",
    "        s_label.data.fill_(fake_label)\n",
    "        s_output,c_output = discriminator(fake)\n",
    "        s_errD_fake = s_criterion(s_output[:,0], s_label.resize_(batch_size))\n",
    "        c_errD_fake = c_criterion(c_output, c_label)\n",
    "        errD_fake = s_errD_fake + c_errD_fake\n",
    "\n",
    "        errD = s_errD_real + s_errD_fake\n",
    "        errD.backward()\n",
    "        optimizerD.step()\n",
    "\n",
    "        ###########################\n",
    "        # (2) Update G network\n",
    "        ###########################\n",
    "        generator.zero_grad()\n",
    "        s_label.resize_(batch_size, 1).data.fill_(real_label)\n",
    "        fake = generator(noise, c_label)\n",
    "        s_output, c_output = discriminator(fake)\n",
    "        s_errG = s_criterion(s_output[:,0], s_label.resize_(batch_size))\n",
    "        c_errG = c_criterion(c_output, c_label)\n",
    "\n",
    "        errG = s_errG + c_errG\n",
    "        errG.backward()\n",
    "        optimizerG.step()\n",
    "        metrics['G_losses'].append(errG.item())\n",
    "        metrics['G_class_losses'].append(c_errG.item())\n",
    "        metrics['G_syn_losses'].append(s_errG.item())\n",
    "        metrics['D_losses'].append(errD.item())\n",
    "        metrics['D_real_losses'].append(errD_real.item())\n",
    "        metrics['D_fake_losses'].append(errD_fake.item())\n",
    "        metrics['D_class_losses'].append((c_errD_real + c_errD_fake).item())\n",
    "        metrics['D_syn_losses'].append((s_errD_real + s_errD_fake).item())\n",
    "        metrics['Losses'].append((errG + errD).item())\n",
    "        metrics['Accuracy'].append(correct/length)\n",
    "\n",
    "        print('[%d/%d][%d/%d] ========== Loss_D: %.4f, Loss_G: %.4f, Acc_G: %d/%d = %.4f'\n",
    "              % (epoch + 1, num_epochs, i + 1, len(train_loader),\n",
    "                 errD.data, errG.data, correct, length, 100.0*correct/length))\n",
    "        if i % 100 == 0:\n",
    "            vutils.save_image(img, './GANAug/output_images/ACGAN/real_samples_e' + str(epoch) + '_d' + str(i) + '.jpg', normalize=True)\n",
    "            fake = generator(noise_fixed, label_fixed)\n",
    "            for j in range(len(fake)):\n",
    "                vutils.save_image(fake[j].data,\n",
    "                        '%s/fake_samples_epoch_%03d.jpg' % ('./GANAug/output_images/ACGAN/' + label_dict[label_fixed_data[j]], epoch), \n",
    "                                  normalize=True)\n",
    "\n",
    "    # do checkpointing\n",
    "    torch.save(generator.state_dict(), '%s/G_epoch_%d.pth' % (os.path.join('.', 'GANAug/model/ACGAN'), epoch))\n",
    "    torch.save(discriminator.state_dict(), '%s/D_epoch_%d.pth' % (os.path.join('.', 'GANAug/model/ACGAN'), epoch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b6d9c4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tr_plot(tr_data, start_epoch):\n",
    "    Epoch_count=len(metrics['G_losses']) + start_epoch\n",
    "    Epochs=[i + 1 for i in range (start_epoch ,Epoch_count)]  \n",
    "    plt.figure(figsize=(20, 20))\n",
    "    plt.style.use('fivethirtyeight')\n",
    "    \n",
    "    plt.subplot(2, 2, 1)\n",
    "    plt.plot(Epochs, metrics['G_losses'], 'r', label='G_losses')\n",
    "    plt.plot(Epochs, metrics['D_losses'], 'g',label='D_losses' )\n",
    "    plt.title('G&D_losses')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.subplot(2, 2, 2)\n",
    "    plt.plot(Epochs, metrics['G_class_losses'], 'r', label='G_class_losses')\n",
    "    plt.plot(Epochs, metrics['G_syn_losses'], 'g',label='G_syn_losses' )\n",
    "    plt.title('G_class&syn_losses')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "\n",
    "    plt.subplot(2, 2, 3)\n",
    "    plt.plot(Epochs, metrics['D_class_losses'], 'r', label='D_class_losses')\n",
    "    plt.plot(Epochs, metrics['D_syn_losses'], 'g',label='D_syn_losses' )\n",
    "    plt.title('D_class&syn_losses')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "\n",
    "    plt.subplot(2, 2, 4)\n",
    "    plt.plot(Epochs, metrics['D_real_losses'], 'r', label='D_real_losses')\n",
    "    plt.plot(Epochs, metrics['D_fake_losses'], 'g',label='D_fake_losses' )\n",
    "    plt.title('D_real&fake_losses')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.tight_layout\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "235fede4",
   "metadata": {},
   "outputs": [],
   "source": [
    "tr_plot(metrics, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ec852c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tr_plot(tr_data, start_epoch):\n",
    "    Epoch_count=len(metrics['Losses']) + start_epoch\n",
    "    Epochs=[i + 1 for i in range (start_epoch ,Epoch_count)]  \n",
    "    plt.figure(figsize=(20, 10))\n",
    "    plt.style.use('fivethirtyeight')\n",
    "    \n",
    "    plt.subplot(2, 2, 1)\n",
    "    plt.plot(Epochs, metrics['Losses'], 'blue', label='Loss')\n",
    "    plt.title('Losses')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.subplot(2, 2, 2)\n",
    "    plt.plot(Epochs, metrics['Accuracy'], 'orange', label='Accuracy')\n",
    "    plt.title('Accuracy')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.tight_layout\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44c508b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "tr_plot(metrics, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09e18531",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_gan(generator, discriminator, num_epochs, metrics, loader):\n",
    "    now = datetime.datetime.now()\n",
    "    g_losses = metrics['G_losses'][-1]\n",
    "    d_losses = metrics['D_losses'][-1]\n",
    "    path='GANAug/output_images/ACGAN'\n",
    "    try:\n",
    "        os.mkdir(os.path.join('.', path))\n",
    "    except Exception as error:\n",
    "        print(error)\n",
    "\n",
    "    real_batch = next(iter(loader))\n",
    "    \n",
    "    test_img_list = []\n",
    "    test_noise = torch.randn(batch_size, nz, device=device)\n",
    "    test_label = torch.randn(batch_size, nb_label, device=device)\n",
    "    test_fake = generator(test_noise, test_label).detach().cpu()\n",
    "    test_img_list.append(vutils.make_grid(test_fake, padding=2, normalize=True))\n",
    "\n",
    "    fig = plt.figure(figsize=(20, 20))\n",
    "    ax1 = plt.subplot(1,2,1)\n",
    "    ax1 = plt.axis(\"off\")\n",
    "    ax1 = plt.title(\"Real Images\")\n",
    "    ax1 = plt.imshow(np.transpose(vutils.make_grid(real_batch[0].to(device)[:64], padding=5, normalize=True).cpu(),(1,2,0)))\n",
    "\n",
    "    ax2 = plt.subplot(1,2,2)\n",
    "    ax2 = plt.axis(\"off\")\n",
    "    ax2 = plt.title(\"Fake Images\")\n",
    "    ax2 = plt.imshow(np.transpose(test_img_list[-1],(1,2,0)))\n",
    "    plt.show()\n",
    "    fig.savefig('%s/image_%.3f_%.3f_%d_%s.png' %\n",
    "                   (path, g_losses, d_losses, num_epochs, now.strftime(\"%Y-%m-%d_%H:%M:%S\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03814c7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_gan(generator, discriminator, num_epochs, metrics, train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9cca5ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_gan(name, train_epoch, values, path, save):\n",
    "    clear_output(wait=True)\n",
    "    plt.close('all')\n",
    "    fig = plt.figure()\n",
    "    fig = plt.ion()\n",
    "    fig = plt.subplot(1, 1, 1)\n",
    "    fig = plt.title('epoch: %s -> %s: %s' % (train_epoch, name, values[-1]))\n",
    "    fig = plt.ylabel(name)\n",
    "    fig = plt.xlabel('train_set')\n",
    "    fig = plt.plot(values)\n",
    "    fig = plt.grid()\n",
    "    get_fig = plt.gcf()\n",
    "    fig = plt.draw()  # draw the plot\n",
    "    fig = plt.pause(1)  # show it for 1 second\n",
    "    plt.show()\n",
    "    if save:\n",
    "        now = datetime.datetime.now()\n",
    "        get_fig.savefig('%s/%s_%.3f_%d_%s.png' %\n",
    "                        (path, name, train_epoch, values[-1], now.strftime(\"%Y-%m-%d_%H:%M:%S\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d5e7892",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model(generator, discriminator, gen_optimizer, dis_optimizer, metrics, num_epochs):\n",
    "    now = datetime.datetime.now()\n",
    "    g_losses = metrics['G_losses'][-1]\n",
    "    d_losses = metrics['D_losses'][-1]\n",
    "    path='GANAug/plots/ACGAN/train_%+.3f_%+.3f_%s'% (g_losses, d_losses, now.strftime(\"%Y-%m-%d_%H-%M-%S\"))\n",
    "    try:\n",
    "        os.mkdir(os.path.join('.', path))\n",
    "    except Exception as error:\n",
    "        print(error)\n",
    "    plot_gan('G_losses', num_epochs, metrics['G_losses'], path, True)\n",
    "    plot_gan('D_losses', num_epochs, metrics['D_losses'], path, True)\n",
    "    plot_gan('G_syn_losses', num_epochs, metrics['G_syn_losses'], path, True)\n",
    "    plot_gan('G_class_losses', num_epochs, metrics['G_class_losses'], path, True)\n",
    "    plot_gan('G_syn_losses', num_epochs, metrics['G_syn_losses'], path, True)\n",
    "    plot_gan('D_class_losses', num_epochs, metrics['D_class_losses'], path, True)\n",
    "    plot_gan('D_syn_losses', num_epochs, metrics['D_syn_losses'], path, True)\n",
    "    plot_gan('Losses', num_epochs, metrics['Losses'], path, True)\n",
    "    plot_gan('Accuracy', num_epochs, metrics['Accuracy'], path, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f66ad873",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_model(generator, discriminator, optimizerG, optimizerD, metrics, num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c56cd9c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_img_list = []\n",
    "test_noise = torch.randn(batch_size, nz, device=device)\n",
    "test_label = torch.randn(batch_size, nb_label, device=device)\n",
    "test_img = generator(test_noise, test_label)\n",
    "\n",
    "s_output, c_label_op = discriminator(test_img.detach().to(device))\n",
    "print('Discriminator s', s_output)\n",
    "print('Discriminator c', c_label_op)\n",
    "\n",
    "test_img = test_img.detach().cpu()\n",
    "test_img_list.append(vutils.make_grid(test_img, padding=2, normalize=True))\n",
    "plt.imshow(np.transpose(test_img_list[-1],(1,2,0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d45fe52a",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = next(iter(test_loader))\n",
    "test_noise, test_class_lable = data\n",
    "test_img = test_noise\n",
    "print('class label for real', test_class_lable)\n",
    "\n",
    "s_output,c_label_op = discriminator(test_img.detach().to(device))\n",
    "print('Discriminator s', s_output)\n",
    "print('Discriminator c', c_label_op)\n",
    "\n",
    "test_img = test_img.detach().cpu()\n",
    "test_img_list.append(vutils.make_grid(test_img, padding=2, normalize=True))\n",
    "plt.imshow(np.transpose(test_img_list[-1],(1,2,0)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
