{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e3b62dae",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting package metadata (current_repodata.json): ...working... failed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "CondaHTTPError: HTTP 000 CONNECTION FAILED for url <https://repo.anaconda.com/pkgs/main/win-64/current_repodata.json>\n",
      "Elapsed: -\n",
      "\n",
      "An HTTP error occurred when trying to retrieve this URL.\n",
      "HTTP errors are often intermittent, and a simple retry will get you on your way.\n",
      "\n",
      "If your current network has https://www.anaconda.com blocked, please file\n",
      "a support request with your network engineering team.\n",
      "\n",
      "'https//repo.anaconda.com/pkgs/main/win-64'\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!conda install pandas seaborn -y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "370513dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: opencv-python in d:\\miniconda3\\envs\\torch_1-11\\lib\\site-packages (4.5.5.64)\n",
      "Requirement already satisfied: torchsummary in d:\\miniconda3\\envs\\torch_1-11\\lib\\site-packages (1.5.1)\n",
      "Requirement already satisfied: scikit-learn in d:\\miniconda3\\envs\\torch_1-11\\lib\\site-packages (1.1.0)\n",
      "Requirement already satisfied: torchviz in d:\\miniconda3\\envs\\torch_1-11\\lib\\site-packages (0.0.2)\n",
      "Requirement already satisfied: utils in d:\\miniconda3\\envs\\torch_1-11\\lib\\site-packages (1.0.1)\n",
      "Requirement already satisfied: imageio in d:\\miniconda3\\envs\\torch_1-11\\lib\\site-packages (2.19.2)\n",
      "Requirement already satisfied: numpy>=1.14.5 in d:\\miniconda3\\envs\\torch_1-11\\lib\\site-packages (from opencv-python) (1.21.5)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in d:\\miniconda3\\envs\\torch_1-11\\lib\\site-packages (from scikit-learn) (3.1.0)\n",
      "Requirement already satisfied: joblib>=1.0.0 in d:\\miniconda3\\envs\\torch_1-11\\lib\\site-packages (from scikit-learn) (1.1.0)\n",
      "Requirement already satisfied: scipy>=1.3.2 in d:\\miniconda3\\envs\\torch_1-11\\lib\\site-packages (from scikit-learn) (1.7.3)\n",
      "Requirement already satisfied: graphviz in d:\\miniconda3\\envs\\torch_1-11\\lib\\site-packages (from torchviz) (0.20)\n",
      "Requirement already satisfied: torch in d:\\miniconda3\\envs\\torch_1-11\\lib\\site-packages (from torchviz) (1.11.0)\n",
      "Requirement already satisfied: pillow>=8.3.2 in d:\\miniconda3\\envs\\torch_1-11\\lib\\site-packages (from imageio) (9.0.1)\n",
      "Requirement already satisfied: typing_extensions in d:\\miniconda3\\envs\\torch_1-11\\lib\\site-packages (from torch->torchviz) (4.1.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install opencv-python torchsummary scikit-learn torchviz utils imageio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6a6a2202",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.parallel\n",
    "import torch.nn.functional as F\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.optim as optim\n",
    "import torch.utils.data\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.utils as vutils\n",
    "import utils\n",
    "import time\n",
    "from torch.nn.functional import one_hot\n",
    "from torchvision.utils import save_image\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import SubsetRandomSampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6cf2ef10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['sm_37', 'sm_50', 'sm_60', 'sm_61', 'sm_70', 'sm_75', 'sm_80', 'sm_86', 'compute_37'] cuda\n"
     ]
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(torch.cuda.get_arch_list(), device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b8079c45",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import argparse\n",
    "import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import HTML\n",
    "from IPython.display import clear_output\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "from tqdm import notebook\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import gc\n",
    "import matplotlib.colors as mat_color\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import random\n",
    "import cv2\n",
    "from torchvision.datasets import ImageNet, ImageFolder\n",
    "from torch.utils.data import DataLoader\n",
    "from torchsummary import summary\n",
    "from torchviz import make_dot\n",
    "import imageio\n",
    "from torchvision import datasets, transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bf703b9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def weights_init(m):\n",
    "    classname = m.__class__.__name__\n",
    "    if classname.find('Conv') != -1:\n",
    "        m.weight.data.normal_(0.0, 0.02)\n",
    "    elif classname.find('BatchNorm') != -1:\n",
    "        m.weight.data.normal_(1.0, 0.02)\n",
    "        m.bias.data.fill_(0)\n",
    "\n",
    "class Discriminator(nn.Module):\n",
    "\n",
    "    def __init__(self, ndf, nc, nb_label):\n",
    "\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.LeakyReLU = nn.LeakyReLU(0.2, inplace=True)\n",
    "        self.DropOut1 = nn.Dropout(p=0.5)\n",
    "        self.DropOut2 = nn.Dropout(p=0.25)\n",
    "        self.conv1 = nn.Conv2d(nc, ndf, 4, 2, 1, bias=False)\n",
    "        self.conv2 = nn.Conv2d(ndf, ndf * 2, 4, 2, 1, bias=False)\n",
    "        self.BatchNorm2 = nn.BatchNorm2d(ndf * 2)\n",
    "        self.conv3 = nn.Conv2d(ndf * 2, ndf * 4, 4, 2, 1, bias=False)\n",
    "        self.BatchNorm3 = nn.BatchNorm2d(ndf * 4)\n",
    "        self.conv4_1 = nn.Conv2d(ndf * 4, ndf * 4, 4, 2, 1, bias=False)\n",
    "        self.BatchNorm4_1 = nn.BatchNorm2d(ndf * 4)\n",
    "        self.conv4_2 = nn.Conv2d(ndf * 4, ndf * 8, 4, 2, 1, bias=False)\n",
    "        self.BatchNorm4_2 = nn.BatchNorm2d(ndf * 8)\n",
    "        self.conv4_3 = nn.Conv2d(ndf * 8, ndf * 8, 4, 2, 1, bias=False)\n",
    "        self.BatchNorm4_3 = nn.BatchNorm2d(ndf * 8)\n",
    "        self.conv5 = nn.Conv2d(ndf * 8, ndf * 1, 4, 1, 0, bias=False)\n",
    "        self.disc_linear = nn.Linear(ndf * 1, 1)\n",
    "        self.aux_linear = nn.Linear(ndf * 1, nb_label)\n",
    "        self.softmax = nn.Softmax()\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        self.ndf = ndf\n",
    "        self.apply(weights_init)\n",
    "\n",
    "    def forward(self, input):\n",
    "\n",
    "        x = self.conv1(input)\n",
    "        x = self.LeakyReLU(x)\n",
    "        x = self.DropOut1(x)\n",
    "\n",
    "        x = self.conv2(x)\n",
    "        x = self.BatchNorm2(x)\n",
    "        x = self.LeakyReLU(x)\n",
    "        #x = self.DropOut(x)\n",
    "\n",
    "        x = self.conv3(x)\n",
    "        x = self.BatchNorm3(x)\n",
    "        x = self.LeakyReLU(x)\n",
    "        x = self.DropOut1(x)\n",
    "\n",
    "        x = self.conv4_1(x)\n",
    "        x = self.BatchNorm4_1(x)\n",
    "        x = self.LeakyReLU(x)\n",
    "        x = self.DropOut2(x)\n",
    "        x = self.conv4_2(x)\n",
    "        x = self.BatchNorm4_2(x)\n",
    "        x = self.LeakyReLU(x)\n",
    "        x = self.DropOut2(x)\n",
    "        x = self.conv4_3(x)\n",
    "        x = self.BatchNorm4_3(x)\n",
    "        x = self.LeakyReLU(x)\n",
    "        x = self.DropOut2(x)\n",
    "\n",
    "        x = self.conv5(x)\n",
    "        x = x.view(-1, self.ndf * 1)\n",
    "        c = self.aux_linear(x)\n",
    "        c = self.softmax(c)\n",
    "        s = self.disc_linear(x)\n",
    "        s = self.sigmoid(s)\n",
    "        return s, c\n",
    "\n",
    "class Generator(nn.Module):\n",
    "\n",
    "    def __init__(self, nz, ngf, nc):\n",
    "\n",
    "        super(Generator, self).__init__()\n",
    "        self.ReLU = nn.ReLU(True)\n",
    "        self.Tanh = nn.Tanh()\n",
    "        #self.DropOut = nn.Dropout(p=0.75)\n",
    "        #self.conv0 = nn.ConvTranspose2d(nz, ngf * 16, 4, 1, 1, bias=False)\n",
    "        #self.BatchNorm0 = nn.BatchNorm2d(ngf * 16)\n",
    "        self.conv1 = nn.ConvTranspose2d(nz, ngf * 8, 4, 1, 0, bias=False)\n",
    "        self.BatchNorm1 = nn.BatchNorm2d(ngf * 8)\n",
    "\n",
    "        self.conv2 = nn.ConvTranspose2d(ngf * 8, ngf * 8, 4, 2, 1, bias=False)\n",
    "        self.BatchNorm2 = nn.BatchNorm2d(ngf * 8)\n",
    "\n",
    "        self.conv3 = nn.ConvTranspose2d(ngf * 8, ngf * 4, 4, 2, 1, bias=False)\n",
    "        self.BatchNorm3 = nn.BatchNorm2d(ngf * 4)\n",
    "\n",
    "        self.conv4 = nn.ConvTranspose2d(ngf * 4, ngf * 2, 4, 2, 1, bias=False)\n",
    "        self.BatchNorm4 = nn.BatchNorm2d(ngf * 2)\n",
    "        \n",
    "        self.conv5 = nn.ConvTranspose2d(ngf * 2, ngf * 1, 4, 2, 1, bias=False)\n",
    "        self.BatchNorm5 = nn.BatchNorm2d(ngf * 1)\n",
    "\n",
    "        self.conv6 = nn.ConvTranspose2d(ngf * 1, nc, 4, 2, 1, bias=False)\n",
    "\n",
    "        self.apply(weights_init)\n",
    "\n",
    "\n",
    "    def forward(self, input):\n",
    "        #x = self.conv0(input)\n",
    "        #x = self.BatchNorm0(x)\n",
    "        #x = self.ReLU(x)\n",
    "        x = self.conv1(input)\n",
    "        x = self.BatchNorm1(x)\n",
    "        x = self.ReLU(x)\n",
    "        #x = self.DropOut(x)\n",
    "\n",
    "        x = self.conv2(x)\n",
    "        x = self.BatchNorm2(x)\n",
    "        x = self.ReLU(x)\n",
    "        #x = self.DropOut(x)\n",
    "        \n",
    "        x = self.conv2(x)\n",
    "        x = self.BatchNorm2(x)\n",
    "        x = self.ReLU(x)\n",
    "        #x = self.DropOut(x)\n",
    "\n",
    "        x = self.conv3(x)\n",
    "        x = self.BatchNorm3(x)\n",
    "        x = self.ReLU(x)\n",
    "        #x = self.DropOut(x)\n",
    "\n",
    "        x = self.conv4(x)\n",
    "        x = self.BatchNorm4(x)\n",
    "        x = self.ReLU(x)\n",
    "        #x = self.DropOut(x)\n",
    "\n",
    "        x = self.conv5(x)\n",
    "        x = self.BatchNorm5(x)\n",
    "        x = self.ReLU(x)\n",
    "        #x = self.DropOut(x)\n",
    "\n",
    "        x = self.conv6(x)\n",
    "        output = self.Tanh(x)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8b174419",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_animation(path, num):\n",
    "    images = []\n",
    "    for e in range(num):\n",
    "        img_name = path + '_generate_animation_epoch%03d' % (e+1) + '.png'\n",
    "        images.append(imageio.imread(img_name))\n",
    "    imageio.mimsave(path + '_generate_animation.gif', images, fps=5)\n",
    "\n",
    "def loss_plot_unit(hist, path, model_name, index):\n",
    "    x = range(len(hist[index[0]]))\n",
    "\n",
    "    plt.plot(x, hist[index[0]], label=index[0])\n",
    "    if len(index) >= 2:\n",
    "        plt.plot(x, hist[index[1]], label=index[1])\n",
    "\n",
    "    plt.xlabel('Iter')\n",
    "    plt.ylabel(model_name)\n",
    "\n",
    "    plt.legend(loc=4)\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "\n",
    "    now = datetime.datetime.now()\n",
    "    path = os.path.join(path, now.strftime(\"%Y-%m-%d_%H-%M-%S_\") + model_name + '.png')\n",
    "    plt.savefig(path)\n",
    "    plt.close()\n",
    "\n",
    "def loss_plot(hist, path='Train_hist', model_name=''):\n",
    "    loss_plot_unit(hist=hist, path=path, model_name='Total_Loss', index=['G_losses', 'D_losses'])\n",
    "    loss_plot_unit(hist=hist, path=path, model_name='G_C_S_Loss', index=['G_class_losses', 'G_syn_losses'])\n",
    "    loss_plot_unit(hist=hist, path=path, model_name='D_R_F_Loss', index=['D_real_losses', 'D_fake_losses'])\n",
    "    loss_plot_unit(hist=hist, path=path, model_name='D_C_S_Loss', index=['D_class_losses', 'D_syn_losses'])\n",
    "    loss_plot_unit(hist=hist, path=path, model_name='Overall_Loss', index=['Losses'])\n",
    "    loss_plot_unit(hist=hist, path=path, model_name='Time', index=['Time_per_epoch'])\n",
    "    loss_plot_unit(hist=hist, path=path, model_name='G_class_losses', index=['G_class_losses'])\n",
    "    loss_plot_unit(hist=hist, path=path, model_name='G_syn_losses', index=['G_syn_losses'])\n",
    "    loss_plot_unit(hist=hist, path=path, model_name='D_real_losses', index=['D_real_losses'])\n",
    "    loss_plot_unit(hist=hist, path=path, model_name='D_fake_losses', index=['D_fake_losses'])\n",
    "    loss_plot_unit(hist=hist, path=path, model_name='D_class_losses', index=['D_class_losses'])\n",
    "    loss_plot_unit(hist=hist, path=path, model_name='D_syn_losses', index=['D_syn_losses'])\n",
    "    loss_plot_unit(hist=hist, path=path, model_name='G_losses', index=['G_losses'])\n",
    "    loss_plot_unit(hist=hist, path=path, model_name='D_losses', index=['D_losses'])\n",
    "    \n",
    "\n",
    "def save_images(images, size, image_path):\n",
    "    return imsave(images, size, image_path)\n",
    "\n",
    "def imsave(images, size, path):\n",
    "    image = np.squeeze(merge(images, size))\n",
    "    print(\"image saved at ->\", path)\n",
    "    return imageio.imwrite(path, image)\n",
    "\n",
    "def merge(images, size):\n",
    "    h, w = images.shape[1], images.shape[2]\n",
    "    if (images.shape[3] in (3,4)):\n",
    "        c = images.shape[3]\n",
    "        img = np.zeros((h * size[0], w * size[1], c))\n",
    "        for idx, image in enumerate(images):\n",
    "            i = idx % size[1]\n",
    "            j = idx // size[1]\n",
    "            img[j * h:j * h + h, i * w:i * w + w, :] = image\n",
    "        return img\n",
    "    elif images.shape[3]==1:\n",
    "        img = np.zeros((h * size[0], w * size[1]))\n",
    "        for idx, image in enumerate(images):\n",
    "            i = idx % size[1]\n",
    "            j = idx // size[1]\n",
    "            img[j * h:j * h + h, i * w:i * w + w] = image[:,:,0]\n",
    "        return img\n",
    "    else:\n",
    "        raise ValueError('in merge(images,size) images parameter ''must have dimensions: HxW or HxWx3 or HxWx4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "73154b3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CACGAN(object):\n",
    "    def __init__(self):\n",
    "        self.model_name = 'ACGAN'\n",
    "#         self.I_want_to_train_faster = 100\n",
    "        self.num_epoch = 500\n",
    "        self.base_path = './data'\n",
    "        self.base_folder = \"Covid-19 Image Dataset\"\n",
    "        self.classic_folder = 'Coivd-19_Classic'\n",
    "        self.synthetic_folder = 'Coivd-19_Synthetic'\n",
    "        self.data_dir = os.path.join(self.base_path, self.classic_folder)\n",
    "        self.batch_size = 64\n",
    "        self.train_path = os.path.join(self.data_dir, \"train\")\n",
    "        self.test_path = os.path.join(self.data_dir, \"test\")\n",
    "        self.labels = os.listdir(self.data_dir)\n",
    "        self.no_norm = mat_color.Normalize(vmin=0, vmax=255, clip=False)\n",
    "        self.label_dict = {\n",
    "            i : self.labels[i]\n",
    "            for i in range(len(self.labels))\n",
    "        }\n",
    "        self.img_size = 256\n",
    "        self.num_of_ch = 3\n",
    "        # size of z latent vector (i.e. size of generator input)\n",
    "        self.size_of_z = self.img_size\n",
    "        self.num_of_label = len(self.labels)\n",
    "        self.sample_num = self.num_of_label ** 2\n",
    "#         self.learning_rate_g = 0.00000002\n",
    "#         self.learning_rate_d = 0.00000002\n",
    "#         self.learning_rate_g = 0.0000001\n",
    "#         self.learning_rate_d = 0.0000001\n",
    "        self.learning_rate_g = 0.0001\n",
    "        self.learning_rate_d = 0.0001\n",
    "        # beta1 hyperparam for adam\n",
    "        self.adam_beta_1 = 0.5\n",
    "        # beta2 hyperparam for adam\n",
    "        self.adam_beta_2 = 0.999\n",
    "        self.real_label = 1.0\n",
    "        self.fake_label = 0.0\n",
    "        self.syn_criterion = nn.BCELoss().to(device) # synthesizing\n",
    "        self.class_criterion = nn.CrossEntropyLoss().to(device) # classification\n",
    "        \n",
    "        self.train_loader, self.test_loader, self.train_data, self.test_data = self.load_dataset()\n",
    "        self.generator = Generator(self.size_of_z, self.img_size, self.num_of_ch).to(device)\n",
    "        self.discriminator = Discriminator(self.img_size, self.num_of_ch, self.num_of_label).to(device)\n",
    "\n",
    "        self.optimizer_d = optim.Adam(self.discriminator.parameters(), \n",
    "                                      lr=self.learning_rate_d, \n",
    "                                      betas=(self.adam_beta_1, self.adam_beta_2))\n",
    "        self.optimizer_g = optim.Adam(self.generator.parameters(), \n",
    "                                      lr=self.learning_rate_g, \n",
    "                                      betas=(self.adam_beta_1, self.adam_beta_2))\n",
    "\n",
    "        # fixed noise & label\n",
    "        self.sample_z_fixed = torch.zeros((self.sample_num, self.size_of_z))\n",
    "        for i in range(self.num_of_label):\n",
    "            self.sample_z_fixed[i*self.num_of_label] = torch.rand(1, self.size_of_z)\n",
    "            for j in range(1, self.num_of_label):\n",
    "                self.sample_z_fixed[i*self.num_of_label + j] = self.sample_z_fixed[i*self.num_of_label]\n",
    "\n",
    "        temp = torch.zeros((self.num_of_label, 1))\n",
    "        for i in range(self.num_of_label):\n",
    "            temp[i, 0] = i\n",
    "\n",
    "        temp_y = torch.zeros((self.sample_num, 1))\n",
    "        for i in range(self.num_of_label):\n",
    "            temp_y[i*self.num_of_label: (i+1)*self.num_of_label] = temp\n",
    "\n",
    "        self.sample_y_fixed = torch.zeros((self.sample_num, self.num_of_label)).scatter_(1, temp_y.type(torch.LongTensor), 1)\n",
    "        self.sample_z_fixed, self.sample_y_fixed = self.sample_z_fixed.to(device), self.sample_y_fixed.to(device)\n",
    "        \n",
    "    def load_dataset(self):\n",
    "        train_dir = self.train_path\n",
    "        test_dir = self.test_path\n",
    "        transform = transforms.Compose([\n",
    "            transforms.Resize((self.img_size, self.img_size)),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "            ])\n",
    "        train_data = datasets.ImageFolder(train_dir, transform=transform)\n",
    "        train_loader = DataLoader(train_data, self.batch_size, shuffle=True, num_workers=1)\n",
    "\n",
    "        test_data = datasets.ImageFolder(test_dir ,transform=transform)\n",
    "        test_loader = DataLoader(test_data, self.batch_size, shuffle=True, num_workers=1)\n",
    "\n",
    "        return train_loader, test_loader, train_data, test_data\n",
    "\n",
    "        return train_loader, test_loader, train_data, test_data\n",
    "\n",
    "    def modeltorchviz(model, input_1, input_2):\n",
    "        if input_2 != None:\n",
    "            y = model(input_1.to(device), input_2.to(device))\n",
    "        else:\n",
    "            y = model(input_1to(device))\n",
    "        if input_2 != None:\n",
    "            MyConvNetVis = make_dot(y, params=dict(list(model.named_parameters()) + [('x', input_1)] + [('x', input_2)]))\n",
    "        else:\n",
    "            MyConvNetVis = make_dot(y, params=dict(list(model.named_parameters()) + [('x', input_1)]))\n",
    "        MyConvNetVis.format = \"png\"\n",
    "        MyConvNetVis.directory = \"images\"\n",
    "        MyConvNetVis.view() \n",
    "    \n",
    "    def visualize_results(self, epoch, fix=True):\n",
    "        self.generator.eval()\n",
    "\n",
    "        image_frame_dim = round(np.sqrt(self.sample_num))\n",
    "        if fix:\n",
    "            \"\"\" fixed noise \"\"\"\n",
    "            samples = self.generator(self.sample_z_fixed.resize_(self.sample_z_fixed.shape[0], self.sample_z_fixed.shape[1], 1, 1))\n",
    "        else:\n",
    "            \"\"\" random noise \"\"\"\n",
    "            sample_y_ = torch.zeros(self.batch_size, self.class_num).scatter_(1, torch.randint(0, self.class_num - 1, (self.batch_size, 1)).type(torch.LongTensor), 1)\n",
    "            sample_z_ = torch.rand((self.batch_size, self.z_dim))\n",
    "            sample_z_, sample_y_ = sample_z_.cuda(), sample_y_.cuda()\n",
    "            samples = self.generator(sample_z_.resize_(self.sample_z_.shape[0], self.sample_z_.shape[1], 1, 1))\n",
    "\n",
    "        samples = samples.cpu().data.numpy().transpose(0, 2, 3, 1)\n",
    "        samples = (samples + 1) / 2\n",
    "        save_images(samples[:image_frame_dim * image_frame_dim, :, :, :], [image_frame_dim, image_frame_dim],\n",
    "                    os.path.join('.', 'GANAug/plots/Lung') + \"/\" + self.model_name + '_generate_animation_epoch%03d' % epoch + '.png')\n",
    "    \n",
    "    def train(self):\n",
    "        for func in [\n",
    "            lambda: os.mkdir(os.path.join('.', 'GANAug')),\n",
    "            lambda: os.mkdir(os.path.join('.', 'GANAug/model')),\n",
    "            lambda: os.mkdir(os.path.join('.', 'GANAug/plots')),\n",
    "            lambda: os.mkdir(os.path.join('.', 'GANAug/model/Lung')),\n",
    "            lambda: os.mkdir(os.path.join('.', 'GANAug/plots/Lung')),\n",
    "            lambda: os.mkdir(os.path.join('.', 'GANAug/output_images')),\n",
    "            lambda: os.mkdir(os.path.join('.', 'GANAug/output_images/Lung')),\n",
    "            lambda: os.mkdir(os.path.join('.', 'GANAug/output_images/Lung/' + self.label_dict[0])),\n",
    "            lambda: os.mkdir(os.path.join('.', 'GANAug/output_images/Lung/' + self.label_dict[1])),\n",
    "            lambda: os.mkdir(os.path.join('.', 'GANAug/output_images/Lung/' + self.label_dict[2])),\n",
    "            lambda: os.mkdir(os.path.join('.', 'GANAug/output_images/Lung/' + self.label_dict[3])),\n",
    "            lambda: os.mkdir(os.path.join('.', 'GANAug/output_images/Lung/' + self.label_dict[4]))]:\n",
    "            try:\n",
    "                func()\n",
    "            except Exception as error:\n",
    "                print(error)\n",
    "                continue\n",
    "        matrix_fields = [\n",
    "            'G_losses',\n",
    "            'G_class_losses',\n",
    "            'G_syn_losses',\n",
    "            'D_losses',\n",
    "            'D_real_losses',\n",
    "            'D_fake_losses',\n",
    "            'D_class_losses',\n",
    "            'D_syn_losses',\n",
    "            'Losses',\n",
    "            'Time_per_epoch',\n",
    "            'Total_time'\n",
    "        ]\n",
    "        self.metrics = {field: list() for field in matrix_fields}\n",
    "        early_stop_count = 0\n",
    "        early_stop_patient = len(self.train_loader) * 4422 # A ha ha ha ha\n",
    "        early_stop = False\n",
    "        best_batch_loss = -1\n",
    "        save_model = False\n",
    "        number_of_model_saved = 10\n",
    "        label_fixed_data = [i%self.num_of_label for i in range(self.sample_num)]\n",
    "        \n",
    "        def get_file_list(file_path, length):\n",
    "            dir_list = os.listdir(file_path)\n",
    "            dir_list = only_pth_file(dir_list)\n",
    "            if not dir_list:\n",
    "                return\n",
    "            else:\n",
    "                dir_list = sorted(dir_list,  key=lambda x: os.path.getmtime(os.path.join(file_path, x)))\n",
    "                print(\"files to be deleted < length =\", length ,\"> ->\", dir_list[0:length])\n",
    "                return dir_list[0:length]\n",
    "\n",
    "        def only_pth_file(file_list):\n",
    "            new_list = []\n",
    "            for file in file_list:\n",
    "                if file[-4:] == \".pth\":\n",
    "                    new_list.append(file)\n",
    "            return new_list\n",
    "            \n",
    "        start_time = time.time()\n",
    "        print(\"Start Testing ...\")\n",
    "        print(self.generator)\n",
    "        print(self.discriminator)\n",
    "        summary(self.generator, (self.size_of_z, 1, 1), batch_size=self.batch_size, device=device)\n",
    "        summary(self.discriminator, (self.num_of_ch, self.img_size, self.img_size), batch_size=self.batch_size, device=device)\n",
    "        g_output = self.generator(torch.rand((self.batch_size, self.size_of_z, 1, 1)).to(device))\n",
    "        print(g_output.shape)\n",
    "        s_output, c_output = self.discriminator(torch.rand(g_output.shape).to(device))\n",
    "        print(s_output.shape)\n",
    "        print(c_output.shape)\n",
    "        del g_output, s_output, c_output\n",
    "        print(\"Testing Done in ->\", time.time() - start_time)\n",
    "        start_time = time.time()\n",
    "        print(\"Start Training ...\")\n",
    "\n",
    "        self.discriminator.train()\n",
    "        for epoch in range(self.num_epoch):\n",
    "            epoch += 1\n",
    "            log_in_data_loader = []\n",
    "            self.generator.train()\n",
    "            epoch_start_time = time.time()\n",
    "            for i, data in enumerate(tqdm(self.train_loader, 0)):\n",
    "                x_, y_ = data\n",
    "                batch_size_x_ = x_.size(0)\n",
    "                z_ = torch.rand((batch_size_x_, self.size_of_z))\n",
    "                y_vec_ = torch.zeros((batch_size_x_, self.num_of_label)).scatter_(1, y_.type(torch.LongTensor).unsqueeze(1), 1)\n",
    "                x_, z_, y_vec_ = x_.cuda(), z_.cuda(), y_vec_.cuda()\n",
    "                self.y_real_, self.y_fake_ = torch.ones(batch_size_x_, 1), torch.zeros(batch_size_x_, 1)\n",
    "                self.y_real_, self.y_fake_ = self.y_real_.to(device), self.y_fake_.to(device)\n",
    "        \n",
    "                # update D network\n",
    "                self.optimizer_d.zero_grad()\n",
    "\n",
    "                D_real, C_real = self.discriminator(x_)\n",
    "                D_real_loss = self.syn_criterion(D_real, self.y_real_)\n",
    "                C_real_loss = self.class_criterion(C_real, torch.max(y_vec_, 1)[1])\n",
    "\n",
    "                G_ = self.generator(z_.resize_(batch_size_x_, self.size_of_z, 1, 1))\n",
    "                D_fake, C_fake = self.discriminator(G_)\n",
    "                \n",
    "                D_fake_loss = self.syn_criterion(D_fake, self.y_fake_)\n",
    "                C_fake_loss = self.class_criterion(C_fake, torch.max(y_vec_, 1)[1])\n",
    "\n",
    "                D_loss = D_real_loss + C_real_loss + D_fake_loss + C_fake_loss\n",
    "                self.metrics['D_losses'].append(D_loss.item())\n",
    "                self.metrics['D_real_losses'].append((D_real_loss + C_real_loss).item())\n",
    "                self.metrics['D_fake_losses'].append((D_fake_loss + C_fake_loss).item())\n",
    "                self.metrics['D_class_losses'].append((C_real_loss + C_fake_loss).item())\n",
    "                self.metrics['D_syn_losses'].append((D_real_loss + D_fake_loss).item())\n",
    "\n",
    "                D_loss.backward()\n",
    "                self.optimizer_d.step()\n",
    "\n",
    "                # update G network\n",
    "                self.optimizer_g.zero_grad()\n",
    "\n",
    "                G_ = self.generator(z_.resize_(batch_size_x_, self.size_of_z, 1, 1))\n",
    "                D_fake, C_fake = self.discriminator(G_)\n",
    "\n",
    "                D_fake_loss = self.syn_criterion(D_fake, self.y_real_)\n",
    "                C_fake_loss = self.class_criterion(C_fake, torch.max(y_vec_, 1)[1])\n",
    "\n",
    "                G_loss = D_fake_loss + C_fake_loss\n",
    "                self.metrics['G_losses'].append(G_loss.item())\n",
    "                self.metrics['G_syn_losses'].append(D_fake_loss.item())\n",
    "                self.metrics['G_class_losses'].append(C_fake_loss.item())\n",
    "                self.metrics['Losses'].append(self.metrics['G_losses'][-1] + self.metrics['D_losses'][-1])\n",
    "\n",
    "                G_loss.backward()\n",
    "                self.optimizer_g.step()\n",
    "\n",
    "                if best_batch_loss < 0:\n",
    "                    best_batch_loss = self.metrics['Losses'][-1]\n",
    "                    early_stop_count = 0\n",
    "                else:\n",
    "                    if best_batch_loss >= self.metrics['Losses'][-1]:\n",
    "                        best_batch_loss = self.metrics['Losses'][-1]\n",
    "                        early_stop_count = 0\n",
    "                        log_in_data_loader.append(\"---------------------<lowest loss update -> \" + str(best_batch_loss) + \" at -> \" + str(i + 1) + \">---------------------\")\n",
    "                        save_model = True\n",
    "                    else:\n",
    "                        early_stop_count += 1\n",
    "                        if early_stop_count >= early_stop_patient and early_stop == False:\n",
    "                            log_in_data_loader.append(\"-----------------------------------< early stopping ... >-----------------------------------\")\n",
    "                            early_stop = True\n",
    "\n",
    "                if i % round(len(self.train_loader)/5) == 0:\n",
    "                    vutils.save_image(x_, './GANAug/output_images/Lung/real_samples_e' + str(epoch) + '_d' + str(i) + '.jpg', normalize=True)\n",
    "                    fake = self.generator(self.sample_z_fixed.resize_(self.sample_z_fixed.shape[0], self.sample_z_fixed.shape[1], 1, 1))\n",
    "                    for j in range(len(fake)):\n",
    "                        vutils.save_image(fake[j].data,\n",
    "                                '%s/fake_samples_epoch_%03d.jpg' % ('./GANAug/output_images/Lung/' + self.label_dict[label_fixed_data[j]], epoch), \n",
    "                                          normalize=True)\n",
    "\n",
    "            self.metrics['Time_per_epoch'].append(time.time() - epoch_start_time)\n",
    "            with torch.no_grad():\n",
    "                self.visualize_results(epoch)\n",
    "    \n",
    "            for message in log_in_data_loader:\n",
    "                print(message)\n",
    "            print('[%d/%d] ======================================================================== \\nLoss_D: %.8f, Loss_G: %.8f\\nLoss_S_D: %.8f, Loss_C_D: %.8f, Loss_R_D: %.8f, Loss_F_D: %.8f\\nLoss_S_G: %.8f, Loss_C_G: %.8f'\n",
    "                  % (epoch, self.num_epoch, \n",
    "                     np.mean(self.metrics['D_losses'][-len(self.train_loader):]), \n",
    "                     np.mean(self.metrics['G_losses'][-len(self.train_loader):]),\n",
    "                     np.mean(self.metrics['D_syn_losses'][-len(self.train_loader):]), \n",
    "                     np.mean(self.metrics['D_class_losses'][-len(self.train_loader):]), \n",
    "                     np.mean(self.metrics['D_real_losses'][-len(self.train_loader):]), \n",
    "                     np.mean(self.metrics['D_fake_losses'][-len(self.train_loader):]), \n",
    "                     np.mean(self.metrics['G_syn_losses'][-len(self.train_loader):]),\n",
    "                     np.mean(self.metrics['G_class_losses'][-len(self.train_loader):])))\n",
    "\n",
    "            # do checkpointing\n",
    "            if save_model == True:\n",
    "                save_model = False\n",
    "                torch.save(self.generator.state_dict(), '%s/G_epoch_%d_save_model.pth' % (os.path.join('.', 'GANAug/model/Lung'), epoch))\n",
    "                torch.save(self.discriminator.state_dict(), '%s/D_epoch_%d_save_model.pth' % (os.path.join('.', 'GANAug/model/Lung'), epoch))\n",
    "            elif early_stop:\n",
    "                torch.save(self.generator.state_dict(), '%s/G_epoch_%d_early_stop.pth' % (os.path.join('.', 'GANAug/model/Lung'), epoch))\n",
    "                torch.save(self.discriminator.state_dict(), '%s/D_epoch_%d_early_stop.pth' % (os.path.join('.', 'GANAug/model/Lung'), epoch))\n",
    "                break\n",
    "            elif epoch % round(self.num_epoch/10) == 0:\n",
    "                torch.save(self.generator.state_dict(), '%s/G_epoch_%d.pth' % (os.path.join('.', 'GANAug/model/Lung'), epoch))\n",
    "                torch.save(self.discriminator.state_dict(), '%s/D_epoch_%d.pth' % (os.path.join('.', 'GANAug/model/Lung'), epoch))\n",
    "            else:\n",
    "                print(\"---------------------< no model saved at epoch:\", epoch, \">---------------------\")\n",
    "\n",
    "            if len(only_pth_file(os.listdir(os.path.join('.', 'GANAug/model/Lung')))) > number_of_model_saved:\n",
    "                delete_list = get_file_list(os.path.join('.', 'GANAug/model/Lung'), \n",
    "                                            len(only_pth_file(os.listdir(os.path.join('.', 'GANAug/model/Lung')))) - number_of_model_saved)\n",
    "                for file in delete_list:\n",
    "                    if os.path.exists(os.path.join(os.path.join('.', 'GANAug/model/Lung'), file)):\n",
    "                        os.remove(os.path.join(os.path.join('.', 'GANAug/model/Lung'), file))\n",
    "                    else:\n",
    "                        print(\"file ->\", os.path.join(os.path.join('.', 'GANAug/model/Lung'), file), \"does not exist\")\n",
    "\n",
    "        self.metrics['Total_time'].append(time.time() - start_time)\n",
    "        print(\"Average epoch time: %.2f, total %d epochs time: %.2f\" % (np.mean(self.metrics['Time_per_epoch']),\n",
    "                                                                        self.num_epoch, self.metrics['Total_time'][0]))\n",
    "        generate_animation(os.path.join('.', 'GANAug/plots/Lung/') + self.model_name, self.num_epoch)\n",
    "        loss_plot(self.metrics, os.path.join('.', 'GANAug/plots/Lung/'))\n",
    "        print(\"Training Done !\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "11433097",
   "metadata": {},
   "outputs": [],
   "source": [
    "acgan = CACGAN()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "53308270",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[WinError 183] 当文件已存在时，无法创建该文件。: '.\\\\GANAug'\n",
      "[WinError 183] 当文件已存在时，无法创建该文件。: '.\\\\GANAug/model'\n",
      "[WinError 183] 当文件已存在时，无法创建该文件。: '.\\\\GANAug/plots'\n",
      "[WinError 183] 当文件已存在时，无法创建该文件。: '.\\\\GANAug/output_images'\n",
      "2\n",
      "3\n",
      "4\n",
      "Start Testing ...\n",
      "Generator(\n",
      "  (ReLU): ReLU(inplace=True)\n",
      "  (Tanh): Tanh()\n",
      "  (conv1): ConvTranspose2d(256, 2048, kernel_size=(4, 4), stride=(1, 1), bias=False)\n",
      "  (BatchNorm1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (conv2): ConvTranspose2d(2048, 2048, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "  (BatchNorm2): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (conv3): ConvTranspose2d(2048, 1024, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "  (BatchNorm3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (conv4): ConvTranspose2d(1024, 512, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "  (BatchNorm4): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (conv5): ConvTranspose2d(512, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "  (BatchNorm5): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (conv6): ConvTranspose2d(256, 3, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      ")\n",
      "Discriminator(\n",
      "  (LeakyReLU): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "  (DropOut1): Dropout(p=0.5, inplace=False)\n",
      "  (DropOut2): Dropout(p=0.25, inplace=False)\n",
      "  (conv1): Conv2d(3, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "  (conv2): Conv2d(256, 512, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "  (BatchNorm2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (conv3): Conv2d(512, 1024, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "  (BatchNorm3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (conv4_1): Conv2d(1024, 1024, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "  (BatchNorm4_1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (conv4_2): Conv2d(1024, 2048, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "  (BatchNorm4_2): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (conv4_3): Conv2d(2048, 2048, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "  (BatchNorm4_3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (conv5): Conv2d(2048, 256, kernel_size=(4, 4), stride=(1, 1), bias=False)\n",
      "  (disc_linear): Linear(in_features=256, out_features=1, bias=True)\n",
      "  (aux_linear): Linear(in_features=256, out_features=2, bias=True)\n",
      "  (softmax): Softmax(dim=None)\n",
      "  (sigmoid): Sigmoid()\n",
      ")\n",
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "   ConvTranspose2d-1           [64, 2048, 4, 4]       8,388,608\n",
      "       BatchNorm2d-2           [64, 2048, 4, 4]           4,096\n",
      "              ReLU-3           [64, 2048, 4, 4]               0\n",
      "   ConvTranspose2d-4           [64, 2048, 8, 8]      67,108,864\n",
      "       BatchNorm2d-5           [64, 2048, 8, 8]           4,096\n",
      "              ReLU-6           [64, 2048, 8, 8]               0\n",
      "   ConvTranspose2d-7         [64, 2048, 16, 16]      67,108,864\n",
      "       BatchNorm2d-8         [64, 2048, 16, 16]           4,096\n",
      "              ReLU-9         [64, 2048, 16, 16]               0\n",
      "  ConvTranspose2d-10         [64, 1024, 32, 32]      33,554,432\n",
      "      BatchNorm2d-11         [64, 1024, 32, 32]           2,048\n",
      "             ReLU-12         [64, 1024, 32, 32]               0\n",
      "  ConvTranspose2d-13          [64, 512, 64, 64]       8,388,608\n",
      "      BatchNorm2d-14          [64, 512, 64, 64]           1,024\n",
      "             ReLU-15          [64, 512, 64, 64]               0\n",
      "  ConvTranspose2d-16        [64, 256, 128, 128]       2,097,152\n",
      "      BatchNorm2d-17        [64, 256, 128, 128]             512\n",
      "             ReLU-18        [64, 256, 128, 128]               0\n",
      "  ConvTranspose2d-19          [64, 3, 256, 256]          12,288\n",
      "             Tanh-20          [64, 3, 256, 256]               0\n",
      "================================================================\n",
      "Total params: 186,674,688\n",
      "Trainable params: 186,674,688\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.06\n",
      "Forward/backward pass size (MB): 11952.00\n",
      "Params size (MB): 712.11\n",
      "Estimated Total Size (MB): 12664.17\n",
      "----------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lebro\\AppData\\Local\\Temp\\ipykernel_35556\\1075561295.py:68: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  c = self.softmax(c)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1        [64, 256, 128, 128]          12,288\n",
      "         LeakyReLU-2        [64, 256, 128, 128]               0\n",
      "           Dropout-3        [64, 256, 128, 128]               0\n",
      "            Conv2d-4          [64, 512, 64, 64]       2,097,152\n",
      "       BatchNorm2d-5          [64, 512, 64, 64]           1,024\n",
      "         LeakyReLU-6          [64, 512, 64, 64]               0\n",
      "            Conv2d-7         [64, 1024, 32, 32]       8,388,608\n",
      "       BatchNorm2d-8         [64, 1024, 32, 32]           2,048\n",
      "         LeakyReLU-9         [64, 1024, 32, 32]               0\n",
      "          Dropout-10         [64, 1024, 32, 32]               0\n",
      "           Conv2d-11         [64, 1024, 16, 16]      16,777,216\n",
      "      BatchNorm2d-12         [64, 1024, 16, 16]           2,048\n",
      "        LeakyReLU-13         [64, 1024, 16, 16]               0\n",
      "          Dropout-14         [64, 1024, 16, 16]               0\n",
      "           Conv2d-15           [64, 2048, 8, 8]      33,554,432\n",
      "      BatchNorm2d-16           [64, 2048, 8, 8]           4,096\n",
      "        LeakyReLU-17           [64, 2048, 8, 8]               0\n",
      "          Dropout-18           [64, 2048, 8, 8]               0\n",
      "           Conv2d-19           [64, 2048, 4, 4]      67,108,864\n",
      "      BatchNorm2d-20           [64, 2048, 4, 4]           4,096\n",
      "        LeakyReLU-21           [64, 2048, 4, 4]               0\n",
      "          Dropout-22           [64, 2048, 4, 4]               0\n",
      "           Conv2d-23            [64, 256, 1, 1]       8,388,608\n",
      "           Linear-24                    [64, 2]             514\n",
      "          Softmax-25                    [64, 2]               0\n",
      "           Linear-26                    [64, 1]             257\n",
      "          Sigmoid-27                    [64, 1]               0\n",
      "================================================================\n",
      "Total params: 136,341,251\n",
      "Trainable params: 136,341,251\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 48.00\n",
      "Forward/backward pass size (MB): 12096.13\n",
      "Params size (MB): 520.10\n",
      "Estimated Total Size (MB): 12664.23\n",
      "----------------------------------------------------------------\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA out of memory. Tried to allocate 1024.00 MiB (GPU 0; 6.00 GiB total capacity; 3.78 GiB already allocated; 0 bytes free; 4.60 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Input \u001b[1;32mIn [10]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43macgan\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[1;32mIn [8]\u001b[0m, in \u001b[0;36mCACGAN.train\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    182\u001b[0m summary(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgenerator, (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msize_of_z, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m), batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_size, device\u001b[38;5;241m=\u001b[39mdevice)\n\u001b[0;32m    183\u001b[0m summary(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdiscriminator, (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_of_ch, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mimg_size, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mimg_size), batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_size, device\u001b[38;5;241m=\u001b[39mdevice)\n\u001b[1;32m--> 184\u001b[0m g_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerator\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrand\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msize_of_z\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    185\u001b[0m \u001b[38;5;28mprint\u001b[39m(g_output\u001b[38;5;241m.\u001b[39mshape)\n\u001b[0;32m    186\u001b[0m s_output, c_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdiscriminator(torch\u001b[38;5;241m.\u001b[39mrand(g_output\u001b[38;5;241m.\u001b[39mshape)\u001b[38;5;241m.\u001b[39mto(device))\n",
      "File \u001b[1;32mD:\\Miniconda3\\envs\\torch_1-11\\lib\\site-packages\\torch\\nn\\modules\\module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1106\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1107\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1108\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1109\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1110\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1111\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1112\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Input \u001b[1;32mIn [6]\u001b[0m, in \u001b[0;36mGenerator.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    130\u001b[0m \u001b[38;5;66;03m#x = self.DropOut(x)\u001b[39;00m\n\u001b[0;32m    132\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv5(x)\n\u001b[1;32m--> 133\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mBatchNorm5\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    134\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mReLU(x)\n\u001b[0;32m    135\u001b[0m \u001b[38;5;66;03m#x = self.DropOut(x)\u001b[39;00m\n",
      "File \u001b[1;32mD:\\Miniconda3\\envs\\torch_1-11\\lib\\site-packages\\torch\\nn\\modules\\module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1106\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1107\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1108\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1109\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1110\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1111\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1112\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32mD:\\Miniconda3\\envs\\torch_1-11\\lib\\site-packages\\torch\\nn\\modules\\batchnorm.py:168\u001b[0m, in \u001b[0;36m_BatchNorm.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    161\u001b[0m     bn_training \u001b[38;5;241m=\u001b[39m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrunning_mean \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;129;01mand\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrunning_var \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m    163\u001b[0m \u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    164\u001b[0m \u001b[38;5;124;03mBuffers are only updated if they are to be tracked and we are in training mode. Thus they only need to be\u001b[39;00m\n\u001b[0;32m    165\u001b[0m \u001b[38;5;124;03mpassed when the update should occur (i.e. in training mode when they are tracked), or when buffer stats are\u001b[39;00m\n\u001b[0;32m    166\u001b[0m \u001b[38;5;124;03mused for normalization (i.e. in eval mode when buffers are not None).\u001b[39;00m\n\u001b[0;32m    167\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m--> 168\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbatch_norm\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    169\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    170\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# If buffers are not to be tracked, ensure that they won't be updated\u001b[39;49;00m\n\u001b[0;32m    171\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrunning_mean\u001b[49m\n\u001b[0;32m    172\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrack_running_stats\u001b[49m\n\u001b[0;32m    173\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    174\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrunning_var\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrack_running_stats\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    175\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    176\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    177\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbn_training\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    178\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexponential_average_factor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    179\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    180\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mD:\\Miniconda3\\envs\\torch_1-11\\lib\\site-packages\\torch\\nn\\functional.py:2421\u001b[0m, in \u001b[0;36mbatch_norm\u001b[1;34m(input, running_mean, running_var, weight, bias, training, momentum, eps)\u001b[0m\n\u001b[0;32m   2418\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m training:\n\u001b[0;32m   2419\u001b[0m     _verify_batch_size(\u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39msize())\n\u001b[1;32m-> 2421\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbatch_norm\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2422\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrunning_mean\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrunning_var\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtraining\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmomentum\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackends\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcudnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menabled\u001b[49m\n\u001b[0;32m   2423\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 1024.00 MiB (GPU 0; 6.00 GiB total capacity; 3.78 GiB already allocated; 0 bytes free; 4.60 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "acgan.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09e18531",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_gan(acgan):\n",
    "    now = datetime.datetime.now()\n",
    "    g_losses = acgan.metrics['G_losses'][-1]\n",
    "    d_losses = acgan.metrics['D_losses'][-1]\n",
    "    path='GANAug/output_images/Lung'\n",
    "    try:\n",
    "        os.mkdir(os.path.join('.', path))\n",
    "    except Exception as error:\n",
    "        print(error)\n",
    "\n",
    "    real_batch = next(iter(acgan.train_loader))\n",
    "    \n",
    "    test_img_list = []\n",
    "    test_noise = torch.randn(acgan.batch_size, acgan.size_of_z, device=device)\n",
    "    test_label = torch.randn(acgan.batch_size, acgan.num_of_label, device=device)\n",
    "    test_fake = acgan.generator(test_noise.resize_(acgan.batch_size, acgan.size_of_z, 1, 1)).detach().cpu()\n",
    "    test_img_list.append(vutils.make_grid(test_fake, padding=2, normalize=True))\n",
    "\n",
    "    fig = plt.figure(figsize=(20, 20))\n",
    "    ax1 = plt.subplot(1,2,1)\n",
    "    ax1 = plt.axis(\"off\")\n",
    "    ax1 = plt.title(\"Real Images\")\n",
    "    ax1 = plt.imshow(np.transpose(vutils.make_grid(real_batch[0].to(device)[:64], padding=5, normalize=True).cpu(),(1,2,0)))\n",
    "\n",
    "    ax2 = plt.subplot(1,2,2)\n",
    "    ax2 = plt.axis(\"off\")\n",
    "    ax2 = plt.title(\"Fake Images\")\n",
    "    ax2 = plt.imshow(np.transpose(test_img_list[-1],(1,2,0)))\n",
    "    plt.show()\n",
    "    fig.savefig('%s/image_%.3f_%.3f_%d_%s.png' %\n",
    "                   (path, g_losses, d_losses, acgan.num_epoch, now.strftime(\"%Y-%m-%d_%H-%M-%S\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03814c7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_gan(acgan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9cca5ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_gan(name, train_epoch, values, path, save):\n",
    "    clear_output(wait=True)\n",
    "    plt.close('all')\n",
    "    fig = plt.figure(figsize=(20, 20))\n",
    "    fig = plt.ion()\n",
    "    fig = plt.subplot(1, 1, 1)\n",
    "    fig = plt.title('epoch: %s -> %s: %s' % (train_epoch, name, values[-1]))\n",
    "    fig = plt.ylabel(name)\n",
    "    fig = plt.xlabel('train_set')\n",
    "    fig = plt.plot(values)\n",
    "    fig = plt.grid()\n",
    "    get_fig = plt.gcf()\n",
    "    fig = plt.draw()  # draw the plot\n",
    "    fig = plt.pause(1)  # show it for 1 second\n",
    "    plt.show()\n",
    "    if save:\n",
    "        now = datetime.datetime.now()\n",
    "        get_fig.savefig('%s/%s_%.3f_%d_%s.png' %\n",
    "                        (path, name, train_epoch, values[-1], now.strftime(\"%Y-%m-%d_%H-%M-%S\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d5e7892",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model(generator, discriminator, gen_optimizer, dis_optimizer, metrics, num_epochs):\n",
    "    now = datetime.datetime.now()\n",
    "    g_losses = metrics['G_losses'][-1]\n",
    "    d_losses = metrics['D_losses'][-1]\n",
    "    path='GANAug/plots/Lung/train_%+.3f_%+.3f_%s'% (g_losses, d_losses, now.strftime(\"%Y-%m-%d_%H-%M-%S\"))\n",
    "    try:\n",
    "        os.mkdir(os.path.join('.', path))\n",
    "    except Exception as error:\n",
    "        print(error)\n",
    "    plot_gan('G_losses', num_epochs, metrics['G_losses'], path, True)\n",
    "    plot_gan('D_losses', num_epochs, metrics['D_losses'], path, True)\n",
    "    plot_gan('G_syn_losses', num_epochs, metrics['G_syn_losses'], path, True)\n",
    "    plot_gan('G_class_losses', num_epochs, metrics['G_class_losses'], path, True)\n",
    "    plot_gan('G_syn_losses', num_epochs, metrics['G_syn_losses'], path, True)\n",
    "    plot_gan('D_class_losses', num_epochs, metrics['D_class_losses'], path, True)\n",
    "    plot_gan('D_syn_losses', num_epochs, metrics['D_syn_losses'], path, True)\n",
    "    plot_gan('Losses', num_epochs, metrics['Losses'], path, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f66ad873",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "save_model(acgan.generator, acgan.discriminator, acgan.optimizer_g, acgan.optimizer_d, acgan.metrics, acgan.num_epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c56cd9c9",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "test_img_list = []\n",
    "test_noise = torch.randn(acgan.batch_size, acgan.size_of_z, device=device)\n",
    "test_label = torch.randn(acgan.batch_size, acgan.num_of_label, device=device)\n",
    "test_img = acgan.generator(test_noise.resize_(acgan.batch_size, acgan.size_of_z, 1, 1))\n",
    "\n",
    "s_output, c_label_op = acgan.discriminator(test_img.detach().to(device))\n",
    "print('Discriminator s', s_output)\n",
    "print('Discriminator c', c_label_op)\n",
    "\n",
    "test_img = test_img.detach().cpu()\n",
    "test_img_list.append(vutils.make_grid(test_img, padding=2, normalize=True))\n",
    "plt.imshow(np.transpose(test_img_list[-1],(1,2,0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19c52ffc",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = next(iter(acgan.test_loader))\n",
    "test_noise, test_class_lable = data\n",
    "test_img = test_noise\n",
    "print('class label for real', test_class_lable)\n",
    "\n",
    "s_output,c_label_op = acgan.discriminator(test_img.detach().to(device))\n",
    "print('Discriminator s', s_output)\n",
    "print('Discriminator c', c_label_op)\n",
    "\n",
    "test_img = test_img.detach().cpu()\n",
    "test_img_list.append(vutils.make_grid(test_img, padding=2, normalize=True))\n",
    "plt.imshow(np.transpose(test_img_list[-1],(1,2,0)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
