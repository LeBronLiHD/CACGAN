{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6a6a2202",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.parallel\n",
    "import torch.nn.functional as F\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.optim as optim\n",
    "import torch.utils.data\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.utils as vutils\n",
    "from torchvision.utils import save_image\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import SubsetRandomSampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6cf2ef10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['sm_37', 'sm_50', 'sm_60', 'sm_61', 'sm_70', 'sm_75', 'sm_80', 'sm_86', 'compute_37'] cuda\n"
     ]
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(torch.cuda.get_arch_list(), device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b8079c45",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import argparse\n",
    "import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import HTML\n",
    "from IPython.display import clear_output\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "from tqdm import notebook\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from tabulate import tabulate\n",
    "import gc\n",
    "import matplotlib.colors as mat_color\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import random\n",
    "import cv2\n",
    "from torchvision.datasets import ImageNet, ImageFolder\n",
    "from torch.utils.data import DataLoader\n",
    "from torchviz import make_dot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6dcbda96",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "\n",
    "    def __init__(self, nz, ngf, nc):\n",
    "\n",
    "        super(Generator, self).__init__()\n",
    "        self.ReLU = nn.ReLU(True)\n",
    "        self.Tanh = nn.Tanh()\n",
    "        self.DropOut = nn.Dropout(p=0.5)\n",
    "        self.conv1 = nn.ConvTranspose2d(nz, ngf * 8, 4, 1, 0, bias=False)\n",
    "        self.BatchNorm1 = nn.BatchNorm2d(ngf * 8)\n",
    "\n",
    "        self.conv2 = nn.ConvTranspose2d(ngf * 8, ngf * 4, 4, 2, 1, bias=False)\n",
    "        self.BatchNorm2 = nn.BatchNorm2d(ngf * 4)\n",
    "\n",
    "        self.conv3 = nn.ConvTranspose2d(ngf * 4, ngf * 2, 4, 2, 1, bias=False)\n",
    "        self.BatchNorm3 = nn.BatchNorm2d(ngf * 2)\n",
    "\n",
    "        self.conv4 = nn.ConvTranspose2d(ngf * 2, ngf * 1, 4, 2, 1, bias=False)\n",
    "        self.BatchNorm4 = nn.BatchNorm2d(ngf * 1)\n",
    "\n",
    "        self.conv5 = nn.ConvTranspose2d(ngf * 1, nc, 4, 2, 1, bias=False)\n",
    "\n",
    "        self.apply(weights_init)\n",
    "\n",
    "\n",
    "    def forward(self, input):\n",
    "        x = self.conv1(input)\n",
    "        x = self.BatchNorm1(x)\n",
    "        x = self.ReLU(x)\n",
    "        #x = self.DropOut(x)\n",
    "\n",
    "        x = self.conv2(x)\n",
    "        x = self.BatchNorm2(x)\n",
    "        x = self.ReLU(x)\n",
    "        #x = self.DropOut(x)\n",
    "\n",
    "        x = self.conv3(x)\n",
    "        x = self.BatchNorm3(x)\n",
    "        x = self.ReLU(x)\n",
    "        #x = self.DropOut(x)\n",
    "\n",
    "        x = self.conv4(x)\n",
    "        x = self.BatchNorm4(x)\n",
    "        x = self.ReLU(x)\n",
    "        #x = self.DropOut(x)\n",
    "\n",
    "        x = self.conv5(x)\n",
    "        output = self.Tanh(x)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "62fd43ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "\n",
    "    def __init__(self, ndf, nc, nb_label):\n",
    "\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.LeakyReLU = nn.LeakyReLU(0.2, inplace=True)\n",
    "        self.DropOut1 = nn.Dropout(p=0.5)\n",
    "        self.DropOut2 = nn.Dropout(p=0.25)\n",
    "        self.conv1 = nn.Conv2d(nc, ndf, 4, 2, 1, bias=False)\n",
    "        self.conv2 = nn.Conv2d(ndf, ndf * 2, 4, 2, 1, bias=False)\n",
    "        self.BatchNorm2 = nn.BatchNorm2d(ndf * 2)\n",
    "        self.conv3 = nn.Conv2d(ndf * 2, ndf * 4, 4, 2, 1, bias=False)\n",
    "        self.BatchNorm3 = nn.BatchNorm2d(ndf * 4)\n",
    "        self.conv4 = nn.Conv2d(ndf * 4, ndf * 8, 4, 2, 1, bias=False)\n",
    "        self.BatchNorm4 = nn.BatchNorm2d(ndf * 8)\n",
    "        self.conv5 = nn.Conv2d(ndf * 8, ndf * 1, 4, 1, 0, bias=False)\n",
    "        self.disc_linear = nn.Linear(ndf * 1, 1)\n",
    "        self.aux_linear = nn.Linear(ndf * 1, nb_label)\n",
    "        self.softmax = nn.Softmax()\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        self.ndf = ndf\n",
    "        self.apply(weights_init)\n",
    "\n",
    "    def forward(self, input):\n",
    "\n",
    "        x = self.conv1(input)\n",
    "        x = self.LeakyReLU(x)\n",
    "        x = self.DropOut1(x)\n",
    "\n",
    "        x = self.conv2(x)\n",
    "        x = self.BatchNorm2(x)\n",
    "        x = self.LeakyReLU(x)\n",
    "\n",
    "        x = self.conv3(x)\n",
    "        x = self.BatchNorm3(x)\n",
    "        x = self.LeakyReLU(x)\n",
    "        x = self.DropOut1(x)\n",
    "\n",
    "        x = self.conv4(x)\n",
    "        x = self.BatchNorm4(x)\n",
    "        x = self.LeakyReLU(x)\n",
    "        x = self.DropOut2(x)\n",
    "\n",
    "        x = self.conv5(x)\n",
    "        x = x.view(-1, self.ndf * 1)\n",
    "        c = self.aux_linear(x)\n",
    "        c = self.softmax(c)\n",
    "        s = self.disc_linear(x)\n",
    "        s = self.sigmoid(s)\n",
    "        return s, c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6707e895",
   "metadata": {},
   "outputs": [],
   "source": [
    "def weights_init(m):\n",
    "    classname = m.__class__.__name__\n",
    "    if classname.find('Conv') != -1:\n",
    "        m.weight.data.normal_(0.0, 0.05)\n",
    "    elif classname.find('BatchNorm') != -1:\n",
    "        m.weight.data.normal_(1.0, 0.05)\n",
    "        m.bias.data.fill_(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "220b9685",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_path = './data'\n",
    "base_folder = \"Covid-19 Image Dataset\"\n",
    "classic_folder = 'Coivd-19_Classic'\n",
    "synthetic_folder = 'Coivd-19_Synthetic'\n",
    "data_dir = os.path.join(base_path, classic_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9b6ea5a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_size = 512\n",
    "batch_size = 16\n",
    "train_path = os.path.join(data_dir, \"train\")\n",
    "test_path = os.path.join(data_dir, \"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1bf35c4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Covid', 'Normal', 'Viral Pneumonia']\n"
     ]
    }
   ],
   "source": [
    "labels = os.listdir(train_path)\n",
    "print(labels)\n",
    "no_norm = mat_color.Normalize(vmin=0, vmax=255, clip=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "28c7ac6b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[ 0.0000]],\n",
       "\n",
       "         [[ 1.0000]],\n",
       "\n",
       "         [[ 0.0000]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[ 0.8238]],\n",
       "\n",
       "         [[-0.8609]],\n",
       "\n",
       "         [[ 0.8747]]],\n",
       "\n",
       "\n",
       "        [[[ 1.0000]],\n",
       "\n",
       "         [[ 0.0000]],\n",
       "\n",
       "         [[ 0.0000]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[ 0.4400]],\n",
       "\n",
       "         [[ 1.0010]],\n",
       "\n",
       "         [[ 3.2787]]],\n",
       "\n",
       "\n",
       "        [[[ 0.0000]],\n",
       "\n",
       "         [[ 1.0000]],\n",
       "\n",
       "         [[ 0.0000]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[ 1.0701]],\n",
       "\n",
       "         [[-1.4963]],\n",
       "\n",
       "         [[-0.4839]]],\n",
       "\n",
       "\n",
       "        ...,\n",
       "\n",
       "\n",
       "        [[[ 1.0000]],\n",
       "\n",
       "         [[ 0.0000]],\n",
       "\n",
       "         [[ 0.0000]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[-0.1090]],\n",
       "\n",
       "         [[ 0.2267]],\n",
       "\n",
       "         [[ 0.2689]]],\n",
       "\n",
       "\n",
       "        [[[ 1.0000]],\n",
       "\n",
       "         [[ 0.0000]],\n",
       "\n",
       "         [[ 0.0000]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[ 1.0895]],\n",
       "\n",
       "         [[ 0.4633]],\n",
       "\n",
       "         [[ 0.1911]]],\n",
       "\n",
       "\n",
       "        [[[ 0.0000]],\n",
       "\n",
       "         [[ 0.0000]],\n",
       "\n",
       "         [[ 1.0000]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[ 0.2516]],\n",
       "\n",
       "         [[-0.7250]],\n",
       "\n",
       "         [[-0.8954]]]], device='cuda:0')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Number of training epochs\n",
    "num_epochs = 100\n",
    "\n",
    "#All images will be resized to this size using a transformer.\n",
    "#image_size = 64\n",
    "imageSize = 512\n",
    "\n",
    "# Number of channels in the training images. For color images this is 3\n",
    "nc = 3\n",
    "\n",
    "# Size of z latent vector (i.e. size of generator input)\n",
    "nz = 128\n",
    "\n",
    "# Size of feature maps in generator\n",
    "ngf = 256\n",
    "\n",
    "# Size of feature maps in discriminator\n",
    "ndf = 256\n",
    "\n",
    "# No of labels\n",
    "nb_label = len(labels)\n",
    "\n",
    "# Learning rate for optimizers\n",
    "lr = 0.002\n",
    "lr_d = 0.0002\n",
    "# Beta1 hyperparam for Adam optimizers\n",
    "beta1 = 0.5\n",
    "# Beta2 hyperparam for Adam optimizers\n",
    "beta2 = 0.999\n",
    "\n",
    "real_label = 1.\n",
    "fake_label = 0.\n",
    "# Input to generator\n",
    "fixed_noise = torch.randn(64, nz, 1, 1, device=device) #batch of 64\n",
    "# Define Loss function\n",
    "s_criterion = nn.BCELoss().to(device) #For synthesizing\n",
    "c_criterion = nn.NLLLoss().to(device) #For classification\n",
    "\n",
    "input = torch.FloatTensor(batch_size, nc, imageSize, imageSize).to(device)\n",
    "noise = torch.FloatTensor(batch_size, nz, 1, 1).to(device)\n",
    "fixed_noise = torch.FloatTensor(batch_size, nz, 1, 1).normal_(0, 1).to(device)\n",
    "s_label = torch.FloatTensor(batch_size).to(device)\n",
    "c_label = torch.LongTensor(batch_size).to(device)\n",
    "\n",
    "input = Variable(input)\n",
    "s_label = Variable(s_label)\n",
    "c_label = Variable(c_label)\n",
    "noise = Variable(noise)\n",
    "fixed_noise = Variable(fixed_noise)\n",
    "fixed_noise_ = np.random.normal(0, 1, (batch_size, nz))\n",
    "random_label = np.random.randint(0, nb_label, batch_size)\n",
    "#print('fixed label:{}'.format(random_label))\n",
    "random_onehot = np.zeros((batch_size, nb_label))\n",
    "random_onehot[np.arange(batch_size), random_label] = 1\n",
    "fixed_noise_[np.arange(batch_size), :nb_label] = random_onehot[np.arange(batch_size)]\n",
    "\n",
    "\n",
    "fixed_noise_ = (torch.from_numpy(fixed_noise_))\n",
    "fixed_noise_ = fixed_noise_.resize_(batch_size, nz, 1, 1)\n",
    "fixed_noise.data.copy_(fixed_noise_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bd17020a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[WinError 183] 当文件已存在时，无法创建该文件。: '.\\\\GANAug'\n",
      "[WinError 183] 当文件已存在时，无法创建该文件。: '.\\\\GANAug/model'\n",
      "[WinError 183] 当文件已存在时，无法创建该文件。: '.\\\\GANAug/plots'\n",
      "[WinError 183] 当文件已存在时，无法创建该文件。: '.\\\\GANAug/model/ACGAN'\n",
      "[WinError 183] 当文件已存在时，无法创建该文件。: '.\\\\GANAug/plots/ACGAN'\n",
      "[WinError 183] 当文件已存在时，无法创建该文件。: '.\\\\GANAug/output_images'\n",
      "[WinError 183] 当文件已存在时，无法创建该文件。: '.\\\\GANAug/output_images/ACGAN'\n"
     ]
    }
   ],
   "source": [
    "for func in [\n",
    "    lambda: os.mkdir(os.path.join('.', 'GANAug')),\n",
    "    lambda: os.mkdir(os.path.join('.', 'GANAug/model')),\n",
    "    lambda: os.mkdir(os.path.join('.', 'GANAug/plots')),\n",
    "    lambda: os.mkdir(os.path.join('.', 'GANAug/model/ACGAN')),\n",
    "    lambda: os.mkdir(os.path.join('.', 'GANAug/plots/ACGAN')),\n",
    "    lambda: os.mkdir(os.path.join('.', 'GANAug/output_images')),\n",
    "    lambda: os.mkdir(os.path.join('.', 'GANAug/output_images/ACGAN'))]:  # create directories\n",
    "    try:\n",
    "        func()\n",
    "    except Exception as error:\n",
    "        print(error)\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "74dda24a",
   "metadata": {},
   "outputs": [],
   "source": [
    "METRIC_FIELDS = [\n",
    "    'train.D_x',\n",
    "    'train.D_G_z1',\n",
    "    'train.D_G_z2',\n",
    "    'train.G_losses',\n",
    "    'train.D_losses',\n",
    "]\n",
    "metrics = {field: list() for field in METRIC_FIELDS}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a26a75d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset(train_dir=train_path, test_dir=test_path):\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((img_size, img_size)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "        ])\n",
    "    train_data = datasets.ImageFolder(train_dir, transform=transform)\n",
    "    train_loader = DataLoader(train_data, batch_size, shuffle=True, num_workers=1)\n",
    "\n",
    "    test_data = datasets.ImageFolder(test_dir ,transform=transform)\n",
    "    test_loader = DataLoader(test_data, batch_size, shuffle=True, num_workers=1)\n",
    "\n",
    "    return train_loader, test_loader, train_data, test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bbb0b552",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader, test_loader, train_data, test_data = load_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0360680c",
   "metadata": {},
   "outputs": [],
   "source": [
    "generator = Generator(nz, ngf, nc).to(device)\n",
    "discriminator = Discriminator(ndf, nc, nb_label).to(device)\n",
    "\n",
    "# setup optimizer\n",
    "optimizerD = optim.Adam(discriminator.parameters(), lr=lr_d, betas=(beta1, beta2))\n",
    "optimizerG = optim.Adam(generator.parameters(), lr=lr, betas=(beta1, beta2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "065eed00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torchviz in d:\\miniconda3\\envs\\torch_1-11\\lib\\site-packages (0.0.2)\n",
      "Requirement already satisfied: graphviz in d:\\miniconda3\\envs\\torch_1-11\\lib\\site-packages (from torchviz) (0.20)\n",
      "Requirement already satisfied: torch in d:\\miniconda3\\envs\\torch_1-11\\lib\\site-packages (from torchviz) (1.11.0)\n",
      "Requirement already satisfied: typing_extensions in d:\\miniconda3\\envs\\torch_1-11\\lib\\site-packages (from torch->torchviz) (4.1.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install torchviz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "81981a02",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generator(\n",
      "  (ReLU): ReLU(inplace=True)\n",
      "  (Tanh): Tanh()\n",
      "  (DropOut): Dropout(p=0.5, inplace=False)\n",
      "  (conv1): ConvTranspose2d(128, 2048, kernel_size=(4, 4), stride=(1, 1), bias=False)\n",
      "  (BatchNorm1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (conv2): ConvTranspose2d(2048, 1024, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "  (BatchNorm2): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (conv3): ConvTranspose2d(1024, 512, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "  (BatchNorm3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (conv4): ConvTranspose2d(512, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "  (BatchNorm4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (conv5): ConvTranspose2d(256, 3, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      ")\n",
      "Discriminator(\n",
      "  (LeakyReLU): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "  (DropOut1): Dropout(p=0.5, inplace=False)\n",
      "  (DropOut2): Dropout(p=0.25, inplace=False)\n",
      "  (conv1): Conv2d(3, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "  (conv2): Conv2d(256, 512, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "  (BatchNorm2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (conv3): Conv2d(512, 1024, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "  (BatchNorm3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (conv4): Conv2d(1024, 2048, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "  (BatchNorm4): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (conv5): Conv2d(2048, 256, kernel_size=(4, 4), stride=(1, 1), bias=False)\n",
      "  (disc_linear): Linear(in_features=256, out_features=1, bias=True)\n",
      "  (aux_linear): Linear(in_features=256, out_features=3, bias=True)\n",
      "  (softmax): Softmax(dim=None)\n",
      "  (sigmoid): Sigmoid()\n",
      ")\n",
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "   ConvTranspose2d-1          [1, 2048, 259, 6]       4,194,304\n",
      "       BatchNorm2d-2          [1, 2048, 259, 6]           4,096\n",
      "              ReLU-3          [1, 2048, 259, 6]               0\n",
      "   ConvTranspose2d-4         [1, 1024, 518, 12]      33,554,432\n",
      "       BatchNorm2d-5         [1, 1024, 518, 12]           2,048\n",
      "              ReLU-6         [1, 1024, 518, 12]               0\n",
      "   ConvTranspose2d-7         [1, 512, 1036, 24]       8,388,608\n",
      "       BatchNorm2d-8         [1, 512, 1036, 24]           1,024\n",
      "              ReLU-9         [1, 512, 1036, 24]               0\n",
      "  ConvTranspose2d-10         [1, 256, 2072, 48]       2,097,152\n",
      "      BatchNorm2d-11         [1, 256, 2072, 48]             512\n",
      "             ReLU-12         [1, 256, 2072, 48]               0\n",
      "  ConvTranspose2d-13           [1, 3, 4144, 96]          12,288\n",
      "             Tanh-14           [1, 3, 4144, 96]               0\n",
      "================================================================\n",
      "Total params: 48,254,464\n",
      "Trainable params: 48,254,464\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.38\n",
      "Forward/backward pass size (MB): 1110.87\n",
      "Params size (MB): 184.08\n",
      "Estimated Total Size (MB): 1295.32\n",
      "----------------------------------------------------------------\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Given groups=1, weight of size [256, 3, 4, 4], expected input[2, 256, 3, 3] to have 3 channels, but got 256 channels instead",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Input \u001b[1;32mIn [17]\u001b[0m, in \u001b[0;36m<cell line: 7>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorchsummary\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m summary\n\u001b[0;32m      6\u001b[0m summary(generator, (nz, ngf, nc), batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, device\u001b[38;5;241m=\u001b[39mdevice)\n\u001b[1;32m----> 7\u001b[0m \u001b[43msummary\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdiscriminator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mndf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnb_label\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mD:\\Miniconda3\\envs\\torch_1-11\\lib\\site-packages\\torchsummary\\torchsummary.py:72\u001b[0m, in \u001b[0;36msummary\u001b[1;34m(model, input_size, batch_size, device)\u001b[0m\n\u001b[0;32m     68\u001b[0m model\u001b[38;5;241m.\u001b[39mapply(register_hook)\n\u001b[0;32m     70\u001b[0m \u001b[38;5;66;03m# make a forward pass\u001b[39;00m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;66;03m# print(x.shape)\u001b[39;00m\n\u001b[1;32m---> 72\u001b[0m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     74\u001b[0m \u001b[38;5;66;03m# remove these hooks\u001b[39;00m\n\u001b[0;32m     75\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m h \u001b[38;5;129;01min\u001b[39;00m hooks:\n",
      "File \u001b[1;32mD:\\Miniconda3\\envs\\torch_1-11\\lib\\site-packages\\torch\\nn\\modules\\module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1106\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1107\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1108\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1109\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1110\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1111\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1112\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Input \u001b[1;32mIn [5]\u001b[0m, in \u001b[0;36mDiscriminator.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m---> 26\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv1\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     27\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mLeakyReLU(x)\n\u001b[0;32m     28\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mDropOut1(x)\n",
      "File \u001b[1;32mD:\\Miniconda3\\envs\\torch_1-11\\lib\\site-packages\\torch\\nn\\modules\\module.py:1128\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1125\u001b[0m     bw_hook \u001b[38;5;241m=\u001b[39m hooks\u001b[38;5;241m.\u001b[39mBackwardHook(\u001b[38;5;28mself\u001b[39m, full_backward_hooks)\n\u001b[0;32m   1126\u001b[0m     \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m bw_hook\u001b[38;5;241m.\u001b[39msetup_input_hook(\u001b[38;5;28minput\u001b[39m)\n\u001b[1;32m-> 1128\u001b[0m result \u001b[38;5;241m=\u001b[39m forward_call(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1129\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks:\n\u001b[0;32m   1130\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m hook \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;241m*\u001b[39m_global_forward_hooks\u001b[38;5;241m.\u001b[39mvalues(), \u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks\u001b[38;5;241m.\u001b[39mvalues()):\n",
      "File \u001b[1;32mD:\\Miniconda3\\envs\\torch_1-11\\lib\\site-packages\\torch\\nn\\modules\\conv.py:447\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    446\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 447\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_conv_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mD:\\Miniconda3\\envs\\torch_1-11\\lib\\site-packages\\torch\\nn\\modules\\conv.py:443\u001b[0m, in \u001b[0;36mConv2d._conv_forward\u001b[1;34m(self, input, weight, bias)\u001b[0m\n\u001b[0;32m    439\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mzeros\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m    440\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mconv2d(F\u001b[38;5;241m.\u001b[39mpad(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode),\n\u001b[0;32m    441\u001b[0m                     weight, bias, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstride,\n\u001b[0;32m    442\u001b[0m                     _pair(\u001b[38;5;241m0\u001b[39m), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdilation, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroups)\n\u001b[1;32m--> 443\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv2d\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    444\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroups\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Given groups=1, weight of size [256, 3, 4, 4], expected input[2, 256, 3, 3] to have 3 channels, but got 256 channels instead"
     ]
    }
   ],
   "source": [
    "print(generator)\n",
    "print(discriminator)\n",
    "\n",
    "from torchsummary import summary\n",
    "\n",
    "summary(generator, (nz, ngf, nc), batch_size=1, device=device)\n",
    "summary(discriminator, (ndf, nc, nb_label), batch_size=1, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0580f119",
   "metadata": {},
   "outputs": [],
   "source": [
    "# os.environ[\"PATH\"] += os.pathsep + 'D:\\Miniconda3\\pkgs\\graphviz-2.38-hfd603c8_2\\Library\\bin\\graphviz\\dot.exe'\n",
    "\n",
    "def modeltorchviz(model,input2):\n",
    "    y = model(input2.cuda())    # 获取网络的预测值\n",
    "    MyConvNetVis = make_dot(y, params=dict(list(model.named_parameters()) + [('x', input2)]))\n",
    "    MyConvNetVis.format = \"png\"\n",
    "    # 指定文件生成的文件夹\n",
    "    MyConvNetVis.directory = \"images\"\n",
    "    # 生成文件\n",
    "    MyConvNetVis.view() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f0cd9edc",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "CUDA out of memory. Tried to allocate 1.52 GiB (GPU 0; 6.00 GiB total capacity; 3.08 GiB already allocated; 0 bytes free; 4.03 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Input \u001b[1;32mIn [19]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mmodeltorchviz\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgenerator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrandn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnz\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mngf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnc\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequires_grad_\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[1;32mIn [18]\u001b[0m, in \u001b[0;36mmodeltorchviz\u001b[1;34m(model, input2)\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmodeltorchviz\u001b[39m(model,input2):\n\u001b[1;32m----> 4\u001b[0m     y \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput2\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcuda\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m    \u001b[38;5;66;03m# 获取网络的预测值\u001b[39;00m\n\u001b[0;32m      5\u001b[0m     MyConvNetVis \u001b[38;5;241m=\u001b[39m make_dot(y, params\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mdict\u001b[39m(\u001b[38;5;28mlist\u001b[39m(model\u001b[38;5;241m.\u001b[39mnamed_parameters()) \u001b[38;5;241m+\u001b[39m [(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mx\u001b[39m\u001b[38;5;124m'\u001b[39m, input2)]))\n\u001b[0;32m      6\u001b[0m     MyConvNetVis\u001b[38;5;241m.\u001b[39mformat \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpng\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[1;32mD:\\Miniconda3\\envs\\torch_1-11\\lib\\site-packages\\torch\\nn\\modules\\module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1106\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1107\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1108\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1109\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1110\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1111\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1112\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Input \u001b[1;32mIn [4]\u001b[0m, in \u001b[0;36mGenerator.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m     39\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mReLU(x)\n\u001b[0;32m     40\u001b[0m \u001b[38;5;66;03m#x = self.DropOut(x)\u001b[39;00m\n\u001b[1;32m---> 42\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv4\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     43\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mBatchNorm4(x)\n\u001b[0;32m     44\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mReLU(x)\n",
      "File \u001b[1;32mD:\\Miniconda3\\envs\\torch_1-11\\lib\\site-packages\\torch\\nn\\modules\\module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1106\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1107\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1108\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1109\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1110\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1111\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1112\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32mD:\\Miniconda3\\envs\\torch_1-11\\lib\\site-packages\\torch\\nn\\modules\\conv.py:925\u001b[0m, in \u001b[0;36mConvTranspose2d.forward\u001b[1;34m(self, input, output_size)\u001b[0m\n\u001b[0;32m    920\u001b[0m \u001b[38;5;66;03m# One cannot replace List by Tuple or Sequence in \"_output_padding\" because\u001b[39;00m\n\u001b[0;32m    921\u001b[0m \u001b[38;5;66;03m# TorchScript does not support `Sequence[T]` or `Tuple[T, ...]`.\u001b[39;00m\n\u001b[0;32m    922\u001b[0m output_padding \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_output_padding(\n\u001b[0;32m    923\u001b[0m     \u001b[38;5;28minput\u001b[39m, output_size, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstride, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkernel_size, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdilation)  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[1;32m--> 925\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv_transpose2d\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    926\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    927\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_padding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroups\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdilation\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 1.52 GiB (GPU 0; 6.00 GiB total capacity; 3.08 GiB already allocated; 0 bytes free; 4.03 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA out of memory. Tried to allocate 1.52 GiB (GPU 0; 6.00 GiB total capacity; 3.08 GiB already allocated; 0 bytes free; 4.03 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Input \u001b[1;32mIn [19]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mmodeltorchviz\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgenerator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrandn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnz\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mngf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnc\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequires_grad_\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[1;32mIn [18]\u001b[0m, in \u001b[0;36mmodeltorchviz\u001b[1;34m(model, input2)\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmodeltorchviz\u001b[39m(model,input2):\n\u001b[1;32m----> 4\u001b[0m     y \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput2\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcuda\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m    \u001b[38;5;66;03m# 获取网络的预测值\u001b[39;00m\n\u001b[0;32m      5\u001b[0m     MyConvNetVis \u001b[38;5;241m=\u001b[39m make_dot(y, params\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mdict\u001b[39m(\u001b[38;5;28mlist\u001b[39m(model\u001b[38;5;241m.\u001b[39mnamed_parameters()) \u001b[38;5;241m+\u001b[39m [(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mx\u001b[39m\u001b[38;5;124m'\u001b[39m, input2)]))\n\u001b[0;32m      6\u001b[0m     MyConvNetVis\u001b[38;5;241m.\u001b[39mformat \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpng\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[1;32mD:\\Miniconda3\\envs\\torch_1-11\\lib\\site-packages\\torch\\nn\\modules\\module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1106\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1107\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1108\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1109\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1110\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1111\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1112\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Input \u001b[1;32mIn [4]\u001b[0m, in \u001b[0;36mGenerator.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m     39\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mReLU(x)\n\u001b[0;32m     40\u001b[0m \u001b[38;5;66;03m#x = self.DropOut(x)\u001b[39;00m\n\u001b[1;32m---> 42\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv4\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     43\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mBatchNorm4(x)\n\u001b[0;32m     44\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mReLU(x)\n",
      "File \u001b[1;32mD:\\Miniconda3\\envs\\torch_1-11\\lib\\site-packages\\torch\\nn\\modules\\module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1106\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1107\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1108\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1109\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1110\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1111\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1112\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32mD:\\Miniconda3\\envs\\torch_1-11\\lib\\site-packages\\torch\\nn\\modules\\conv.py:925\u001b[0m, in \u001b[0;36mConvTranspose2d.forward\u001b[1;34m(self, input, output_size)\u001b[0m\n\u001b[0;32m    920\u001b[0m \u001b[38;5;66;03m# One cannot replace List by Tuple or Sequence in \"_output_padding\" because\u001b[39;00m\n\u001b[0;32m    921\u001b[0m \u001b[38;5;66;03m# TorchScript does not support `Sequence[T]` or `Tuple[T, ...]`.\u001b[39;00m\n\u001b[0;32m    922\u001b[0m output_padding \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_output_padding(\n\u001b[0;32m    923\u001b[0m     \u001b[38;5;28minput\u001b[39m, output_size, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstride, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkernel_size, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdilation)  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[1;32m--> 925\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv_transpose2d\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    926\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    927\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_padding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroups\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdilation\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 1.52 GiB (GPU 0; 6.00 GiB total capacity; 3.08 GiB already allocated; 0 bytes free; 4.03 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "modeltorchviz(generator, torch.randn(batch_size, nz, ngf, nc).requires_grad_(True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d02b9f92",
   "metadata": {},
   "outputs": [],
   "source": [
    "modeltorchviz(discriminator, torch.randn(batch_size, ndf, nc, nb_label).requires_grad_(True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d1b6459",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(predict, labels):\n",
    "    correct = 0\n",
    "    pred = predict.data.max(1)[1]\n",
    "    correct = pred.eq(labels.data).cpu().sum()\n",
    "    return correct, len(labels.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0714f81e",
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(num_epochs):\n",
    "    for i, data in enumerate(tqdm(train_loader, 0)):\n",
    "        ###########################\n",
    "        # (1) Update D network\n",
    "        ###########################\n",
    "        # train with real\n",
    "        discriminator.zero_grad()\n",
    "        img, label = data\n",
    "        batch_size = img.size(0)\n",
    "        with torch.no_grad():\n",
    "            input.resize_(img.size()).copy_(img)\n",
    "            s_label.resize_(batch_size).fill_(real_label)\n",
    "            c_label.resize_(batch_size).copy_(label)\n",
    "        s_output, c_output = discriminator(input)\n",
    "        s_errD_real = s_criterion(s_output, s_label)\n",
    "        c_errD_real = c_criterion(c_output, c_label)\n",
    "        errD_real = s_errD_real + c_errD_real\n",
    "        errD_real.backward()\n",
    "        D_x = s_output.data.mean()\n",
    "        \n",
    "        correct, length = test(c_output, c_label)\n",
    "\n",
    "        # train with fake\n",
    "        with torch.no_grad():\n",
    "            noise.resize_(batch_size, nz, 1, 1)\n",
    "            noise.normal_(0, 1)\n",
    "\n",
    "        label = np.random.randint(0, nb_label, batch_size)\n",
    "        noise_ = np.random.normal(0, 1, (batch_size, nz))\n",
    "        label_onehot = np.zeros((batch_size, nb_label))\n",
    "        label_onehot[np.arange(batch_size), label] = 1\n",
    "        noise_[np.arange(batch_size), :nb_label] = label_onehot[np.arange(batch_size)]\n",
    "        \n",
    "        noise_ = (torch.from_numpy(noise_))\n",
    "        noise_ = noise_.resize_(batch_size, nz, 1, 1)\n",
    "        noise.data.copy_(noise_)\n",
    "\n",
    "        c_label.data.resize_(batch_size).copy_(torch.from_numpy(label))\n",
    "\n",
    "        fake = generator(noise)\n",
    "        s_label.data.fill_(fake_label)\n",
    "        s_output,c_output = discriminator(fake.detach())\n",
    "        s_errD_fake = s_criterion(s_output, s_label)\n",
    "        c_errD_fake = c_criterion(c_output, c_label)\n",
    "        errD_fake = s_errD_fake + c_errD_fake\n",
    "\n",
    "        errD_fake.backward()\n",
    "        D_G_z1 = s_output.data.mean()\n",
    "        errD = s_errD_real + s_errD_fake\n",
    "        optimizerD.step()\n",
    "\n",
    "        ###########################\n",
    "        # (2) Update G network\n",
    "        ###########################\n",
    "        generator.zero_grad()\n",
    "        s_label.data.fill_(real_label)  # fake labels are real for generator cost\n",
    "        s_output,c_output = discriminator(fake)\n",
    "        s_errG = s_criterion(s_output, s_label)\n",
    "        c_errG = c_criterion(c_output, c_label)\n",
    "        \n",
    "        errG = s_errG + c_errG\n",
    "        errG.backward()\n",
    "        D_G_z2 = s_output.data.mean()\n",
    "        optimizerG.step()\n",
    "        metrics['train.G_losses'].append(errG.item())\n",
    "        metrics['train.D_losses'].append(errD.item())\n",
    "\n",
    "        print('[%d/%d][%d/%d] Loss_D: %.4f Loss_G: %.4f D(x): %.4f D(G(z)): %.4f / %.4f, Accuracy: %.4f / %.4f = %.4f'\n",
    "              % (epoch, num_epochs, i, len(train_loader),\n",
    "                 errD.data, errG.data, D_x, D_G_z1, D_G_z2,\n",
    "                 correct, length, 100.* correct / length))\n",
    "        if i % 100 == 0:\n",
    "            vutils.save_image(img,\n",
    "                    '%s/real_samples.png' % './augGAN/output_images/ACGAN', normalize=True)\n",
    "            #fake = netG(fixed_cat)\n",
    "            fake = generator(fixed_noise)\n",
    "            vutils.save_image(fake.data,\n",
    "                    '%s/fake_samples_epoch_%03d.png' % ('./augGAN/output_images/ACGAN', epoch), normalize=True)\n",
    "\n",
    "    # do checkpointing\n",
    "    #torch.save(generator.state_dict(), '%s/netG_epoch_%d.pth' % (os.path.join('.', 'augGAN/model/ACGAN'), epoch))\n",
    "    #torch.save(discriminator.state_dict(), '%s/netD_epoch_%d.pth' % (os.path.join('.', 'augGAN/model/ACGAN'), epoch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09e18531",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test2(generator, discriminator, num_epochs, metrics, loader):\n",
    "    print('Testing Block.........')\n",
    "    now = datetime.datetime.now()\n",
    "    #g_losses = metrics['train.G_losses'][-1]\n",
    "    #d_losses = metrics['train.D_losses'][-1]\n",
    "    path='augGAN/output_images/ACGAN'\n",
    "    try:\n",
    "      os.mkdir(os.path.join('.', path))\n",
    "    except Exception as error:\n",
    "      print(error)\n",
    "\n",
    "    real_batch = next(iter(loader))\n",
    "    \n",
    "    test_img_list = []\n",
    "    test_noise = torch.randn(batch_size, nz, 1, 1, device=device)\n",
    "    test_fake = generator(test_noise).detach().cpu()\n",
    "    test_img_list.append(vutils.make_grid(test_fake, padding=2, normalize=True))\n",
    "\n",
    "    fig = plt.figure(figsize=(15,15))\n",
    "    ax1 = plt.subplot(1,2,1)\n",
    "    ax1 = plt.axis(\"off\")\n",
    "    ax1 = plt.title(\"Real Images\")\n",
    "    ax1 = plt.imshow(np.transpose(vutils.make_grid(real_batch[0].to(device)[:64], padding=5, normalize=True).cpu(),(1,2,0)))\n",
    "\n",
    "    ax2 = plt.subplot(1,2,2)\n",
    "    ax2 = plt.axis(\"off\")\n",
    "    ax2 = plt.title(\"Fake Images\")\n",
    "    ax2 = plt.imshow(np.transpose(test_img_list[-1],(1,2,0)))\n",
    "    #ax2 = plt.show()\n",
    "    #fig.savefig('%s/image_%.3f_%.3f_%d_%s.png' %\n",
    "    #                (path, g_losses, d_losses, num_epochs, now.strftime(\"%Y-%m-%d_%H:%M:%S\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9cca5ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot(name, train_epoch, values, path, save):\n",
    "    clear_output(wait=True)\n",
    "    plt.close('all')\n",
    "    fig = plt.figure()\n",
    "    fig = plt.ion()\n",
    "    fig = plt.subplot(1, 1, 1)\n",
    "    fig = plt.title('epoch: %s -> %s: %s' % (train_epoch, name, values[-1]))\n",
    "    fig = plt.ylabel(name)\n",
    "    fig = plt.xlabel('train_set')\n",
    "    fig = plt.plot(values)\n",
    "    fig = plt.grid()\n",
    "    get_fig = plt.gcf()\n",
    "    fig = plt.draw()  # draw the plot\n",
    "    fig = plt.pause(1)  # show it for 1 second\n",
    "    if save:\n",
    "        now = datetime.datetime.now()\n",
    "        get_fig.savefig('%s/%s_%.3f_%d_%s.png' %\n",
    "                        (path, name, train_epoch, values[-1], now.strftime(\"%Y-%m-%d_%H:%M:%S\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d5e7892",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model(generator, discriminator, gen_optimizer, dis_optimizer, metrics, num_epochs):\n",
    "    now = datetime.datetime.now()\n",
    "    g_losses = metrics['train.G_losses'][-1]\n",
    "    d_losses = metrics['train.D_losses'][-1]\n",
    "    name = \"%+.3f_%+.3f_%d_%s.dat\" % (g_losses, d_losses, num_epochs, now.strftime(\"%Y-%m-%d_%H:%M:%S\"))\n",
    "    # fname = os.path.join('.', 'augGAN/model', name)\n",
    "    # states = {\n",
    "    #         'state_dict_generator': generator.state_dict(),\n",
    "    #         'state_dict_discriminator': discriminator.state_dict(),\n",
    "    #         'gen_optimizer': gen_optimizer.state_dict(),\n",
    "    #         'dis_optimizer': dis_optimizer.state_dict(),\n",
    "    #         'metrics': metrics,\n",
    "    #         'train_epoch': num_epochs,\n",
    "    #         'date': now.strftime(\"%Y-%m-%d_%H:%M:%S\"),\n",
    "    # }\n",
    "    # torch.save(states, fname)\n",
    "    path='augGAN/plots/ACGAN/train_%+.3f_%+.3f_%s'% (g_losses, d_losses, now.strftime(\"%Y-%m-%d_%H:%M:%S\"))\n",
    "    try:\n",
    "        os.mkdir(os.path.join('.', path))\n",
    "    except Exception as error:\n",
    "        print(error)\n",
    "\n",
    "    plot('G_losses', num_epochs, metrics['train.G_losses'], path, True)\n",
    "    plot('D_losses', num_epochs, metrics['train.D_losses'], path, True)\n",
    "    plot('D_x', num_epochs, metrics['train.D_x'], path, True)\n",
    "    plot('D_G_z1', num_epochs, metrics['train.D_G_z1'], path, True)\n",
    "    plot('D_G_z2', num_epochs, metrics['train.D_G_z2'], path, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a280fe2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "test2(generator, discriminator, num_epochs, metrics, train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f66ad873",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_model(generator, discriminator, optimizerG, optimizerD, metrics, num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c56cd9c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_batch = 16\n",
    "test_fake = 1\n",
    "\n",
    "if test_fake:\n",
    "    #check for fake image\n",
    "    test_img_list = []\n",
    "    test_noise = torch.randn(test_batch, nz, 1, 1, device=device)\n",
    "    test_img = generator(test_noise)#.detach().cpu()\n",
    "\n",
    "else:\n",
    "    #check for real image\n",
    "    test_loader = torch.utils.data.DataLoader(train_set, batch_size=test_batch,\n",
    "                                            shuffle=True)\n",
    "    data = next(iter(test_loader))\n",
    "    test_noise, test_class_lable = data\n",
    "    test_img.data.resize_(test_noise.size()).copy_(test_noise)\n",
    "    #print(data[0].size())\n",
    "    print('class label for real', test_class_lable)\n",
    "\n",
    "s_output,c_label_op = discriminator(test_img.detach().to(device))\n",
    "print('Discriminator s o/p', s_output)\n",
    "print('Discriminator c o/p', c_label_op)\n",
    "\n",
    "# label = np.random.randint(0, nb_label, batch_size)\n",
    "# c_label.data.resize_(batch_size).copy_(torch.from_numpy(label))\n",
    "# print(c_label)\n",
    "\n",
    "test_img = test_img.detach().cpu()\n",
    "test_img_list.append(vutils.make_grid(test_img, padding=2, normalize=True))\n",
    "plt.imshow(np.transpose(test_img_list[-1],(1,2,0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d45fe52a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
